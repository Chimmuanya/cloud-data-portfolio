{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f1790e-3ea3-4a8c-8507-e3dc33187988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-ai-portfolio/project-01-serverless-data-qc\n",
      "SRC path appended: /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-ai-portfolio/project-01-serverless-data-qc/src\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — environment setup\n",
    "import sys, os\n",
    "project_root = os.path.abspath(\"..\")   # one level up from notebooks/\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "import pandas as pd\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"SRC path appended:\", src_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f22dd4-10f7-4f9f-8d72-0d92db8c5568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>email</th>\n",
       "      <th>signup_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alice</td>\n",
       "      <td>30</td>\n",
       "      <td>a@x.com</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bob</td>\n",
       "      <td>twenty</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-02-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   name     age    email signup_date\n",
       "0   1  Alice      30  a@x.com  2020-01-01\n",
       "1   2    Bob  twenty     None  2021-02-02"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2 — sample dataframe for prototyping\n",
    "df = pd.DataFrame([\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"age\": 30, \"email\": \"a@x.com\", \"signup_date\": \"2020-01-01\"},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"age\": \"twenty\", \"email\": None, \"signup_date\": \"2021-02-02\"},\n",
    "])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4796362-4796-4266-b11a-149ae2172133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0 issues (Expected: []): []\n",
      "Row 1 issues (Expected: 'age' not_integer): [{'column': 'age', 'issue': 'not_integer', 'value': 'twenty'}]\n",
      "Row 2 issues (Expected: 'name' missing, 'age' out_of_range): [{'column': 'name', 'issue': 'missing_required', 'value': None}, {'column': 'age', 'issue': 'age_out_of_range', 'value': 150}]\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — prototype validator (quick, iterative)\n",
    "\n",
    "# Import the pandas library, which is necessary for handling DataFrame structures\n",
    "# and using its specific utility functions like pd.isna().\n",
    "import pandas as pd\n",
    "\n",
    "# --- Mock DataFrame Setup (Added for runnable code) ---\n",
    "# NOTE: The original script referenced a global 'df', so we create a mock\n",
    "# DataFrame here to make the example fully runnable and testable.\n",
    "data = {\n",
    "    \"id\": [1, 2, 3],\n",
    "    \"name\": [\"Alice\", \"Bob\", None],\n",
    "    \"age\": [30, \"twenty\", 150]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "# df.iloc[0] is {'id': 1, 'name': 'Alice', 'age': 30} (Expected: No issues)\n",
    "# df.iloc[1] is {'id': 2, 'name': 'Bob', 'age': 'twenty'} (Expected: 'age' not_integer)\n",
    "# df.iloc[2] is {'id': 3, 'name': None, 'age': 150} (Expected: 'name' missing_required, 'age' out_of_range)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "def validate_row(row):\n",
    "    \"\"\"\n",
    "    Validates a single row (Pandas Series or dictionary-like object) for\n",
    "    missing required fields and age data type/range constraints.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series or dict): A single record to validate.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents a\n",
    "              data quality issue found in the row.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store all validation issues found in the row.\n",
    "    issues = []\n",
    "\n",
    "    # Define the list of columns that must be present and non-null in every row.\n",
    "    required = [\"id\", \"name\", \"age\"]\n",
    "\n",
    "    # 1. Missing Required Field Check\n",
    "    for col in required:\n",
    "        # Check if the column key is missing from the row OR if the value\n",
    "        # associated with the column is considered NaN (Not a Number) by pandas,\n",
    "        # which covers None, np.nan, and pd.NA.\n",
    "        # .get(col, None) safely retrieves the value, defaulting to None if the key doesn't exist.\n",
    "        if col not in row or pd.isna(row.get(col, None)):\n",
    "            # If an issue is found, append a detailed dictionary describing it.\n",
    "            issues.append({\n",
    "                \"column\": col,\n",
    "                \"issue\": \"missing_required\",\n",
    "                \"value\": row.get(col, None) # The value that was found (often None or NaN)\n",
    "            })\n",
    "\n",
    "    # 2. Age-Specific Checks (Type and Range)\n",
    "    try:\n",
    "        # Retrieve the age value.\n",
    "        age_val = row.get(\"age\", None)\n",
    "\n",
    "        # Attempt to convert the age value to an integer. This will fail\n",
    "        # if the value is non-numeric (e.g., \"twenty\").\n",
    "        age_int = int(age_val)\n",
    "\n",
    "        # Check if the converted integer age is outside a reasonable range (0 to 120).\n",
    "        if age_int < 0 or age_int > 120:\n",
    "            # If out of range, record the specific issue.\n",
    "            issues.append({\n",
    "                \"column\": \"age\",\n",
    "                \"issue\": \"age_out_of_range\",\n",
    "                \"value\": age_int\n",
    "            })\n",
    "\n",
    "    # This catches exceptions like ValueError (if int(age_val) fails because it's non-numeric)\n",
    "    # or TypeError (if age_val is None and fails conversion, though pd.isna should catch missing values).\n",
    "    except Exception:\n",
    "        # If the conversion to integer fails, record the \"not_integer\" issue.\n",
    "        # This includes cases where age is non-numeric or missing (though missing is often caught above).\n",
    "        issues.append({\n",
    "            \"column\": \"age\",\n",
    "            \"issue\": \"not_integer\",\n",
    "            \"value\": row.get(\"age\", None) # Shows the original value that caused the error\n",
    "        })\n",
    "# in notebook cell\n",
    "from importlib import reload\n",
    "import src.validator as validator\n",
    "reload(validator)\n",
    "df = pd.read_csv(\"../samples/good.csv\")\n",
    "report = validator.validate_dataframe(df)\n",
    "report\n",
    "\n",
    "    # Return the complete list of issues. If the list is empty, the row is valid.\n",
    "    return issues\n",
    "\n",
    "# quick tests\n",
    "# Use the iloc accessor to pass a single row (as a Series) to the validator.\n",
    "print(\"Row 0 issues (Expected: []):\", validate_row(df.iloc[0]))\n",
    "print(\"Row 1 issues (Expected: 'age' not_integer):\", validate_row(df.iloc[1]))\n",
    "print(\"Row 2 issues (Expected: 'name' missing, 'age' out_of_range):\", validate_row(df.iloc[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2142cd73-dcef-4f48-8dc1-3a7e6631c954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading S3 object good.csv from local-bucket: An error occurred (AccessDenied) when calling the GetObject operation: Access Denied\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (AccessDenied) when calling the GetObject operation: Access Denied",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m utils_mod.write_s3_json = fake_write\n\u001b[32m     27\u001b[39m event = {\u001b[33m\"\u001b[39m\u001b[33mRecords\u001b[39m\u001b[33m\"\u001b[39m:[{\u001b[33m\"\u001b[39m\u001b[33ms3\u001b[39m\u001b[33m\"\u001b[39m:{\u001b[33m\"\u001b[39m\u001b[33mbucket\u001b[39m\u001b[33m\"\u001b[39m:{\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mlocal-bucket\u001b[39m\u001b[33m\"\u001b[39m},\u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m:{\u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mgood.csv\u001b[39m\u001b[33m\"\u001b[39m}}}]}\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m res = \u001b[43mhandler_mod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mwritten keys:\u001b[39m\u001b[33m\"\u001b[39m, written.keys())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-ai-portfolio/project-01-serverless-data-qc/notebooks/../src/handler.py:61\u001b[39m, in \u001b[36mhandler\u001b[39m\u001b[34m(event, context)\u001b[39m\n\u001b[32m     57\u001b[39m bucket, key = s3_event_to_bucket_key(event)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# 2. Read Input Data: Download the file content as raw bytes using the utility function.\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# The 'raw' variable now holds the CSV file data.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m raw = \u001b[43mread_s3_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# 3. Data Processing: Convert the raw bytes into a Pandas DataFrame.\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Assumes the input is text/csv. io.BytesIO wraps the bytes so pandas can read it \u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# directly from memory without needing to save it to a temporary disk file.\u001b[39;00m\n\u001b[32m     66\u001b[39m df = pd.read_csv(io.BytesIO(raw))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-ai-portfolio/project-01-serverless-data-qc/notebooks/../src/utils.py:25\u001b[39m, in \u001b[36mread_s3_object\u001b[39m\u001b[34m(bucket, key)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03mReads the full content of an object (file) from S3.\u001b[39;00m\n\u001b[32m     15\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \u001b[33;03m    bytes: The raw content of the file as bytes.\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# Calls the S3 GetObject API. This is the primary action for downloading data.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     resp = \u001b[43ms3\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# Reads the entire content from the response body and returns it as raw bytes.\u001b[39;00m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resp[\u001b[33m\"\u001b[39m\u001b[33mBody\u001b[39m\u001b[33m\"\u001b[39m].read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/botocore/client.py:602\u001b[39m, in \u001b[36mClientCreator._create_api_method.<locals>._api_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    599\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m() only accepts keyword arguments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    600\u001b[39m     )\n\u001b[32m    601\u001b[39m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/botocore/context.py:123\u001b[39m, in \u001b[36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[32m    122\u001b[39m     hook()\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/botocore/client.py:1078\u001b[39m, in \u001b[36mBaseClient._make_api_call\u001b[39m\u001b[34m(self, operation_name, api_params)\u001b[39m\n\u001b[32m   1074\u001b[39m     error_code = request_context.get(\n\u001b[32m   1075\u001b[39m         \u001b[33m'\u001b[39m\u001b[33merror_code_override\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1076\u001b[39m     ) \u001b[38;5;129;01mor\u001b[39;00m error_info.get(\u001b[33m\"\u001b[39m\u001b[33mCode\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1077\u001b[39m     error_class = \u001b[38;5;28mself\u001b[39m.exceptions.from_code(error_code)\n\u001b[32m-> \u001b[39m\u001b[32m1078\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[32m   1079\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1080\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[31mClientError\u001b[39m: An error occurred (AccessDenied) when calling the GetObject operation: Access Denied"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Get the path of the current notebook (the directory it's in).\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# 2. Go up one level to find the project root (where src/ is located).\n",
    "project_root = os.path.join(notebook_dir, '..')\n",
    "\n",
    "# 3. Add the project root to the Python search path.\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Now, imports should work!\n",
    "\n",
    "import src.handler as handler_mod\n",
    "import src.utils as utils_mod\n",
    "\n",
    "# monkeypatch read_s3_object to read file bytes\n",
    "orig_read = utils_mod.read_s3_object\n",
    "orig_write = utils_mod.write_s3_json\n",
    "\n",
    "utils_mod.read_s3_object = lambda b,k: open(\"samples/good.csv\",\"rb\").read()\n",
    "written = {}\n",
    "def fake_write(b,k,p): written[k]=p\n",
    "utils_mod.write_s3_json = fake_write\n",
    "\n",
    "event = {\"Records\":[{\"s3\":{\"bucket\":{\"name\":\"local-bucket\"},\"object\":{\"key\":\"good.csv\"}}}]}\n",
    "res = handler_mod.handler(event, None)\n",
    "print(res)\n",
    "print(\"written keys:\", written.keys())\n",
    "\n",
    "# restore\n",
    "utils_mod.read_s3_object = orig_read\n",
    "utils_mod.write_s3_json = orig_write\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b49d343-2624-42c7-af27-ad6ef0c2a984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting handler execution with mock S3 I/O ---\n",
      "Error reading S3 object good.csv from local-bucket: An error occurred (AccessDenied) when calling the GetObject operation: Access Denied\n",
      "\n",
      "--- Handler execution failed ---\n",
      "Error: An error occurred (AccessDenied) when calling the GetObject operation: Access Denied\n",
      "\n",
      "Cleanup complete: Original S3 functions restored.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import io\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# --- Setup to fix module imports (remains the same) ---\n",
    "# 1. Get the path of the current notebook (the directory it's in).\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# 2. Go up one level to find the project root (where src/ is located).\n",
    "project_root = os.path.join(notebook_dir, '..')\n",
    "\n",
    "# 3. Add the project root to the Python search path.\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Now, imports should work!\n",
    "import src.handler as handler_mod\n",
    "import src.utils as utils_mod\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# --- Monkeypatching for Local Testing ---\n",
    "# Goal: Replace the AWS-dependent S3 functions with local file access/mocking\n",
    "# to test the handler logic without needing live AWS credentials.\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 1. Backup original functions\n",
    "orig_read = utils_mod.read_s3_object\n",
    "orig_write = utils_mod.write_s3_json\n",
    "\n",
    "# 2. Mock storage for written reports\n",
    "written_reports = {}\n",
    "\n",
    "# 3. Replace read_s3_object to read a local sample file (e.g., samples/good.csv)\n",
    "# NOTE: The lambda function doesn't need to check the bucket/key; it just reads the mock file.\n",
    "# We are assuming 'samples/good.csv' exists in the project root structure.\n",
    "def fake_read(bucket, key):\n",
    "    # This reads the CSV file from your local disk and returns its bytes, \n",
    "    # perfectly simulating what the real S3 call would return.\n",
    "    try:\n",
    "        # Assuming the samples folder is in the project root directory\n",
    "        file_path = os.path.join(project_root, \"samples\", \"good.csv\")\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Sample file not found at {file_path}\")\n",
    "        # To mimic the real function, we raise an error if the mock file is missing.\n",
    "        raise\n",
    "\n",
    "utils_mod.read_s3_object = fake_read\n",
    "\n",
    "# 4. Replace write_s3_json to save the report to the local 'written_reports' dictionary\n",
    "def fake_write(bucket, key, payload, acl=None):\n",
    "    # Saves the report payload (Python dict) to our local mock storage.\n",
    "    written_reports[key] = payload\n",
    "\n",
    "utils_mod.write_s3_json = fake_write\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# --- Test Execution ---\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Mock the S3 event payload.\n",
    "event = {\"Records\":[\n",
    "    {\"s3\":{\n",
    "        \"bucket\":{\"name\":\"local-bucket\"},\n",
    "        \"object\":{\"key\":\"good.csv\"} # This key will be used to generate the report path\n",
    "    }}\n",
    "]}\n",
    "\n",
    "# Run the handler (Lambda execution simulation)\n",
    "# It will use fake_read (Step 3) and fake_write (Step 4).\n",
    "try:\n",
    "    print(\"--- Starting handler execution with mock S3 I/O ---\")\n",
    "    res = handler_mod.handler(event, None)\n",
    "    \n",
    "    # Check results\n",
    "    print(f\"\\nHandler Response: {res}\")\n",
    "    print(f\"Report Key Written: {list(written_reports.keys())[0]}\")\n",
    "    \n",
    "    # You can inspect the generated report here\n",
    "    # report_content = written_reports[list(written_reports.keys())[0]]\n",
    "    # print(json.dumps(report_content, indent=2))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- Handler execution failed ---\")\n",
    "    # If the handler fails, we print the error here instead of relying on the traceback.\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# --- Cleanup ---\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Restore the original AWS-dependent functions so other tests don't break.\n",
    "utils_mod.read_s3_object = orig_read\n",
    "utils_mod.write_s3_json = orig_write\n",
    "\n",
    "print(\"\\nCleanup complete: Original S3 functions restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68133047-b9e9-4227-ac4d-4a351cb7c075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting handler execution with mock S3 I/O ---\n",
      "\n",
      "Handler Response: {'statusCode': 200, 'body': '{\"report_key\": \"reports/good.csv.qc.json\"}'}\n",
      "Report Key Written: reports/good.csv.qc.json\n",
      "\n",
      "--- Validation Report Content (First 1000 chars) ---\n",
      "{\n",
      "  \"num_rows\": 2,\n",
      "  \"num_columns\": 5,\n",
      "  \"columns_present\": [\n",
      "    \"id\",\n",
      "    \"name\",\n",
      "    \"age\",\n",
      "    \"email\",\n",
      "    \"signup_date\"\n",
      "  ],\n",
      "  \"row_issues\": {},\n",
      "  \"summary\": {\n",
      "    \"columns\": {\n",
      "      \"id\": {\n",
      "        \"non_null_count\": 2,\n",
      "        \"unique\": 2,\n",
      "        \"dtype\": \"int64\"\n",
      "      },\n",
      "      \"name\": {\n",
      "        \"non_null_count\": 2,\n",
      "        \"unique\": 2,\n",
      "        \"dtype\": \"object\"\n",
      "      },\n",
      "      \"age\": {\n",
      "        \"non_null_count\": 2,\n",
      "        \"unique\": 2,\n",
      "        \"dtype\": \"int64\"\n",
      "      },\n",
      "      \"email\": {\n",
      "        \"non_null_count\": 2,\n",
      "        \"unique\": 2,\n",
      "        \"dtype\": \"object\"\n",
      "      },\n",
      "      \"signup_date\": {\n",
      "        \"non_null_count\": 2,\n",
      "        \"unique\": 2,\n",
      "        \"dtype\": \"object\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"missing_required_columns\": [],\n",
      "  \"total_issues\": 0\n",
      "}...\n",
      "\n",
      "Cleanup complete: Original S3 functions restored.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import io\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# --- Setup to fix module imports (remains the same) ---\n",
    "# 1. Get the path of the current notebook (the directory it's in).\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# 2. Go up one level to find the project root (where src/ is located).\n",
    "project_root = os.path.join(notebook_dir, '..')\n",
    "\n",
    "# 3. Add the project root to the Python search path.\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Now, imports should work!\n",
    "import src.handler as handler_mod\n",
    "import src.utils as utils_mod\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# --- Monkeypatching for Local Testing ---\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 1. Backup original functions (from the utils module)\n",
    "orig_read = utils_mod.read_s3_object\n",
    "orig_write = utils_mod.write_s3_json\n",
    "\n",
    "# 2. Mock storage for written reports\n",
    "written_reports = {}\n",
    "\n",
    "# 3. Define the fake read function\n",
    "def fake_read(bucket, key):\n",
    "    # This reads the CSV file from your local disk and returns its bytes.\n",
    "    try:\n",
    "        # Assuming the samples folder is in the project root directory\n",
    "        file_path = os.path.join(project_root, \"samples\", \"good.csv\")\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Sample file not found at {file_path}\")\n",
    "        raise\n",
    "\n",
    "# 4. Define the fake write function\n",
    "def fake_write(bucket, key, payload, acl=None):\n",
    "    # Saves the report payload (Python dict) to our local mock storage.\n",
    "    written_reports[key] = payload\n",
    "\n",
    "# 5. !!! CRITICAL FIX: PATCH BOTH MODULES !!!\n",
    "# We patch both handler_mod and utils_mod to ensure the functions are replaced \n",
    "# where they are defined AND where they are called.\n",
    "utils_mod.read_s3_object = fake_read\n",
    "utils_mod.write_s3_json = fake_write\n",
    "\n",
    "# Since handler.py imported these functions by name, we MUST patch them there too.\n",
    "handler_mod.read_s3_object = fake_read\n",
    "handler_mod.write_s3_json = fake_write\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# --- Test Execution ---\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Mock the S3 event payload.\n",
    "event = {\"Records\":[\n",
    "    {\"s3\":{\n",
    "        \"bucket\":{\"name\":\"local-bucket\"},\n",
    "        \"object\":{\"key\":\"good.csv\"} # This key will be used to generate the report path\n",
    "    }}\n",
    "]}\n",
    "\n",
    "# Run the handler (Lambda execution simulation)\n",
    "try:\n",
    "    print(\"--- Starting handler execution with mock S3 I/O ---\")\n",
    "    res = handler_mod.handler(event, None)\n",
    "    \n",
    "    # Check results\n",
    "    print(f\"\\nHandler Response: {res}\")\n",
    "    print(f\"Report Key Written: {list(written_reports.keys())[0]}\")\n",
    "    \n",
    "    # Print the report content to verify the validation worked\n",
    "    report_key = list(written_reports.keys())[0]\n",
    "    report_content = written_reports[report_key]\n",
    "    print(\"\\n--- Validation Report Content (First 1000 chars) ---\")\n",
    "    print(json.dumps(report_content, indent=2)[:2000] + \"...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- Handler execution failed ---\\nError: {e}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# --- Cleanup ---\\n# ----------------------------------------------------------------------\n",
    "\n",
    "# Restore the original AWS-dependent functions so other tests don't break.\n",
    "utils_mod.read_s3_object = orig_read\n",
    "utils_mod.write_s3_json = orig_write\n",
    "\n",
    "# Also restore the handler module's original references\n",
    "handler_mod.read_s3_object = orig_read\n",
    "handler_mod.write_s3_json = orig_write\n",
    "\n",
    "print(\"\\nCleanup complete: Original S3 functions restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed74a40f-de05-4ccc-bc9b-e7839fc7cd50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
