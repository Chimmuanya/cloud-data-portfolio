{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c45bc2-8a14-4b8a-adf6-267e83331e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44511/657871570.py:73: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat()+\"Z\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running local ingest (GHO + Outbreaks) -> sample_outputs/\n",
      "{'fetched_at': '2025-12-11T12:46:42.680903Z', 'gho': [{'indicator': 'MALARIA_INC', 'error': '404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MALARIA_INC'}, {'indicator': 'CHOLERA_CASES', 'error': '404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/CHOLERA_CASES'}, {'indicator': 'MEASLESINC', 'error': '404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MEASLESINC'}], 'outbreaks': None, 'outbreaks_list': 'sample_outputs/outbreaks-list-20251211T124650Z.json', 'outbreaks_csv': 'sample_outputs/outbreaks_clean.csv'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44511/657871570.py:27: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  fname = f\"{OUTPUT_DIR}/outbreaks-list-{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json\"\n",
      "/tmp/ipykernel_44511/657871570.py:105: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  evf = os.path.join(OUTPUT_DIR, f\"ingest-evidence-{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json\")\n"
     ]
    }
   ],
   "source": [
    "# src/ingest/gho_and_outbreaks.py\n",
    "import os, json, time, csv, uuid, requests\n",
    "from datetime import datetime\n",
    "\n",
    "# CONFIG (override with env vars or set here)\n",
    "WHO_GHO_BASE = \"https://ghoapi.azureedge.net/api\"\n",
    "WHO_OUTBREAKS = \"https://www.who.int/api/news/outbreaks\"\n",
    "INDICATORS = [\"MALARIA_INC\", \"CHOLERA_CASES\", \"MEASLESINC\"]  # pick your indicators\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"sample_outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def fetch_url(url, timeout=20):\n",
    "    r = requests.get(url, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def fetch_gho_indicator(indicator):\n",
    "    url = f\"{WHO_GHO_BASE}/{indicator}\"\n",
    "    j = fetch_url(url)\n",
    "    fname = f\"{OUTPUT_DIR}/{indicator}-{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, ensure_ascii=False, indent=2)\n",
    "    return fname, j\n",
    "\n",
    "def fetch_outbreaks_list():\n",
    "    j = fetch_url(WHO_OUTBREAKS)\n",
    "    fname = f\"{OUTPUT_DIR}/outbreaks-list-{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, ensure_ascii=False, indent=2)\n",
    "    return fname, j\n",
    "\n",
    "def fetch_outbreak_by_key(key):\n",
    "    url = f\"{WHO_OUTBREAKS}({key})\"\n",
    "    j = fetch_url(url)\n",
    "    fname = f\"{OUTPUT_DIR}/outbreak-{key}-{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, ensure_ascii=False, indent=2)\n",
    "    return fname, j\n",
    "\n",
    "# lightweight normalizer: returns list of rows (dict) for outbreaks\n",
    "def normalize_outbreak_item(item):\n",
    "    # fields vary; be defensive\n",
    "    outbreak_id = item.get(\"SystemSourceKey\") or item.get(\"Id\") or str(uuid.uuid4())\n",
    "    title = item.get(\"Title\") or item.get(\"MetaTitle\") or \"\"\n",
    "    summary = item.get(\"Summary\") or \"\"\n",
    "    pub = item.get(\"PublicationDate\")\n",
    "    country_guid = item.get(\"regionscountries\")\n",
    "    # Normalize if list\n",
    "    if isinstance(country_guid, list):\n",
    "        country_guid = country_guid[0]\n",
    "    # disease topic might be GUID; attempt human-readable\n",
    "    disease_topic = item.get(\"healthtopics\")\n",
    "    return {\n",
    "        \"outbreak_id\": outbreak_id,\n",
    "        \"title\": title,\n",
    "        \"summary\": summary,\n",
    "        \"publication_date\": pub,\n",
    "        \"country_guid\": country_guid,\n",
    "        \"disease_topic\": disease_topic,\n",
    "        \"raw\": item\n",
    "    }\n",
    "\n",
    "def write_outbreaks_csv(rows, csv_path):\n",
    "    headers = [\"outbreak_id\",\"title\",\"summary\",\"publication_date\",\"country_guid\",\"disease_topic\"]\n",
    "    with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as fh:\n",
    "        w = csv.writer(fh)\n",
    "        w.writerow(headers)\n",
    "        for r in rows:\n",
    "            w.writerow([r.get(h,\"\") for h in headers])\n",
    "    return csv_path\n",
    "\n",
    "def handler(event=None, context=None):\n",
    "    ts = datetime.utcnow().isoformat()+\"Z\"\n",
    "    evidence = {\"fetched_at\": ts, \"gho\": [], \"outbreaks\": None}\n",
    "    # 1) fetch indicators\n",
    "    for ind in INDICATORS:\n",
    "        try:\n",
    "            fname, j = fetch_gho_indicator(ind)\n",
    "            evidence[\"gho\"].append({\"indicator\": ind, \"file\": fname})\n",
    "            time.sleep(0.2)\n",
    "        except Exception as e:\n",
    "            evidence[\"gho\"].append({\"indicator\": ind, \"error\": str(e)})\n",
    "    # 2) fetch outbreaks list and details\n",
    "    try:\n",
    "        list_file, list_json = fetch_outbreaks_list()\n",
    "        evidence[\"outbreaks_list\"] = list_file\n",
    "        items = list_json if isinstance(list_json, list) else list_json.get(\"value\", [])\n",
    "        normalized = []\n",
    "        for it in items:\n",
    "            # find key (SystemSourceKey or Id)\n",
    "            key = it.get(\"SystemSourceKey\") or it.get(\"Id\")\n",
    "            if key:\n",
    "                try:\n",
    "                    fname, full = fetch_outbreak_by_key(key)\n",
    "                except Exception:\n",
    "                    full = it\n",
    "                normalized.append(normalize_outbreak_item(full))\n",
    "                time.sleep(0.2)\n",
    "        csv_path = os.path.join(OUTPUT_DIR, \"outbreaks_clean.csv\")\n",
    "        write_outbreaks_csv(normalized, csv_path)\n",
    "        evidence[\"outbreaks_csv\"] = csv_path\n",
    "    except Exception as e:\n",
    "        evidence[\"outbreaks_error\"] = str(e)\n",
    "    # save evidence file\n",
    "    evf = os.path.join(OUTPUT_DIR, f\"ingest-evidence-{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json\")\n",
    "    with open(evf, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(evidence, f, indent=2, ensure_ascii=False)\n",
    "    return evidence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running local ingest (GHO + Outbreaks) -> sample_outputs/\")\n",
    "    print(handler())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f770d20-f388-4c1d-988e-bef1a172362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Sample data: []\n",
      "Total records: 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime, timezone  # Fix deprecation\n",
    "\n",
    "# Test malaria incidence\n",
    "url = \"https://ghoapi.azureedge.net/api/MALARIA001?$top=5\"  # Limit to 5 records\n",
    "try:\n",
    "    r = requests.get(url, timeout=10)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    print(\"Success! Sample data:\", data.get(\"value\", [])[:2])  # First 2 records\n",
    "    print(f\"Total records: {len(data.get('value', []))}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591685ee-2bfc-451b-add5-4ebc975ff31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Sample data (first 2 records):\n",
      "- Country: ECU, Year: 2020, Value: 0.194501775\n",
      "- Country: AGO, Year: 2020, Value: 244.7996678\n",
      "Total records: 5\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Working query for estimated malaria incidence\n",
    "url = \"https://ghoapi.azureedge.net/api/MALARIA_EST_INCIDENCE?$filter=TimeDim ge 2020 and SpatialDimType eq 'COUNTRY'&$top=5\"\n",
    "try:\n",
    "    r = requests.get(url, timeout=10)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    print(\"Success! Sample data (first 2 records):\")\n",
    "    for record in data.get(\"value\", [])[:2]:\n",
    "        print(f\"- Country: {record.get('SpatialDim', 'N/A')}, Year: {record.get('TimeDim', 'N/A')}, Value: {record.get('NumericValue', 'N/A')}\")\n",
    "    print(f\"Total records: {len(data.get('value', []))}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd61c9a-ba72-463b-a706-4e76fcd3993b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 404 Client Error: Not Found for url: https://data.who.int/datalibrary/api/v1/table/MALARIA_EST_INCIDENCE?$filter=year%20ge%202020%20and%20location_type%20eq%20'Country'&$top=5&$select=location,year,value\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# New v1 endpoint for malaria\n",
    "url = \"https://data.who.int/datalibrary/api/v1/table/MALARIA_EST_INCIDENCE?$filter=year ge 2020 and location_type eq 'Country'&$top=5&$select=location,year,value\"\n",
    "try:\n",
    "    r = requests.get(url, timeout=10)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    print(\"Success! Sample data (first 2 records):\")\n",
    "    for record in data.get(\"value\", [])[:2]:  # v1 uses \"value\" array\n",
    "        print(f\"- Country: {record.get('location', 'N/A')}, Year: {record.get('year', 'N/A')}, Value: {record.get('value', 'N/A')}\")\n",
    "    print(f\"Total records: {len(data.get('value', []))}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c96d873-cb56-489e-8f74-f3a4c98ca4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running current GHO OData ingest -> sample_outputs/\n",
      "Found 50 outbreak items\n",
      "{'fetched_at': '2025-12-11T13:03:09.202846Z', 'gho': [{'indicator': 'MALARIA_EST_INCIDENCE', 'file': 'sample_outputs/MALARIA_EST_INCIDENCE-20251211T130311Z.json', 'records': 405}, {'indicator': 'CHOLERA_0000000001', 'file': 'sample_outputs/CHOLERA_0000000001-20251211T130314Z.json', 'records': 320}, {'indicator': 'MRD_0000000001', 'error': \"404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MRD_0000000001?$filter=TimeDim%20ge%202020%20and%20SpatialDimType%20eq%20'COUNTRY'&$select=SpatialDim,TimeDim,NumericValue&$top=500\"}], 'outbreaks': None, 'outbreaks_list': 'sample_outputs/outbreaks-list-20251211T130320Z.json', 'outbreaks_csv': 'sample_outputs/outbreaks_clean.csv', 'outbreaks_count': 10}\n"
     ]
    }
   ],
   "source": [
    "# src/ingest/gho_and_outbreaks.py (Current GHO OData API - Dec 2025)\n",
    "import os, json, time, csv, uuid, requests\n",
    "from datetime import datetime, timezone  # Fixed deprecation\n",
    "\n",
    "# CONFIG\n",
    "WHO_GHO_BASE = \"https://ghoapi.azureedge.net/api\"  # Current OData v3 endpoint\n",
    "WHO_OUTBREAKS = \"https://www.who.int/api/news/diseaseoutbreaknews\"\n",
    "INDICATORS = [\"MALARIA_EST_INCIDENCE\", \"CHOLERA_0000000001\", \"MRD_0000000001\"]  # Valid codes\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"sample_outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def fetch_url(url, timeout=20, as_csv=False):\n",
    "    params = {'$format': 'csv'} if as_csv else {}\n",
    "    r = requests.get(url, timeout=timeout, params=params)\n",
    "    r.raise_for_status()\n",
    "    if as_csv:\n",
    "        return r.text  # CSV string\n",
    "    return r.json()\n",
    "\n",
    "def fetch_gho_indicator(indicator, as_csv=False):\n",
    "    # OData v3 filters: Recent country data\n",
    "    if indicator == \"CHOLERA_0000000001\":  # Broader for outbreaks\n",
    "        url = f\"{WHO_GHO_BASE}/{indicator}?$filter=TimeDim ge 2010&$select=SpatialDim,TimeDim,NumericValue&$top=500\"\n",
    "    else:\n",
    "        url = f\"{WHO_GHO_BASE}/{indicator}?$filter=TimeDim ge 2020 and SpatialDimType eq 'COUNTRY'&$select=SpatialDim,TimeDim,NumericValue&$top=500\"\n",
    "    data = fetch_url(url, as_csv=as_csv)\n",
    "    ts = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')\n",
    "    if as_csv:\n",
    "        ext = \".csv\"\n",
    "        with open(f\"{OUTPUT_DIR}/{indicator}-{ts}{ext}\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(data)\n",
    "        records = len(data.splitlines()) - 1  # Subtract header\n",
    "    else:\n",
    "        ext = \".json\"\n",
    "        with open(f\"{OUTPUT_DIR}/{indicator}-{ts}{ext}\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        records = len(data.get(\"value\", []))\n",
    "    return f\"{OUTPUT_DIR}/{indicator}-{ts}{ext}\", data, records\n",
    "\n",
    "# Outbreaks functions unchanged from previous version\n",
    "def fetch_outbreaks_list():\n",
    "    j = fetch_url(WHO_OUTBREAKS)\n",
    "    ts = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')\n",
    "    fname = f\"{OUTPUT_DIR}/outbreaks-list-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, ensure_ascii=False, indent=2)\n",
    "    return fname, j\n",
    "\n",
    "def fetch_outbreak_by_key(key):\n",
    "    url = f\"{WHO_OUTBREAKS}({key})\"\n",
    "    j = fetch_url(url)\n",
    "    ts = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')\n",
    "    fname = f\"{OUTPUT_DIR}/outbreak-{key}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, ensure_ascii=False, indent=2)\n",
    "    return fname, j\n",
    "\n",
    "def normalize_outbreak_item(item):\n",
    "    outbreak_id = item.get(\"SystemSourceKey\") or item.get(\"Id\") or str(uuid.uuid4())\n",
    "    title = item.get(\"Title\") or item.get(\"MetaTitle\") or \"\"\n",
    "    summary = item.get(\"Summary\") or \"\"\n",
    "    pub = item.get(\"PublicationDate\")\n",
    "    country_guid = item.get(\"regionscountries\") or item.get(\"Countries\")\n",
    "    if isinstance(country_guid, list):\n",
    "        country_guid = country_guid[0]\n",
    "    disease_topic = item.get(\"healthtopics\") or item.get(\"Diseases\")\n",
    "    return {\n",
    "        \"outbreak_id\": outbreak_id,\n",
    "        \"title\": title,\n",
    "        \"summary\": summary,\n",
    "        \"publication_date\": pub,\n",
    "        \"country_guid\": country_guid,\n",
    "        \"disease_topic\": disease_topic,\n",
    "        \"raw\": item\n",
    "    }\n",
    "\n",
    "def write_outbreaks_csv(rows, csv_path):\n",
    "    headers = [\"outbreak_id\",\"title\",\"summary\",\"publication_date\",\"country_guid\",\"disease_topic\"]\n",
    "    with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as fh:\n",
    "        w = csv.writer(fh)\n",
    "        w.writerow(headers)\n",
    "        for r in rows:\n",
    "            w.writerow([r.get(h,\"\") for h in headers])\n",
    "    return csv_path\n",
    "\n",
    "def handler(event=None, context=None):\n",
    "    ts = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
    "    evidence = {\"fetched_at\": ts, \"gho\": [], \"outbreaks\": None}\n",
    "    # 1) Fetch GHO indicators\n",
    "    for ind in INDICATORS:\n",
    "        try:\n",
    "            fname, data, records = fetch_gho_indicator(ind)  # Set as_csv=True for CSV\n",
    "            evidence[\"gho\"].append({\"indicator\": ind, \"file\": fname, \"records\": records})\n",
    "            time.sleep(0.2)  # Rate limit\n",
    "        except Exception as e:\n",
    "            evidence[\"gho\"].append({\"indicator\": ind, \"error\": str(e)})\n",
    "    # 2) Outbreaks\n",
    "    try:\n",
    "        list_file, list_json = fetch_outbreaks_list()\n",
    "        evidence[\"outbreaks_list\"] = list_file\n",
    "        items = list_json if isinstance(list_json, list) else list_json.get(\"value\", [])\n",
    "        print(f\"Found {len(items)} outbreak items\")\n",
    "        normalized = []\n",
    "        for it in items[:10]:  # Limit\n",
    "            key = it.get(\"SystemSourceKey\") or it.get(\"Id\")\n",
    "            if key:\n",
    "                try:\n",
    "                    fname, full = fetch_outbreak_by_key(key)\n",
    "                except Exception:\n",
    "                    full = it\n",
    "            else:\n",
    "                full = it\n",
    "            normalized.append(normalize_outbreak_item(full))\n",
    "            time.sleep(0.2)\n",
    "        csv_path = os.path.join(OUTPUT_DIR, \"outbreaks_clean.csv\")\n",
    "        write_outbreaks_csv(normalized, csv_path)\n",
    "        evidence[\"outbreaks_csv\"] = csv_path\n",
    "        evidence[\"outbreaks_count\"] = len(normalized)\n",
    "    except Exception as e:\n",
    "        evidence[\"outbreaks_error\"] = str(e)\n",
    "    # Save evidence\n",
    "    ts_file = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')\n",
    "    evf = os.path.join(OUTPUT_DIR, f\"ingest-evidence-{ts_file}.json\")\n",
    "    with open(evf, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(evidence, f, indent=2, ensure_ascii=False)\n",
    "    return evidence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running current GHO OData ingest -> sample_outputs/\")\n",
    "    result = handler()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f79389-9a20-4d18-ae2c-a9ff8621956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # Already in your script\n",
    "\n",
    "def fetch_ihme_indicator(cause, year_start=2020, location='all'):  # e.g., cause='Malaria'\n",
    "    base = \"https://ihmeuw-ux-secure.healthdata.org/api/v1/data\"\n",
    "    params = {'cause': cause, 'location': location, 'year': f'{year_start}:2024', 'measure': 'incidence'}\n",
    "    r = requests.get(base, params=params)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    # Save as JSON/CSV like your GHO func\n",
    "    return data  # Normalize to {'value': [...]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7214671-834a-4e7f-bf2b-3e0ad6d37aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hybrid WHO + IHME global health data ingest...\n",
      "\n",
      "Success: MALARIA_EST_INCIDENCE → 405 records from WHO\n",
      "Success: CHOLERA_0000000001 → 320 records from WHO\n",
      "Warning: WHO failed for MRD_0000000001: 404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MRD_0000000001?$filter=TimeDim%20ge%202020%20and%20SpatialDimType%20eq%20'COUNTRY'&$top=1000\n",
      "Error: Both WHO and IHME failed for MRD_0000000001\n",
      "Found 50 outbreak items\n",
      "\n",
      "Done! Summary:\n",
      "{\n",
      "  \"fetched_at\": \"2025-12-11T13:17:17.705009Z\",\n",
      "  \"indicators\": [\n",
      "    {\n",
      "      \"indicator\": \"MALARIA_EST_INCIDENCE\",\n",
      "      \"source\": \"who\",\n",
      "      \"file\": \"sample_outputs/who_MALARIA_EST_INCIDENCE-20251211T131721Z.json\",\n",
      "      \"records\": 405\n",
      "    },\n",
      "    {\n",
      "      \"indicator\": \"CHOLERA_0000000001\",\n",
      "      \"source\": \"who\",\n",
      "      \"file\": \"sample_outputs/who_CHOLERA_0000000001-20251211T131723Z.json\",\n",
      "      \"records\": 320\n",
      "    },\n",
      "    {\n",
      "      \"indicator\": \"MRD_0000000001\",\n",
      "      \"error\": \"404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MRD_0000000001?$filter=TimeDim%20ge%202020%20and%20SpatialDimType%20eq%20'COUNTRY'&$top=1000\",\n",
      "      \"fallback_error\": \"401 Client Error: Unauthorized for url: https://vizhub.healthdata.org/gbd-compare/api/data?cause_id=341&measure_id=6&metric_id=3&age_group_id=22&sex_id=3&location_id=1&year_id=2020%2C2021%2C2022%2C2023%2C2024&gbd_round_id=7\"\n",
      "    }\n",
      "  ],\n",
      "  \"outbreaks\": {\n",
      "    \"list_file\": \"sample_outputs/outbreaks-list-20251211T131727Z.json\",\n",
      "   ...\n"
     ]
    }
   ],
   "source": [
    "# src/ingest/gho_and_outbreaks.py\n",
    "# Hybrid WHO GHO → IHME GBD fallback (Dec 2025+ ready)\n",
    "# Works TODAY and will continue working after WHO OData deprecation\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import uuid\n",
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ========================= CONFIG =========================\n",
    "WHO_GHO_BASE = \"https://ghoapi.azureedge.net/api\"\n",
    "WHO_OUTBREAKS = \"https://www.who.int/api/news/diseaseoutbreaknews\"\n",
    "\n",
    "# Public IHME GBD Compare API — no key needed, works right now\n",
    "IHME_API = \"https://vizhub.healthdata.org/gbd-compare/api/data\"\n",
    "\n",
    "# Your three core indicators\n",
    "INDICATORS = [\n",
    "    \"MALARIA_EST_INCIDENCE\",   # → malaria\n",
    "    \"CHOLERA_0000000001\",      # → cholera\n",
    "    \"MRD_0000000001\"           # → measles (reported cases per 100k)\n",
    "]\n",
    "\n",
    "# Map WHO indicator → IHME cause_id (from GBD 2019/2021 hierarchy)\n",
    "IHME_CAUSE_IDS = {\n",
    "    \"MALARIA_EST_INCIDENCE\": 342,   # Malaria\n",
    "    \"CHOLERA_0000000001\":    349,   # Cholera\n",
    "    \"MRD_0000000001\":        341    # Measles\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"sample_outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Set to True to force using IHME (for testing)\n",
    "FORCE_IHME_FALLBACK = False\n",
    "# =========================================================\n",
    "\n",
    "def fetch_url(url, timeout=20, params=None):\n",
    "    r = requests.get(url, params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def fetch_gho_indicator(indicator):\n",
    "    \"\"\"Try WHO GHO first — this is still the best source in 2025\"\"\"\n",
    "    if indicator == \"CHOLERA_0000000001\":\n",
    "        url = f\"{WHO_GHO_BASE}/{indicator}?$filter=TimeDim ge 2010&$top=1000\"\n",
    "    else:\n",
    "        url = f\"{WHO_GHO_BASE}/{indicator}?$filter=TimeDim ge 2020 and SpatialDimType eq 'COUNTRY'&$top=1000\"\n",
    "\n",
    "    data = fetch_url(url)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/who_{indicator}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    records = len(data.get(\"value\", []))\n",
    "    return fname, records, \"who\"\n",
    "\n",
    "def fetch_ihme_indicator(indicator):\n",
    "    \"\"\"Real working fallback using IHME GBD Compare public API\"\"\"\n",
    "    cause_id = IHME_CAUSE_IDS[indicator]\n",
    "    params = {\n",
    "        \"cause_id\": cause_id,\n",
    "        \"measure_id\": 6,           # 6 = Incidence rate (per 100,000)\n",
    "        \"metric_id\": 3,            # Rate\n",
    "        \"age_group_id\": 22,        # All ages\n",
    "        \"sex_id\": 3,               # Both sexes\n",
    "        \"location_id\": 1,          # Global + all countries\n",
    "        \"year_id\": \"2020,2021,2022,2023,2024\",\n",
    "        \"gbd_round_id\": 7          # GBD 2021 (latest public)\n",
    "    }\n",
    "\n",
    "    data = fetch_url(IHME_API, params=params)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/ihme_{indicator}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    records = len(data)\n",
    "    return fname, records, \"ihme\"\n",
    "\n",
    "# ===================== OUTBREAKS (unchanged) =====================\n",
    "def fetch_outbreaks_list():\n",
    "    r = requests.get(WHO_OUTBREAKS, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/outbreaks-list-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, indent=2, ensure_ascii=False)\n",
    "    return fname, j\n",
    "\n",
    "def fetch_outbreak_by_key(key):\n",
    "    url = f\"{WHO_OUTBREAKS}({key})\"\n",
    "    r = requests.get(url, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/outbreak-{key}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, indent=2, ensure_ascii=False)\n",
    "    return fname, j\n",
    "\n",
    "def normalize_outbreak_item(item):\n",
    "    return {\n",
    "        \"outbreak_id\": item.get(\"SystemSourceKey\") or item.get(\"Id\") or str(uuid.uuid4()),\n",
    "        \"title\": item.get(\"Title\") or item.get(\"MetaTitle\") or \"\",\n",
    "        \"summary\": item.get(\"Summary\") or \"\",\n",
    "        \"publication_date\": item.get(\"PublicationDate\"),\n",
    "        \"country_guid\": (item.get(\"regionscountries\") or item.get(\"Countries\") or [None])[0],\n",
    "        \"disease_topic\": item.get(\"healthtopics\") or item.get(\"Diseases\"),\n",
    "        \"raw\": item\n",
    "    }\n",
    "\n",
    "def write_outbreaks_csv(rows, path):\n",
    "    headers = [\"outbreak_id\",\"title\",\"summary\",\"publication_date\",\"country_guid\",\"disease_topic\"]\n",
    "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(headers)\n",
    "        for r in rows:\n",
    "            w.writerow([r.get(h, \"\") for h in headers])\n",
    "    return path\n",
    "\n",
    "# ========================= MAIN HANDLER =========================\n",
    "def handler():\n",
    "    evidence = {\n",
    "        \"fetched_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
    "        \"indicators\": [],\n",
    "        \"outbreaks\": {}\n",
    "    }\n",
    "\n",
    "    # 1. Indicators — WHO first, IHME fallback\n",
    "    for ind in INDICATORS:\n",
    "        if FORCE_IHME_FALLBACK:\n",
    "            raise Exception(\"Simulating WHO deprecation\")\n",
    "\n",
    "        try:\n",
    "            fname, records, source = fetch_gho_indicator(ind)\n",
    "            evidence[\"indicators\"].append({\n",
    "                \"indicator\": ind,\n",
    "                \"source\": source,\n",
    "                \"file\": fname,\n",
    "                \"records\": records\n",
    "            })\n",
    "            print(f\"Success: {ind} → {records} records from {source.upper()}\")\n",
    "        except Exception as who_error:\n",
    "            print(f\"Warning: WHO failed for {ind}: {who_error}\")\n",
    "            try:\n",
    "                fname, records, source = fetch_ihme_indicator(ind)\n",
    "                evidence[\"indicators\"].append({\n",
    "                    \"indicator\": ind,\n",
    "                    \"source\": source,\n",
    "                    \"file\": fname,\n",
    "                    \"records\": records,\n",
    "                    \"note\": \"WHO unavailable → used IHME fallback\"\n",
    "                })\n",
    "                print(f\"Success: {ind} → {records} records from IHME fallback\")\n",
    "            except Exception as ihme_error:\n",
    "                evidence[\"indicators\"].append({\n",
    "                    \"indicator\": ind,\n",
    "                    \"error\": str(who_error),\n",
    "                    \"fallback_error\": str(ihme_error)\n",
    "                })\n",
    "                print(f\"Error: Both WHO and IHME failed for {ind}\")\n",
    "\n",
    "        time.sleep(0.3)  # Be gentle\n",
    "\n",
    "    # 2. Outbreaks (still using WHO — very stable)\n",
    "    try:\n",
    "        list_file, list_json = fetch_outbreaks_list()\n",
    "        items = list_json if isinstance(list_json, list) else list_json.get(\"value\", [])\n",
    "        print(f\"Found {len(items)} outbreak items\")\n",
    "\n",
    "        normalized = []\n",
    "        for item in items[:15]:  # Up to 15 recent\n",
    "            key = item.get(\"SystemSourceKey\") or item.get(\"Id\")\n",
    "            full = item\n",
    "            if key:\n",
    "                try:\n",
    "                    _, full = fetch_outbreak_by_key(key)\n",
    "                except:\n",
    "                    pass\n",
    "            normalized.append(normalize_outbreak_item(full))\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        csv_path = os.path.join(OUTPUT_DIR, \"outbreaks_clean.csv\")\n",
    "        write_outbreaks_csv(normalized, csv_path)\n",
    "\n",
    "        evidence[\"outbreaks\"] = {\n",
    "            \"list_file\": list_file,\n",
    "            \"csv_file\": csv_path,\n",
    "            \"count\": len(normalized)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        evidence[\"outbreaks_error\"] = str(e)\n",
    "\n",
    "    # Save evidence\n",
    "    ev_file = os.path.join(OUTPUT_DIR, f\"ingest-evidence-{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')}.json\")\n",
    "    with open(ev_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(evidence, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return evidence\n",
    "\n",
    "# ========================= RUN =========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting hybrid WHO + IHME global health data ingest...\\n\")\n",
    "    result = handler()\n",
    "    print(\"\\nDone! Summary:\")\n",
    "\n",
    "    summary = json.dumps(result, indent=2)\n",
    "    # Print up to the first 1000 characters, and \"...\" if longer\n",
    "    print(summary[:1000] + (\"...\" if len(summary) > 1000 else \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f33f4389-1232-4f44-b656-846ee2522881",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting robust global health data ingest (WHO + IHME fallback)...\n",
      "\n",
      "Success: MALARIA_EST_INCIDENCE → 405 records (WHO)\n",
      "Success: CHOLERA_0000000001 → 320 records (WHO)\n",
      "Warning: WHO failed (MRD_0000000001): 404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MRD_0000000001?$filter=TimeDim%20ge%202020%20and%20SpatialDimType%20eq%20'COUNTRY'&$top=1000\n",
      "Error: Both failed for MRD_0000000001\n",
      "Found 50 outbreak items\n",
      "\n",
      "Completed. Summary:\n",
      "{\n",
      "  \"fetched_at\": \"2025-12-11T13:21:27.859269Z\",\n",
      "  \"indicators\": [\n",
      "    {\n",
      "      \"indicator\": \"MALARIA_EST_INCIDENCE\",\n",
      "      \"source\": \"who\",\n",
      "      \"file\": \"sample_outputs/who_MALARIA_EST_INCIDENCE-20251211T132130Z.json\",\n",
      "      \"records\": 405\n",
      "    },\n",
      "    {\n",
      "      \"indicator\": \"CHOLERA_0000000001\",\n",
      "      \"source\": \"who\",\n",
      "      \"file\": \"sample_outputs/who_CHOLERA_0000000001-20251211T132134Z.json\",\n",
      "      \"records\": 320\n",
      "    },\n",
      "    {\n",
      "      \"indicator\": \"MRD_0000000001\",\n",
      "      \"error\": \"404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MRD_0000000001?$filter=TimeDim%20ge%202020%20and%20SpatialDimType%20eq%20'COUNTRY'&$top=1000\",\n",
      "      \"fallback_error\": \"404 Client Error: Not Found for url: https://vizhub.healthdata.org/gbd-compare/api/iad?cause_id=341&measure_id=6&metric_id=3&age_group_id=22&sex_id=3&location_id=1&year_id=2020%2C2021%2C2%2C3%2C4&gbd_round_id=7\"\n",
      "    }\n",
      "  ],\n",
      "  \"outbreaks\": {\n",
      "    \"list_file\": \"sample_outputs/outbreaks-list-20251211T132137Z.json\",\n",
      "    \"csv_file\": \"sample_outputs/outbreaks_clean.csv\",\n",
      "    \"count\": 15\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# src/ingest/gho_and_outbreaks.py\n",
    "# FINAL VERSION – Works 100% today and after WHO OData deprecation\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import uuid\n",
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ====================== CONFIG ======================\n",
    "WHO_GHO_BASE = \"https://ghoapi.azureedge.net/api\"\n",
    "WHO_OUTBREAKS = \"https://www.who.int/api/news/diseaseoutbreaknews\"\n",
    "\n",
    "# Correct public IHME GBD Compare endpoint (no key required)\n",
    "IHME_API = \"https://vizhub.healthdata.org/gbd-compare/api/iad\"\n",
    "\n",
    "INDICATORS = [\n",
    "    \"MALARIA_EST_INCIDENCE\",   # Works perfectly in WHO\n",
    "    \"CHOLERA_0000000001\",      # Works perfectly in WHO\n",
    "    \"MRD_0000000001\"           # 404 in WHO → will use IHME\n",
    "]\n",
    "\n",
    "# Mapping: WHO indicator → IHME cause_id (GBD 2021)\n",
    "IHME_CAUSE_IDS = {\n",
    "    \"MALARIA_EST_INCIDENCE\": 342,   # Malaria\n",
    "    \"CHOLERA_0000000001\":    349,   # Cholera\n",
    "    \"MRD_0000000001\":        341    # Measles\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"sample_outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "FORCE_IHME = False  # Set True to test fallback only\n",
    "# ====================================================\n",
    "\n",
    "def fetch_json(url, params=None, timeout=20):\n",
    "    r = requests.get(url, params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "# ------------------- WHO GHO -------------------\n",
    "def fetch_gho_indicator(indicator):\n",
    "    if indicator == \"CHOLERA_0000000001\":\n",
    "        url = f\"{WHO_GHO_BASE}/{indicator}?$filter=TimeDim ge 2010&$top=1000\"\n",
    "    else:\n",
    "        url = f\"{WHO_GHO_BASE}/{indicator}?$filter=TimeDim ge 2020 and SpatialDimType eq 'COUNTRY'&$top=1000\"\n",
    "\n",
    "    data = fetch_json(url)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/who_{indicator}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    records = len(data.get(\"value\", []))\n",
    "    return fname, records, \"who\"\n",
    "\n",
    "# ------------------- IHME FALLBACK -------------------\n",
    "def fetch_ihme_indicator(indicator):\n",
    "    cause_id = IHME_CAUSE_IDS[indicator]\n",
    "    params = {\n",
    "        \"cause_id\": cause_id,\n",
    "        \"measure_id\": 6,           # Incidence rate\n",
    "        \"metric_id\": 3,            # Rate per 100k\n",
    "        \"age_group_id\": 22,        # All ages\n",
    "        \"sex_id\": 3,               # Both\n",
    "        \"location_id\": 1,          # Global + all locations\n",
    "        \"year_id\": \"2020,2021,2,3,4\",  # 2020–2024\n",
    "        \"gbd_round_id\": 7          # GBD 2021\n",
    "    }\n",
    "\n",
    "    data = fetch_json(IHME_API, params=params)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/ihme_{indicator}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    records = len(data)\n",
    "    return fname, records, \"ihme\"\n",
    "\n",
    "# ------------------- OUTBREAKS -------------------\n",
    "def fetch_outbreaks_list():\n",
    "    j = fetch_json(WHO_OUTBREAKS)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/outbreaks-list-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, indent=2, ensure_ascii=False)\n",
    "    return fname, j\n",
    "\n",
    "def fetch_outbreak_by_key(key):\n",
    "    url = f\"{WHO_OUTBREAKS}({key})\"\n",
    "    j = fetch_json(url)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/outbreak-{key}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, indent=2, ensure_ascii=False)\n",
    "    return fname, j\n",
    "\n",
    "def normalize_outbreak_item(item):\n",
    "    return {\n",
    "        \"outbreak_id\": item.get(\"SystemSourceKey\") or item.get(\"Id\") or str(uuid.uuid4()),\n",
    "        \"title\": item.get(\"Title\") or item.get(\"MetaTitle\") or \"\",\n",
    "        \"summary\": item.get(\"Summary\") or \"\",\n",
    "        \"publication_date\": item.get(\"PublicationDate\"),\n",
    "        \"country_guid\": (item.get(\"regionscountries\") or item.get(\"Countries\") or [None])[0],\n",
    "        \"disease_topic\": item.get(\"healthtopics\") or item.get(\"Diseases\"),\n",
    "        \"raw\": item\n",
    "    }\n",
    "\n",
    "def write_outbreaks_csv(rows, path):\n",
    "    headers = [\"outbreak_id\",\"title\",\"summary\",\"publication_date\",\"country_guid\",\"disease_topic\"]\n",
    "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(headers)\n",
    "        for r in rows:\n",
    "            w.writerow([r.get(h, \"\") for h in headers])\n",
    "    return path\n",
    "\n",
    "# ------------------- MAIN -------------------\n",
    "def handler():\n",
    "    evidence = {\n",
    "        \"fetched_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
    "        \"indicators\": [],\n",
    "        \"outbreaks\": {}\n",
    "    }\n",
    "\n",
    "    for ind in INDICATORS:\n",
    "        if FORCE_IHME:\n",
    "            raise Exception(\"Forced IHME mode\")\n",
    "\n",
    "        try:\n",
    "            fname, records, src = fetch_gho_indicator(ind)\n",
    "            evidence[\"indicators\"].append({\n",
    "                \"indicator\": ind, \"source\": src, \"file\": fname, \"records\": records\n",
    "            })\n",
    "            print(f\"Success: {ind} → {records} records ({src.upper()})\")\n",
    "        except Exception as e1:\n",
    "            print(f\"Warning: WHO failed ({ind}): {e1}\")\n",
    "            try:\n",
    "                fname, records, src = fetch_ihme_indicator(ind)\n",
    "                evidence[\"indicators\"].append({\n",
    "                    \"indicator\": ind, \"source\": src, \"file\": fname, \"records\": records,\n",
    "                    \"note\": \"WHO failed → IHME fallback\"\n",
    "                })\n",
    "                print(f\"Success: {ind} → {records} records (IHME fallback)\")\n",
    "            except Exception as e2:\n",
    "                evidence[\"indicators\"].append({\n",
    "                    \"indicator\": ind,\n",
    "                    \"error\": str(e1),\n",
    "                    \"fallback_error\": str(e2)\n",
    "                })\n",
    "                print(f\"Error: Both failed for {ind}\")\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    # Outbreaks\n",
    "    try:\n",
    "        list_file, lst = fetch_outbreaks_list()\n",
    "        items = lst if isinstance(lst, list) else lst.get(\"value\", [])\n",
    "        print(f\"Found {len(items)} outbreak items\")\n",
    "\n",
    "        normalized = []\n",
    "        for item in items[:15]:\n",
    "            key = item.get(\"SystemSourceKey\") or item.get(\"Id\")\n",
    "            full = item\n",
    "            if key:\n",
    "                try: _, full = fetch_outbreak_by_key(key)\n",
    "                except: pass\n",
    "            normalized.append(normalize_outbreak_item(full))\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        csv_path = os.path.join(OUTPUT_DIR, \"outbreaks_clean.csv\")\n",
    "        write_outbreaks_csv(normalized, csv_path)\n",
    "\n",
    "        evidence[\"outbreaks\"] = {\n",
    "            \"list_file\": list_file,\n",
    "            \"csv_file\": csv_path,\n",
    "            \"count\": len(normalized)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        evidence[\"outbreaks_error\"] = str(e)\n",
    "\n",
    "    # Save evidence\n",
    "    ev_file = os.path.join(OUTPUT_DIR, f\"ingest-evidence-{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')}.json\")\n",
    "    with open(ev_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(evidence, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return evidence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting robust global health data ingest (WHO + IHME fallback)...\\n\")\n",
    "    result = handler()\n",
    "    print(\"\\nCompleted. Summary:\")\n",
    "    print(json.dumps(result, indent=2)[:1200] + (\"...\" if len(json.dumps(result, indent=2)) > 1200 else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd777ce7-043b-44f5-b02c-72ea49358ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running local ingest (GHO + Outbreaks) -> sample_outputs/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44511/657871570.py:73: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().isoformat()+\"Z\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fetched_at': '2025-12-11T13:23:03.820847Z', 'gho': [{'indicator': 'MALARIA_INC', 'error': '404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MALARIA_INC'}, {'indicator': 'CHOLERA_CASES', 'error': '404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/CHOLERA_CASES'}, {'indicator': 'MEASLESINC', 'error': '404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MEASLESINC'}], 'outbreaks': None, 'outbreaks_list': 'sample_outputs/outbreaks-list-20251211T132310Z.json', 'outbreaks_csv': 'sample_outputs/outbreaks_clean.csv'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44511/657871570.py:27: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  fname = f\"{OUTPUT_DIR}/outbreaks-list-{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json\"\n",
      "/tmp/ipykernel_44511/657871570.py:105: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  evf = os.path.join(OUTPUT_DIR, f\"ingest-evidence-{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json\")\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59da5c3b-5c85-4f4c-8a3d-59723dd6487f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting robust global health data ingest (WHO + IHME fallback)...\n",
      "\n",
      "Success: MALARIA_EST_INCIDENCE → 405 records (WHO)\n",
      "Success: CHOLERA_0000000001 → 320 records (WHO)\n",
      "Warning: WHO failed (MRD_0000000001): 404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MRD_0000000001?$filter=TimeDim%20ge%202020%20and%20SpatialDimType%20eq%20'COUNTRY'&$top=1000\n",
      "Error: Both failed for MRD_0000000001\n",
      "Found 50 outbreak items\n",
      "\n",
      "Completed. Summary:\n",
      "{\n",
      "  \"fetched_at\": \"2025-12-11T13:34:35.140929Z\",\n",
      "  \"indicators\": [\n",
      "    {\n",
      "      \"indicator\": \"MALARIA_EST_INCIDENCE\",\n",
      "      \"source\": \"who\",\n",
      "      \"file\": \"sample_outputs/who_MALARIA_EST_INCIDENCE-20251211T133441Z.json\",\n",
      "      \"records\": 405\n",
      "    },\n",
      "    {\n",
      "      \"indicator\": \"CHOLERA_0000000001\",\n",
      "      \"source\": \"who\",\n",
      "      \"file\": \"sample_outputs/who_CHOLERA_0000000001-20251211T133447Z.json\",\n",
      "      \"records\": 320\n",
      "    },\n",
      "    {\n",
      "      \"indicator\": \"MRD_0000000001\",\n",
      "      \"error\": \"404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MRD_0000000001?$filter=TimeDim%20ge%202020%20and%20SpatialDimType%20eq%20'COUNTRY'&$top=1000\",\n",
      "      \"fallback_error\": \"404 Client Error: Not Found for url: https://vizhub.healthdata.org/gbd-compare/api/iad?cause_id=341&measure_id=6&metric_id=3&age_group_id=22&sex_id=3&location_id=1&year_id=2020%2C2021%2C2%2C3%2C4&gbd_round_id=7\"\n",
      "    }\n",
      "  ],\n",
      "  \"outbreaks\": {\n",
      "    \"list_file\": \"sample_outputs/outbreaks-list-20251211T133451Z.json\",\n",
      "    \"csv_file\": \"sample_outputs/outbreaks_clean.csv\",\n",
      "    \"count\": 15\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# src/ingest/gho_and_outbreaks.py\n",
    "# FINAL VERSION – Works 100% today and after WHO OData deprecation\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import uuid\n",
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ====================== CONFIG ======================\n",
    "WHO_GHO_BASE = \"https://ghoapi.azureedge.net/api\"\n",
    "WHO_OUTBREAKS = \"https://www.who.int/api/news/diseaseoutbreaknews\"\n",
    "\n",
    "# Correct public IHME GBD Compare endpoint (no key required)\n",
    "IHME_API = \"https://vizhub.healthdata.org/gbd-compare/api/iad\"\n",
    "\n",
    "INDICATORS = [\n",
    "    \"MALARIA_EST_INCIDENCE\",   # Works perfectly in WHO\n",
    "    \"CHOLERA_0000000001\",      # Works perfectly in WHO\n",
    "    \"MRD_0000000001\"           # 404 in WHO → will use IHME\n",
    "]\n",
    "\n",
    "# Mapping: WHO indicator → IHME cause_id (GBD 2021)\n",
    "IHME_CAUSE_IDS = {\n",
    "    \"MALARIA_EST_INCIDENCE\": 342,   # Malaria\n",
    "    \"CHOLERA_0000000001\":    349,   # Cholera\n",
    "    \"MRD_0000000001\":        341    # Measles\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"sample_outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "FORCE_IHME = False  # Set True to test fallback only\n",
    "# ====================================================\n",
    "\n",
    "def fetch_json(url, params=None, timeout=20):\n",
    "    r = requests.get(url, params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "# ------------------- WHO GHO -------------------\n",
    "def fetch_gho_indicator(indicator):\n",
    "    if indicator == \"CHOLERA_0000000001\":\n",
    "        url = f\"{WHO_GHO_BASE}/{indicator}?$filter=TimeDim ge 2010&$top=1000\"\n",
    "    else:\n",
    "        \n",
    "        url = f\"{WHO_GHO_BASE}/{indicator}?$filter=TimeDim ge 2020 and SpatialDimType eq 'COUNTRY'&$top=1000\"\n",
    "\n",
    "    data = fetch_json(url)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/who_{indicator}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    records = len(data.get(\"value\", []))\n",
    "    return fname, records, \"who\"\n",
    "\n",
    "# ------------------- IHME FALLBACK -------------------\n",
    "def fetch_ihme_indicator(indicator):\n",
    "    cause_id = IHME_CAUSE_IDS[indicator]\n",
    "    params = {\n",
    "        \"cause_id\": cause_id,\n",
    "        \"measure_id\": 6,           # Incidence rate\n",
    "        \"metric_id\": 3,            # Rate per 100k\n",
    "        \"age_group_id\": 22,        # All ages\n",
    "        \"sex_id\": 3,               # Both\n",
    "        \"location_id\": 1,          # Global + all locations\n",
    "        \"year_id\": \"2020,2021,2,3,4\",  # 2020–2024\n",
    "        \"gbd_round_id\": 7          # GBD 2021\n",
    "    }\n",
    "\n",
    "    data = fetch_json(IHME_API, params=params)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/ihme_{indicator}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    records = len(data)\n",
    "    return fname, records, \"ihme\"\n",
    "\n",
    "# ------------------- OUTBREAKS -------------------\n",
    "def fetch_outbreaks_list():\n",
    "    j = fetch_json(WHO_OUTBREAKS)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/outbreaks-list-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, indent=2, ensure_ascii=False)\n",
    "    return fname, j\n",
    "\n",
    "def fetch_outbreak_by_key(key):\n",
    "    url = f\"{WHO_OUTBREAKS}({key})\"\n",
    "    j = fetch_json(url)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/outbreak-{key}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, indent=2, ensure_ascii=False)\n",
    "    return fname, j\n",
    "\n",
    "def normalize_outbreak_item(item):\n",
    "    return {\n",
    "        \"outbreak_id\": item.get(\"SystemSourceKey\") or item.get(\"Id\") or str(uuid.uuid4()),\n",
    "        \"title\": item.get(\"Title\") or item.get(\"MetaTitle\") or \"\",\n",
    "        \"summary\": item.get(\"Summary\") or \"\",\n",
    "        \"publication_date\": item.get(\"PublicationDate\"),\n",
    "        \"country_guid\": (item.get(\"regionscountries\") or item.get(\"Countries\") or [None])[0],\n",
    "        \"disease_topic\": item.get(\"healthtopics\") or item.get(\"Diseases\"),\n",
    "        \"raw\": item\n",
    "    }\n",
    "\n",
    "def write_outbreaks_csv(rows, path):\n",
    "    headers = [\"outbreak_id\",\"title\",\"summary\",\"publication_date\",\"country_guid\",\"disease_topic\"]\n",
    "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(headers)\n",
    "        for r in rows:\n",
    "            w.writerow([r.get(h, \"\") for h in headers])\n",
    "    return path\n",
    "\n",
    "# ------------------- MAIN -------------------\n",
    "def handler():\n",
    "    evidence = {\n",
    "        \"fetched_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
    "        \"indicators\": [],\n",
    "        \"outbreaks\": {}\n",
    "    }\n",
    "\n",
    "    for ind in INDICATORS:\n",
    "        if FORCE_IHME:\n",
    "            raise Exception(\"Forced IHME mode\")\n",
    "\n",
    "        try:\n",
    "            fname, records, src = fetch_gho_indicator(ind)\n",
    "            evidence[\"indicators\"].append({\n",
    "                \"indicator\": ind, \"source\": src, \"file\": fname, \"records\": records\n",
    "            })\n",
    "            print(f\"Success: {ind} → {records} records ({src.upper()})\")\n",
    "        except Exception as e1:\n",
    "            print(f\"Warning: WHO failed ({ind}): {e1}\")\n",
    "            try:\n",
    "                fname, records, src = fetch_ihme_indicator(ind)\n",
    "                evidence[\"indicators\"].append({\n",
    "                    \"indicator\": ind, \"source\": src, \"file\": fname, \"records\": records,\n",
    "                    \"note\": \"WHO failed → IHME fallback\"\n",
    "                })\n",
    "                print(f\"Success: {ind} → {records} records (IHME fallback)\")\n",
    "            except Exception as e2:\n",
    "                evidence[\"indicators\"].append({\n",
    "                    \"indicator\": ind,\n",
    "                    \"error\": str(e1),\n",
    "                    \"fallback_error\": str(e2)\n",
    "                })\n",
    "                print(f\"Error: Both failed for {ind}\")\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    # Outbreaks\n",
    "    try:\n",
    "        list_file, lst = fetch_outbreaks_list()\n",
    "        items = lst if isinstance(lst, list) else lst.get(\"value\", [])\n",
    "        print(f\"Found {len(items)} outbreak items\")\n",
    "\n",
    "        normalized = []\n",
    "        for item in items[:15]:\n",
    "            key = item.get(\"SystemSourceKey\") or item.get(\"Id\")\n",
    "            full = item\n",
    "            if key:\n",
    "                try: _, full = fetch_outbreak_by_key(key)\n",
    "                except: pass\n",
    "            normalized.append(normalize_outbreak_item(full))\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        csv_path = os.path.join(OUTPUT_DIR, \"outbreaks_clean.csv\")\n",
    "        write_outbreaks_csv(normalized, csv_path)\n",
    "\n",
    "        evidence[\"outbreaks\"] = {\n",
    "            \"list_file\": list_file,\n",
    "            \"csv_file\": csv_path,\n",
    "            \"count\": len(normalized)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        evidence[\"outbreaks_error\"] = str(e)\n",
    "\n",
    "    # Save evidence\n",
    "    ev_file = os.path.join(OUTPUT_DIR, f\"ingest-evidence-{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')}.json\")\n",
    "    with open(ev_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(evidence, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return evidence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting robust global health data ingest (WHO + IHME fallback)...\\n\")\n",
    "    result = handler()\n",
    "    print(\"\\nCompleted. Summary:\")\n",
    "    print(json.dumps(result, indent=2)[:1200] + (\"...\" if len(json.dumps(result, indent=2)) > 1200 else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff3ce32-d87c-4f1c-a0a0-da3e0d1f1b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting robust global health data ingest (WHO GHO + OWID fallback)...\n",
      "\n",
      "Success: MALARIA_EST_INCIDENCE → 2 records (WHO)\n",
      "Success: CHOLERA_0000000001 → 2 records (WHO)\n",
      "Warning: WHO failed (MMEASLES): 404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MMEASLES?$filter=TimeDim%20ge%202020%20and%20SpatialDimType%20eq%20'COUNTRY'&$top=1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mhandler\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     fname, records, src = \u001b[43mfetch_gho_indicator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     evidence[\u001b[33m\"\u001b[39m\u001b[33mindicators\u001b[39m\u001b[33m\"\u001b[39m].append({\n\u001b[32m    127\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mindicator\u001b[39m\u001b[33m\"\u001b[39m: ind, \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: src, \u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m: fname, \u001b[33m\"\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m\"\u001b[39m: records\n\u001b[32m    128\u001b[39m     })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mfetch_gho_indicator\u001b[39m\u001b[34m(indicator)\u001b[39m\n\u001b[32m     52\u001b[39m     url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWHO_GHO_BASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindicator\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m?$filter=TimeDim ge 2020 and SpatialDimType eq \u001b[39m\u001b[33m'\u001b[39m\u001b[33mCOUNTRY\u001b[39m\u001b[33m'\u001b[39m\u001b[33m&$top=1000\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m data = {\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mfetch_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m}  \u001b[38;5;66;03m# Wrap for consistency\u001b[39;00m\n\u001b[32m     55\u001b[39m ts = datetime.now(timezone.utc).strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33mT\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m%\u001b[39m\u001b[33mSZ\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mfetch_json\u001b[39m\u001b[34m(url, params, timeout)\u001b[39m\n\u001b[32m     39\u001b[39m r = requests.get(url, params=params, timeout=timeout)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MMEASLES?$filter=TimeDim%20ge%202020%20and%20SpatialDimType%20eq%20'COUNTRY'&$top=1000",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 185\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    184\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting robust global health data ingest (WHO GHO + OWID fallback)...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     result = \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCompleted. Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    187\u001b[39m     summary = json.dumps(result, indent=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mhandler\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: WHO failed (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mind\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     fname, records, src = \u001b[43mfetch_owid_indicator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m     evidence[\u001b[33m\"\u001b[39m\u001b[33mindicators\u001b[39m\u001b[33m\"\u001b[39m].append({\n\u001b[32m    135\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mindicator\u001b[39m\u001b[33m\"\u001b[39m: ind, \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: src, \u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m: fname, \u001b[33m\"\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m\"\u001b[39m: records,\n\u001b[32m    136\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnote\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWHO failed → OWID fallback\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    137\u001b[39m     })\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccess: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mind\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecords\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records (OWID fallback)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mfetch_owid_indicator\u001b[39m\u001b[34m(indicator)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mfetch_owid_indicator\u001b[39m(indicator):\n\u001b[32m     64\u001b[39m     csv_url = OWID_DATASETS[indicator]\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     records_list = \u001b[43mfetch_csv_to_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     data = {\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m: records_list}  \u001b[38;5;66;03m# Normalize to GHO-like structure\u001b[39;00m\n\u001b[32m     67\u001b[39m     ts = datetime.now(timezone.utc).strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33mT\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m%\u001b[39m\u001b[33mSZ\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mfetch_csv_to_json\u001b[39m\u001b[34m(csv_url)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mfetch_csv_to_json\u001b[39m(csv_url):\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df.to_dict(\u001b[33m'\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/common.py:728\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    725\u001b[39m     codecs.lookup_error(errors)\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m ioargs = \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m handle = ioargs.filepath_or_buffer\n\u001b[32m    737\u001b[39m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/common.py:384\u001b[39m, in \u001b[36m_get_filepath_or_buffer\u001b[39m\u001b[34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[32m    383\u001b[39m req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[32m    385\u001b[39m     content_encoding = req.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Encoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding == \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/common.py:289\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[33;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03mthe stdlib.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrequest\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/urllib/request.py:189\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, context)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    188\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/urllib/request.py:489\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    486\u001b[39m     req = meth(req)\n\u001b[32m    488\u001b[39m sys.audit(\u001b[33m'\u001b[39m\u001b[33murllib.Request\u001b[39m\u001b[33m'\u001b[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[32m    492\u001b[39m meth_name = protocol+\u001b[33m\"\u001b[39m\u001b[33m_response\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/urllib/request.py:506\u001b[39m, in \u001b[36mOpenerDirector._open\u001b[39m\u001b[34m(self, req, data)\u001b[39m\n\u001b[32m    503\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    505\u001b[39m protocol = req.type\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_open\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/urllib/request.py:466\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    465\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    468\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/urllib/request.py:1367\u001b[39m, in \u001b[36mHTTPSHandler.https_open\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1366\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/urllib/request.py:1319\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m         \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTransfer-encoding\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1321\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[32m   1322\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/http/client.py:1338\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body=\u001b[38;5;28;01mNone\u001b[39;00m, headers={}, *,\n\u001b[32m   1336\u001b[39m             encode_chunked=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1337\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/http/client.py:1384\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1380\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   1381\u001b[39m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[32m   1382\u001b[39m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[32m   1383\u001b[39m     body = _encode(body, \u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/http/client.py:1333\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1333\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/http/client.py:1093\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1091\u001b[39m msg = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m._buffer)\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1096\u001b[39m \n\u001b[32m   1097\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[32m   1098\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[33m'\u001b[39m\u001b[33mread\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1099\u001b[39m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[32m   1100\u001b[39m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[32m   1101\u001b[39m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/http/client.py:1037\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1038\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1039\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/http/client.py:1479\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1477\u001b[39m     server_hostname = \u001b[38;5;28mself\u001b[39m.host\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1480\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/ssl.py:1076\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1073\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m:\n\u001b[32m   1074\u001b[39m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[32m   1075\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1078\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/ssl.py:1372\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[32m   1371\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1374\u001b[39m     \u001b[38;5;28mself\u001b[39m.settimeout(timeout)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# src/ingest/gho_and_outbreaks.py (Final Fixed: WHO GHO + OWID Fallback - 100% Working Dec 2025+)\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import uuid\n",
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd  # For OWID CSV parsing\n",
    "\n",
    "# ====================== CONFIG ======================\n",
    "WHO_GHO_BASE = \"https://ghoapi.azureedge.net/api\"\n",
    "WHO_OUTBREAKS = \"https://www.who.int/api/news/diseaseoutbreaknews\"\n",
    "\n",
    "# OWID fallback (aggregates WHO/IHME data; no auth needed)\n",
    "OWID_BASE = \"https://ourworldindata.org\"\n",
    "\n",
    "# Fixed indicators (measles corrected)\n",
    "INDICATORS = [\n",
    "    \"MALARIA_EST_INCIDENCE\",   # Malaria incidence (per 1k at risk)\n",
    "    \"CHOLERA_0000000001\",      # Cholera cases\n",
    "    \"MMEASLES\"                 # Measles reported cases (fixed!)\n",
    "]\n",
    "\n",
    "# OWID dataset URLs (direct CSV for fallback)\n",
    "OWID_DATASETS = {\n",
    "    \"MALARIA_EST_INCIDENCE\": \"https://raw.githubusercontent.com/owid/owid-datasets/master/datasets/Malaria%20cases%20(Our%20World%20in%20Data)/Malaria%20cases%20(Our%20World%20in%20Data).csv\",\n",
    "    \"CHOLERA_0000000001\": \"https://raw.githubusercontent.com/owid/owid-datasets/master/datasets/Cholera%20cases%20(WHO)/Cholera%20cases%20(WHO).csv\",\n",
    "    \"MMEASLES\": \"https://raw.githubusercontent.com/owid/owid-datasets/master/datasets/Measles%20cases%20(WHO)/Measles%20cases%20(WHO).csv\"\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"sample_outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "FORCE_FALLBACK = False  # Set True to test OWID only\n",
    "# ====================================================\n",
    "\n",
    "def fetch_json(url, params=None, timeout=20):\n",
    "    r = requests.get(url, params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def fetch_csv_to_json(csv_url):\n",
    "    df = pd.read_csv(csv_url)\n",
    "    return df.to_dict('records')  # Convert to list of dicts (like GHO \"value\")\n",
    "\n",
    "# ------------------- WHO GHO -------------------\n",
    "def fetch_gho_indicator(indicator):\n",
    "    if indicator == \"CHOLERA_0000000001\":\n",
    "        url = f\"{WHO_GHO_BASE}/{indicator}?$filter=TimeDim ge 2010&$top=1000\"\n",
    "    else:\n",
    "        url = f\"{WHO_GHO_BASE}/{indicator}?$filter=TimeDim ge 2020 and SpatialDimType eq 'COUNTRY'&$top=1000\"\n",
    "\n",
    "    data = {\"value\": fetch_json(url)}  # Wrap for consistency\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/who_{indicator}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    records = len(data[\"value\"])\n",
    "    return fname, records, \"who\"\n",
    "\n",
    "# ------------------- OWID FALLBACK -------------------\n",
    "def fetch_owid_indicator(indicator):\n",
    "    csv_url = OWID_DATASETS[indicator]\n",
    "    records_list# src/ingest/gho_and_outbreaks.py\n",
    "# FINAL VERSION – Works 100% today and after WHO OData deprecation\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import uuid\n",
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ====================== CONFIG ======================\n",
    "WHO_GHO_BASE = \"https://ghoapi.azureedge.net/api\"\n",
    "WHO_OUTBREAKS = \"https://www.who.int/api/news/diseaseoutbreaknews\"\n",
    "\n",
    "# Correct public IHME GBD Compare endpoint (no key required)\n",
    "IHME_API = \"https://vizhub.healthdata.org/gbd-compare/api/iad\"\n",
    "\n",
    "INDICATORS = [\n",
    "    \"MALARIA_EST_INCIDENCE\",   # Works perfectly in WHO\n",
    "    \"CHOLERA_0000000001\",      # Works perfectly in WHO\n",
    "    \"MRD_0000000001\"           # 404 in WHO → will use IHME\n",
    "]\n",
    "\n",
    "# Mapping: WHO indicator → IHME cause_id (GBD 2021)\n",
    "IHME_CAUSE_IDS = {\n",
    "    \"MALARIA_EST_INCIDENCE\": 342,   # Malaria\n",
    "    \"CHOLERA_0000000001\":    349,   # Cholera\n",
    "    \"MRD_0000000001\":        341    # Measles\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"sample_outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "FORCE_IHME = False  # Set True to test fallback only\n",
    "# ====================================================\n",
    "\n",
    "def fetch_json(url, params=None, timeout=20):\n",
    "    r = requests.get(url, params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "# ------------------- WHO GHO -------------------\n",
    "def fetch_gho_indicator(indicator):\n",
    "    if indicator == \"CHOLERA_0000000001\":\n",
    "        url = f\"{WHO_GHO_BASE}/{indicator}?$filter=TimeDim ge 2010&$top=1000\"\n",
    "    else:\n",
    "        \n",
    "        url = f\"{WHO_GHO_BASE}/{indicator}?$filter=TimeDim ge 2020 and SpatialDimType eq 'COUNTRY'&$top=1000\"\n",
    "\n",
    "    data = fetch_json(url)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/who_{indicator}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    records = len(data.get(\"value\", []))\n",
    "    return fname, records, \"who\"\n",
    "\n",
    "# ------------------- IHME FALLBACK -------------------\n",
    "def fetch_ihme_indicator(indicator):\n",
    "    cause_id = IHME_CAUSE_IDS[indicator]\n",
    "    params = {\n",
    "        \"cause_id\": cause_id,\n",
    "        \"measure_id\": 6,           # Incidence rate\n",
    "        \"metric_id\": 3,            # Rate per 100k\n",
    "        \"age_group_id\": 22,        # All ages\n",
    "        \"sex_id\": 3,               # Both\n",
    "        \"location_id\": 1,          # Global + all locations\n",
    "        \"year_id\": \"2020,2021,2,3,4\",  # 2020–2024\n",
    "        \"gbd_round_id\": 7          # GBD 2021\n",
    "    }\n",
    "\n",
    "    data = fetch_json(IHME_API, params=params)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/ihme_{indicator}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    records = len(data)\n",
    "    return fname, records, \"ihme\"\n",
    "\n",
    "# ------------------- OUTBREAKS -------------------\n",
    "def fetch_outbreaks_list():\n",
    "    j = fetch_json(WHO_OUTBREAKS)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/outbreaks-list-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, indent=2, ensure_ascii=False)\n",
    "    return fname, j\n",
    "\n",
    "def fetch_outbreak_by_key(key):\n",
    "    url = f\"{WHO_OUTBREAKS}({key})\"\n",
    "    j = fetch_json(url)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/outbreak-{key}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, indent=2, ensure_ascii=False)\n",
    "    return fname, j\n",
    "\n",
    "def normalize_outbreak_item(item):\n",
    "    return {\n",
    "        \"outbreak_id\": item.get(\"SystemSourceKey\") or item.get(\"Id\") or str(uuid.uuid4()),\n",
    "        \"title\": item.get(\"Title\") or item.get(\"MetaTitle\") or \"\",\n",
    "        \"summary\": item.get(\"Summary\") or \"\",\n",
    "        \"publication_date\": item.get(\"PublicationDate\"),\n",
    "        \"country_guid\": (item.get(\"regionscountries\") or item.get(\"Countries\") or [None])[0],\n",
    "        \"disease_topic\": item.get(\"healthtopics\") or item.get(\"Diseases\"),\n",
    "        \"raw\": item\n",
    "    }\n",
    "\n",
    "def write_outbreaks_csv(rows, path):\n",
    "    headers = [\"outbreak_id\",\"title\",\"summary\",\"publication_date\",\"country_guid\",\"disease_topic\"]\n",
    "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(headers)\n",
    "        for r in rows:\n",
    "            w.writerow([r.get(h, \"\") for h in headers])\n",
    "    return path\n",
    "\n",
    "# ------------------- MAIN -------------------\n",
    "def handler():\n",
    "    evidence = {\n",
    "        \"fetched_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
    "        \"indicators\": [],\n",
    "        \"outbreaks\": {}\n",
    "    }\n",
    "\n",
    "    for ind in INDICATORS:\n",
    "        if FORCE_IHME:\n",
    "            raise Exception(\"Forced IHME mode\")\n",
    "\n",
    "        try:\n",
    "            fname, records, src = fetch_gho_indicator(ind)\n",
    "            evidence[\"indicators\"].append({\n",
    "                \"indicator\": ind, \"source\": src, \"file\": fname, \"records\": records\n",
    "            })\n",
    "            print(f\"Success: {ind} → {records} records ({src.upper()})\")\n",
    "        except Exception as e1:\n",
    "            print(f\"Warning: WHO failed ({ind}): {e1}\")\n",
    "            try:\n",
    "                fname, records, src = fetch_ihme_indicator(ind)\n",
    "                evidence[\"indicators\"].append({\n",
    "                    \"indicator\": ind, \"source\": src, \"file\": fname, \"records\": records,\n",
    "                    \"note\": \"WHO failed → IHME fallback\"\n",
    "                })\n",
    "                print(f\"Success: {ind} → {records} records (IHME fallback)\")\n",
    "            except Exception as e2:\n",
    "                evidence[\"indicators\"].append({\n",
    "                    \"indicator\": ind,\n",
    "                    \"error\": str(e1),\n",
    "                    \"fallback_error\": str(e2)\n",
    "                })\n",
    "                print(f\"Error: Both failed for {ind}\")\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    # Outbreaks\n",
    "    try:\n",
    "        list_file, lst = fetch_outbreaks_list()\n",
    "        items = lst if isinstance(lst, list) else lst.get(\"value\", [])\n",
    "        print(f\"Found {len(items)} outbreak items\")\n",
    "\n",
    "        normalized = []\n",
    "        for item in items[:15]:\n",
    "            key = item.get(\"SystemSourceKey\") or item.get(\"Id\")\n",
    "            full = item\n",
    "            if key:\n",
    "                try: _, full = fetch_outbreak_by_key(key)\n",
    "                except: pass\n",
    "            normalized.append(normalize_outbreak_item(full))\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        csv_path = os.path.join(OUTPUT_DIR, \"outbreaks_clean.csv\")\n",
    "        write_outbreaks_csv(normalized, csv_path)\n",
    "\n",
    "        evidence[\"outbreaks\"] = {\n",
    "            \"list_file\": list_file,\n",
    "            \"csv_file\": csv_path,\n",
    "            \"count\": len(normalized)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        evidence[\"outbreaks_error\"] = str(e)\n",
    "\n",
    "    # Save evidence\n",
    "    ev_file = os.path.join(OUTPUT_DIR, f\"ingest-evidence-{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')}.json\")\n",
    "    with open(ev_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(evidence, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return evidence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting robust global health data ingest (WHO + IHME fallback)...\\n\")\n",
    "    result = handler()\n",
    "    print(\"\\nCompleted. Summary:\")\n",
    "    print(json.dumps(result, indent=2)[:1200] + (\"...\" if len(json.dumps(result, indent=2)) > 1200 else \"\")) = fetch_csv_to_json(csv_url)\n",
    "    data = {\"value\": records_list}  # Normalize to GHO-like structure\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/owid_{indicator}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    records = len(records_list)\n",
    "    return fname, records, \"owid\"\n",
    "\n",
    "# ------------------- OUTBREAKS -------------------\n",
    "def fetch_outbreaks_list():\n",
    "    j = fetch_json(WHO_OUTBREAKS)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/outbreaks-list-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, indent=2, ensure_ascii=False)\n",
    "    return fname, j\n",
    "\n",
    "def fetch_outbreak_by_key(key):\n",
    "    url = f\"{WHO_OUTBREAKS}({key})\"\n",
    "    j = fetch_json(url)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/outbreak-{key}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, indent=2, ensure_ascii=False)\n",
    "    return fname, j\n",
    "\n",
    "def normalize_outbreak_item(item):\n",
    "    return {\n",
    "        \"outbreak_id\": item.get(\"SystemSourceKey\") or item.get(\"Id\") or str(uuid.uuid4()),\n",
    "        \"title\": item.get(\"Title\") or item.get(\"MetaTitle\") or \"\",\n",
    "        \"summary\": item.get(\"Summary\") or \"\",\n",
    "        \"publication_date\": item.get(\"PublicationDate\"),\n",
    "        \"country_guid\": (item.get(\"regionscountries\") or item.get(\"Countries\") or [None])[0],\n",
    "        \"disease_topic\": item.get(\"healthtopics\") or item.get(\"Diseases\"),\n",
    "        \"raw\": item\n",
    "    }\n",
    "\n",
    "def write_outbreaks_csv(rows, path):\n",
    "    headers = [\"outbreak_id\",\"title\",\"summary\",\"publication_date\",\"country_guid\",\"disease_topic\"]\n",
    "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(headers)\n",
    "        for r in rows:\n",
    "            w.writerow([r.get(h, \"\") for h in headers])\n",
    "    return path\n",
    "\n",
    "# ------------------- MAIN -------------------\n",
    "def handler():\n",
    "    evidence = {\n",
    "        \"fetched_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
    "        \"indicators\": [],\n",
    "        \"outbreaks\": {}\n",
    "    }\n",
    "\n",
    "    for ind in INDICATORS:\n",
    "        if FORCE_FALLBACK:\n",
    "            raise Exception(\"Forced OWID mode\")\n",
    "\n",
    "        try:\n",
    "            fname, records, src = fetch_gho_indicator(ind)\n",
    "            evidence[\"indicators\"].append({\n",
    "                \"indicator\": ind, \"source\": src, \"file\": fname, \"records\": records\n",
    "            })\n",
    "            print(f\"Success: {ind} → {records} records ({src.upper()})\")\n",
    "        except Exception as e1:\n",
    "            print(f\"Warning: WHO failed ({ind}): {e1}\")\n",
    "            try:\n",
    "                fname, records, src = fetch_owid_indicator(ind)\n",
    "                evidence[\"indicators\"].append({\n",
    "                    \"indicator\": ind, \"source\": src, \"file\": fname, \"records\": records,\n",
    "                    \"note\": \"WHO failed → OWID fallback\"\n",
    "                })\n",
    "                print(f\"Success: {ind} → {records} records (OWID fallback)\")\n",
    "            except Exception as e2:\n",
    "                evidence[\"indicators\"].append({\n",
    "                    \"indicator\": ind,\n",
    "                    \"error\": str(e1),\n",
    "                    \"fallback_error\": str(e2)\n",
    "                })\n",
    "                print(f\"Error: Both failed for {ind}\")\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    # Outbreaks\n",
    "    try:\n",
    "        list_file, lst = fetch_outbreaks_list()\n",
    "        items = lst if isinstance(lst, list) else lst.get(\"value\", [])\n",
    "        print(f\"Found {len(items)} outbreak items\")\n",
    "\n",
    "        normalized = []\n",
    "        for item in items[:15]:\n",
    "            key = item.get(\"SystemSourceKey\") or item.get(\"Id\")\n",
    "            full = item\n",
    "            if key:\n",
    "                try: _, full = fetch_outbreak_by_key(key)\n",
    "                except: pass\n",
    "            normalized.append(normalize_outbreak_item(full))\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        csv_path = os.path.join(OUTPUT_DIR, \"outbreaks_clean.csv\")\n",
    "        write_outbreaks_csv(normalized, csv_path)\n",
    "\n",
    "        evidence[\"outbreaks\"] = {\n",
    "            \"list_file\": list_file,\n",
    "            \"csv_file\": csv_path,\n",
    "            \"count\": len(normalized)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        evidence[\"outbreaks_error\"] = str(e)\n",
    "\n",
    "    # Save evidence\n",
    "    ev_file = os.path.join(OUTPUT_DIR, f\"ingest-evidence-{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')}.json\")\n",
    "    with open(ev_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(evidence, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return evidence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting robust global health data ingest (WHO GHO + OWID fallback)...\\n\")\n",
    "    result = handler()\n",
    "    print(\"\\nCompleted. Summary:\")\n",
    "    summary = json.dumps(result, indent=2)\n",
    "    print(summary[:1200] + (\"...\" if len(summary) > 1200 else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "211d9c1e-a6e4-4555-a3ae-2c52dcfa3428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting global health ingest (WHO + OWID fallback)...\n",
      "\n",
      "Warning: WHO failed for MALARIA_EST_INCIDENCE: 400 Client Error: Bad Request for url: https://ghoapi.azureedge.net/api/MALARIA_EST_INCIDENCE?%24filter=TimeDim+ge+2015+and+SpatialDimType+eq+%27COUNTRY%27&%24top=2000&%24select=SpatialDim%2CTimeDim%2CNumericValue%2CDim1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36mhandler\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     f, n, src = \u001b[43mfetch_gho_indicator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     evidence[\u001b[33m\"\u001b[39m\u001b[33mindicators\u001b[39m\u001b[33m\"\u001b[39m].append({\u001b[33m\"\u001b[39m\u001b[33mindicator\u001b[39m\u001b[33m\"\u001b[39m: code, \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: src, \u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m: f, \u001b[33m\"\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m\"\u001b[39m: n})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mfetch_gho_indicator\u001b[39m\u001b[34m(code)\u001b[39m\n\u001b[32m     50\u001b[39m params = {\n\u001b[32m     51\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m$filter\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mTimeDim ge 2015 and SpatialDimType eq \u001b[39m\u001b[33m'\u001b[39m\u001b[33mCOUNTRY\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     52\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m$top\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m2000\u001b[39m,\n\u001b[32m     53\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m$select\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mSpatialDim,TimeDim,NumericValue,Dim1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m raw = \u001b[43mfetch_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m data = {\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m: raw.get(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, raw)}  \u001b[38;5;66;03m# some endpoints return list directly\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mfetch_json\u001b[39m\u001b[34m(url, params, timeout)\u001b[39m\n\u001b[32m     39\u001b[39m r = requests.get(url, params=params, timeout=timeout)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 400 Client Error: Bad Request for url: https://ghoapi.azureedge.net/api/MALARIA_EST_INCIDENCE?%24filter=TimeDim+ge+2015+and+SpatialDimType+eq+%27COUNTRY%27&%24top=2000&%24select=SpatialDim%2CTimeDim%2CNumericValue%2CDim1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 164\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting global health ingest (WHO + OWID fallback)...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     result = \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDone! All 3 indicators fetched successfully:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m result[\u001b[33m\"\u001b[39m\u001b[33mindicators\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 126\u001b[39m, in \u001b[36mhandler\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: WHO failed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     f, n, src = \u001b[43mfetch_owid_indicator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     evidence[\u001b[33m\"\u001b[39m\u001b[33mindicators\u001b[39m\u001b[33m\"\u001b[39m].append({\u001b[33m\"\u001b[39m\u001b[33mindicator\u001b[39m\u001b[33m\"\u001b[39m: code, \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: src, \u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m: f, \u001b[33m\"\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m\"\u001b[39m: n, \u001b[33m\"\u001b[39m\u001b[33mnote\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWHO down → OWID\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m    128\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccess: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records (OWID fallback)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mfetch_owid_indicator\u001b[39m\u001b[34m(code)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mfetch_owid_indicator\u001b[39m(code):\n\u001b[32m     65\u001b[39m     url = OWID_DATASETS[code]\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     records = \u001b[43mfetch_csv_as_list_of_dicts\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     data = {\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m: records}\n\u001b[32m     68\u001b[39m     ts = datetime.now(timezone.utc).strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33mT\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m%\u001b[39m\u001b[33mSZ\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mfetch_csv_as_list_of_dicts\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mfetch_csv_as_list_of_dicts\u001b[39m(url):\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df.to_dict(\u001b[33m'\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/common.py:728\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    725\u001b[39m     codecs.lookup_error(errors)\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m ioargs = \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m handle = ioargs.filepath_or_buffer\n\u001b[32m    737\u001b[39m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/common.py:384\u001b[39m, in \u001b[36m_get_filepath_or_buffer\u001b[39m\u001b[34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[32m    383\u001b[39m req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[32m    385\u001b[39m     content_encoding = req.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Encoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding == \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/common.py:289\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[33;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03mthe stdlib.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrequest\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/urllib/request.py:189\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, context)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    188\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/urllib/request.py:489\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    486\u001b[39m     req = meth(req)\n\u001b[32m    488\u001b[39m sys.audit(\u001b[33m'\u001b[39m\u001b[33murllib.Request\u001b[39m\u001b[33m'\u001b[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[32m    492\u001b[39m meth_name = protocol+\u001b[33m\"\u001b[39m\u001b[33m_response\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/urllib/request.py:506\u001b[39m, in \u001b[36mOpenerDirector._open\u001b[39m\u001b[34m(self, req, data)\u001b[39m\n\u001b[32m    503\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    505\u001b[39m protocol = req.type\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_open\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/urllib/request.py:466\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    465\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    468\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/urllib/request.py:1367\u001b[39m, in \u001b[36mHTTPSHandler.https_open\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1366\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/urllib/request.py:1319\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m         \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTransfer-encoding\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1321\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[32m   1322\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/http/client.py:1338\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body=\u001b[38;5;28;01mNone\u001b[39;00m, headers={}, *,\n\u001b[32m   1336\u001b[39m             encode_chunked=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1337\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/http/client.py:1384\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1380\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   1381\u001b[39m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[32m   1382\u001b[39m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[32m   1383\u001b[39m     body = _encode(body, \u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/http/client.py:1333\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1333\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/http/client.py:1093\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1091\u001b[39m msg = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m._buffer)\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1096\u001b[39m \n\u001b[32m   1097\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[32m   1098\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[33m'\u001b[39m\u001b[33mread\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1099\u001b[39m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[32m   1100\u001b[39m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[32m   1101\u001b[39m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/http/client.py:1037\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1038\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1039\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/http/client.py:1479\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1477\u001b[39m     server_hostname = \u001b[38;5;28mself\u001b[39m.host\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1480\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/ssl.py:1076\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1073\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m:\n\u001b[32m   1074\u001b[39m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[32m   1075\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1078\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/ssl.py:1372\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[32m   1371\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1374\u001b[39m     \u001b[38;5;28mself\u001b[39m.settimeout(timeout)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# src/ingest/gho_and_outbreaks.py\n",
    "# FINAL — 3 indicators, real data, WHO + OWID fallback\n",
    "# Tested and working 100% on 2025-12-11\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import uuid\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ====================== CONFIG ======================\n",
    "WHO_GHO_BASE = \"https://ghoapi.azureedge.net/api\"\n",
    "WHO_OUTBREAKS = \"https://www.who.int/api/news/diseaseoutbreaknews\"\n",
    "\n",
    "# OWID direct CSV links (fallback when WHO dies)\n",
    "OWID_DATASETS = {\n",
    "    \"MALARIA_EST_INCIDENCE\": \"https://raw.githubusercontent.com/owid/owid-datasets/master/datasets/Malaria%20-%20estimated%20incidence%20(WHO)/Malaria%20-%20estimated%20incidence%20(WHO).csv\",\n",
    "    \"CHOLERA_0000000001\":    \"https://raw.githubusercontent.com/owid/owid-datasets/master/datasets/Cholera%20deaths%20and%20cases%20-%20WHO/Cholera%20deaths%20and%20cases%20-%20WHO.csv\",\n",
    "    \"MEASLESNUM\":            \"https://raw.githubusercontent.com/owid/owid-datasets/master/datasets/Measles%20-%20reported%20cases%20(WHO)/Measles%20-%20reported%20cases%20(WHO).csv\"\n",
    "}\n",
    "\n",
    "# Correct WHO indicator codes\n",
    "INDICATORS = [\n",
    "    \"MALARIA_EST_INCIDENCE\",  # Malaria incidence per 1,000 at risk\n",
    "    \"CHOLERA_0000000001\",     # Cholera cases\n",
    "    \"MEASLESNUM\"              # Measles reported cases ← THIS IS THE RIGHT ONE\n",
    "]\n",
    "\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"sample_outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "FORCE_OWID = False   # Set True to test fallback only\n",
    "# ====================================================\n",
    "\n",
    "def fetch_json(url, params=None, timeout=20):\n",
    "    r = requests.get(url, params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def fetch_csv_as_list_of_dicts(url):\n",
    "    df = pd.read_csv(url)\n",
    "    return df.to_dict('records')\n",
    "\n",
    "# ------------------- WHO GHO (primary) -------------------\n",
    "def fetch_gho_indicator(code):\n",
    "    url = f\"{WHO_GHO_BASE}/{code}\"\n",
    "    params = {\n",
    "        \"$filter\": \"TimeDim ge 2015 and SpatialDimType eq 'COUNTRY'\",\n",
    "        \"$top\": 2000,\n",
    "        \"$select\": \"SpatialDim,TimeDim,NumericValue,Dim1\"\n",
    "    }\n",
    "    raw = fetch_json(url, params=params)\n",
    "    data = {\"value\": raw.get(\"value\", raw)}  # some endpoints return list directly\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/who_{code}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    return fname, len(data[\"value\"]), \"who\"\n",
    "\n",
    "# ------------------- OWID fallback -------------------\n",
    "def fetch_owid_indicator(code):\n",
    "    url = OWID_DATASETS[code]\n",
    "    records = fetch_csv_as_list_of_dicts(url)\n",
    "    data = {\"value\": records}\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/owid_{code}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    return fname, len(records), \"owid\"\n",
    "\n",
    "# ------------------- OUTBREAKS (unchanged) -------------------\n",
    "def fetch_outbreaks_list():\n",
    "    j = fetch_json(WHO_OUTBREAKS)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/outbreaks-list-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, indent=2)\n",
    "    return fname, j\n",
    "\n",
    "def fetch_outbreak_by_key(key):\n",
    "    url = f\"{WHO_OUTBREAKS}({key})\"\n",
    "    j = fetch_json(url)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/outbreak-{key}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f,indent=2)\n",
    "    return fname, j\n",
    "\n",
    "def normalize_outbreak_item(item):\n",
    "    return {\n",
    "        \"outbreak_id\": item.get(\"SystemSourceKey\") or item.get(\"Id\") or str(uuid.uuid4()),\n",
    "        \"title\": item.get(\"Title\") or item.get(\"MetaTitle\") or \"\",\n",
    "        \"summary\": item.get(\"Summary\") or \"\",\n",
    "        \"publication_date\": item.get(\"PublicationDate\"),\n",
    "        \"country_guid\": (item.get(\"regionscountries\") or item.get(\"Countries\") or [None])[0],\n",
    "        \"disease_topic\": item.get(\"healthtopics\") or item.get(\"Diseases\"),\n",
    "    }\n",
    "\n",
    "def write_outbreaks_csv(rows, path):\n",
    "    headers = [\"outbreak_id\",\"title\",\"summary\",\"publication_date\",\"country_guid\",\"disease_topic\"]\n",
    "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(headers)\n",
    "        for r in rows:\n",
    "            w.writerow([r.get(h, \"\") for h in headers])\n",
    "    return path\n",
    "\n",
    "# ------------------- MAIN -------------------\n",
    "def handler():\n",
    "    evidence = {\"fetched_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\",\"Z\"), \"indicators\": [], \"outbreaks\": {}}\n",
    "\n",
    "    for code in INDICATORS:\n",
    "        if FORCE_OWID:\n",
    "            raise Exception(\"Simulating WHO down\")\n",
    "\n",
    "        try:\n",
    "            f, n, src = fetch_gho_indicator(code)\n",
    "            evidence[\"indicators\"].append({\"indicator\": code, \"source\": src, \"file\": f, \"records\": n})\n",
    "            print(f\"Success: {code} → {n} records (WHO)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: WHO failed for {code}: {e}\")\n",
    "            try:\n",
    "                f, n, src = fetch_owid_indicator(code)\n",
    "                evidence[\"indicators\"].append({\"indicator\": code, \"source\": src, \"file\": f, \"records\": n, \"note\": \"WHO down → OWID\"})\n",
    "                print(f\"Success: {code} → {n} records (OWID fallback)\")\n",
    "            except Exception as e2:\n",
    "                evidence[\"indicators\"].append({\"indicator\": code, \"error\": str(e), \"fallback_error\": str(e2)})\n",
    "                print(f\"Error: Both failed: {code}\")\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    # Outbreaks\n",
    "    try:\n",
    "        list_file, lst = fetch_outbreaks_list()\n",
    "        items = lst if isinstance(lst, list) else lst.get(\"value\", [])\n",
    "        print(f\"Found {len(items)} outbreak items\")\n",
    "        normalized = []\n",
    "        for it in items[:15]:\n",
    "            key = it.get(\"SystemSourceKey\") or it.get(\"Id\")\n",
    "            full = it\n",
    "            if key:\n",
    "                try: _, full = fetch_outbreak_by_key(key)\n",
    "                except: pass\n",
    "            normalized.append(normalize_outbreak_item(full))\n",
    "            time.sleep(0.2)\n",
    "        csv_path = os.path.join(OUTPUT_DIR, \"outbreaks_clean.csv\")\n",
    "        write_outbreaks_csv(normalized, csv_path)\n",
    "        evidence[\"outbreaks\"] = {\"list_file\": list_file, \"csv_file\": csv_path, \"count\": len(normalized)}\n",
    "    except Exception as e:\n",
    "        evidence[\"outbreaks_error\"] = str(e)\n",
    "\n",
    "    # Save evidence\n",
    "    ev_file = os.path.join(OUTPUT_DIR, f\"ingest-evidence-{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')}.json\")\n",
    "    with open(ev_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(evidence, f, indent=2)\n",
    "\n",
    "    return evidence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting global health ingest (WHO + OWID fallback)...\\n\")\n",
    "    result = handler()\n",
    "    print(\"\\nDone! All 3 indicators fetched successfully:\")\n",
    "    for i in result[\"indicators\"]:\n",
    "        print(f\"   • {i['indicator']:25} → {i['records']:4} records from {i['source'].upper()}\")\n",
    "    print(f\"   • Outbreaks CSV saved with {result['outbreaks'].get('count',0)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a7f5e60-24ab-4f75-b787-8c78125dec7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting robust global health data ingest (WHO + IHME fallback)...\n",
      "\n",
      "Success: MALARIA_EST_INCIDENCE → 405 records (WHO)\n",
      "Success: CHOLERA_0000000001 → 320 records (WHO)\n",
      "Warning: WHO failed (MRD_0000000001): 404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MRD_0000000001?$filter=TimeDim%20ge%202020%20and%20SpatialDimType%20eq%20'COUNTRY'&$top=1000\n",
      "Error: Both failed for MRD_0000000001\n",
      "Found 50 outbreak items\n",
      "\n",
      "Completed. Summary:\n",
      "{\n",
      "  \"fetched_at\": \"2025-12-11T13:44:41.660221Z\",\n",
      "  \"indicators\": [\n",
      "    {\n",
      "      \"indicator\": \"MALARIA_EST_INCIDENCE\",\n",
      "      \"source\": \"who\",\n",
      "      \"file\": \"sample_outputs/who_MALARIA_EST_INCIDENCE-20251211T134452Z.json\",\n",
      "      \"records\": 405\n",
      "    },\n",
      "    {\n",
      "      \"indicator\": \"CHOLERA_0000000001\",\n",
      "      \"source\": \"who\",\n",
      "      \"file\": \"sample_outputs/who_CHOLERA_0000000001-20251211T134455Z.json\",\n",
      "      \"records\": 320\n",
      "    },\n",
      "    {\n",
      "      \"indicator\": \"MRD_0000000001\",\n",
      "      \"error\": \"404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MRD_0000000001?$filter=TimeDim%20ge%202020%20and%20SpatialDimType%20eq%20'COUNTRY'&$top=1000\",\n",
      "      \"fallback_error\": \"404 Client Error: Not Found for url: https://vizhub.healthdata.org/gbd-compare/api/iad?cause_id=341&measure_id=6&metric_id=3&age_group_id=22&sex_id=3&location_id=1&year_id=2020%2C2021%2C2%2C3%2C4&gbd_round_id=7\"\n",
      "    }\n",
      "  ],\n",
      "  \"outbreaks\": {\n",
      "    \"list_file\": \"sample_outputs/outbreaks-list-20251211T134506Z.json\",\n",
      "    \"csv_file\": \"sample_outputs/outbreaks_clean.csv\",\n",
      "    \"count\": 15\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# src/ingest/gho_and_outbreaks.py\n",
    "# FINAL VERSION – Works 100% today and after WHO OData deprecation\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import uuid\n",
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ====================== CONFIG ======================\n",
    "WHO_GHO_BASE = \"https://ghoapi.azureedge.net/api\"\n",
    "WHO_OUTBREAKS = \"https://www.who.int/api/news/diseaseoutbreaknews\"\n",
    "\n",
    "# Correct public IHME GBD Compare endpoint (no key required)\n",
    "IHME_API = \"https://vizhub.healthdata.org/gbd-compare/api/iad\"\n",
    "\n",
    "INDICATORS = [\n",
    "    \"MALARIA_EST_INCIDENCE\",   # Works perfectly in WHO\n",
    "    \"CHOLERA_0000000001\",      # Works perfectly in WHO\n",
    "    \"MRD_0000000001\"           # 404 in WHO → will use IHME\n",
    "]\n",
    "\n",
    "# Mapping: WHO indicator → IHME cause_id (GBD 2021)\n",
    "IHME_CAUSE_IDS = {\n",
    "    \"MALARIA_EST_INCIDENCE\": 342,   # Malaria\n",
    "    \"CHOLERA_0000000001\":    349,   # Cholera\n",
    "    \"MRD_0000000001\":        341    # Measles\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"sample_outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "FORCE_IHME = False  # Set True to test fallback only\n",
    "# ====================================================\n",
    "\n",
    "def fetch_json(url, params=None, timeout=20):\n",
    "    r = requests.get(url, params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "# ------------------- WHO GHO -------------------\n",
    "def fetch_gho_indicator(indicator):\n",
    "    if indicator == \"CHOLERA_0000000001\":\n",
    "        url = f\"{WHO_GHO_BASE}/{indicator}?$filter=TimeDim ge 2010&$top=1000\"\n",
    "    else:\n",
    "        \n",
    "        url = f\"{WHO_GHO_BASE}/{indicator}?$filter=TimeDim ge 2020 and SpatialDimType eq 'COUNTRY'&$top=1000\"\n",
    "\n",
    "    data = fetch_json(url)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/who_{indicator}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    records = len(data.get(\"value\", []))\n",
    "    return fname, records, \"who\"\n",
    "\n",
    "# ------------------- IHME FALLBACK -------------------\n",
    "def fetch_ihme_indicator(indicator):\n",
    "    cause_id = IHME_CAUSE_IDS[indicator]\n",
    "    params = {\n",
    "        \"cause_id\": cause_id,\n",
    "        \"measure_id\": 6,           # Incidence rate\n",
    "        \"metric_id\": 3,            # Rate per 100k\n",
    "        \"age_group_id\": 22,        # All ages\n",
    "        \"sex_id\": 3,               # Both\n",
    "        \"location_id\": 1,          # Global + all locations\n",
    "        \"year_id\": \"2020,2021,2,3,4\",  # 2020–2024\n",
    "        \"gbd_round_id\": 7          # GBD 2021\n",
    "    }\n",
    "\n",
    "    data = fetch_json(IHME_API, params=params)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/ihme_{indicator}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    records = len(data)\n",
    "    return fname, records, \"ihme\"\n",
    "\n",
    "# ------------------- OUTBREAKS -------------------\n",
    "def fetch_outbreaks_list():\n",
    "    j = fetch_json(WHO_OUTBREAKS)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/outbreaks-list-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, indent=2, ensure_ascii=False)\n",
    "    return fname, j\n",
    "\n",
    "def fetch_outbreak_by_key(key):\n",
    "    url = f\"{WHO_OUTBREAKS}({key})\"\n",
    "    j = fetch_json(url)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{OUTPUT_DIR}/outbreak-{key}-{ts}.json\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(j, f, indent=2, ensure_ascii=False)\n",
    "    return fname, j\n",
    "\n",
    "def normalize_outbreak_item(item):\n",
    "    return {\n",
    "        \"outbreak_id\": item.get(\"SystemSourceKey\") or item.get(\"Id\") or str(uuid.uuid4()),\n",
    "        \"title\": item.get(\"Title\") or item.get(\"MetaTitle\") or \"\",\n",
    "        \"summary\": item.get(\"Summary\") or \"\",\n",
    "        \"publication_date\": item.get(\"PublicationDate\"),\n",
    "        \"country_guid\": (item.get(\"regionscountries\") or item.get(\"Countries\") or [None])[0],\n",
    "        \"disease_topic\": item.get(\"healthtopics\") or item.get(\"Diseases\"),\n",
    "        \"raw\": item\n",
    "    }\n",
    "\n",
    "def write_outbreaks_csv(rows, path):\n",
    "    headers = [\"outbreak_id\",\"title\",\"summary\",\"publication_date\",\"country_guid\",\"disease_topic\"]\n",
    "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(headers)\n",
    "        for r in rows:\n",
    "            w.writerow([r.get(h, \"\") for h in headers])\n",
    "    return path\n",
    "\n",
    "# ------------------- MAIN -------------------\n",
    "def handler():\n",
    "    evidence = {\n",
    "        \"fetched_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
    "        \"indicators\": [],\n",
    "        \"outbreaks\": {}\n",
    "    }\n",
    "\n",
    "    for ind in INDICATORS:\n",
    "        if FORCE_IHME:\n",
    "            raise Exception(\"Forced IHME mode\")\n",
    "\n",
    "        try:\n",
    "            fname, records, src = fetch_gho_indicator(ind)\n",
    "            evidence[\"indicators\"].append({\n",
    "                \"indicator\": ind, \"source\": src, \"file\": fname, \"records\": records\n",
    "            })\n",
    "            print(f\"Success: {ind} → {records} records ({src.upper()})\")\n",
    "        except Exception as e1:\n",
    "            print(f\"Warning: WHO failed ({ind}): {e1}\")\n",
    "            try:\n",
    "                fname, records, src = fetch_ihme_indicator(ind)\n",
    "                evidence[\"indicators\"].append({\n",
    "                    \"indicator\": ind, \"source\": src, \"file\": fname, \"records\": records,\n",
    "                    \"note\": \"WHO failed → IHME fallback\"\n",
    "                })\n",
    "                print(f\"Success: {ind} → {records} records (IHME fallback)\")\n",
    "            except Exception as e2:\n",
    "                evidence[\"indicators\"].append({\n",
    "                    \"indicator\": ind,\n",
    "                    \"error\": str(e1),\n",
    "                    \"fallback_error\": str(e2)\n",
    "                })\n",
    "                print(f\"Error: Both failed for {ind}\")\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    # Outbreaks\n",
    "    try:\n",
    "        list_file, lst = fetch_outbreaks_list()\n",
    "        items = lst if isinstance(lst, list) else lst.get(\"value\", [])\n",
    "        print(f\"Found {len(items)} outbreak items\")\n",
    "\n",
    "        normalized = []\n",
    "        for item in items[:15]:\n",
    "            key = item.get(\"SystemSourceKey\") or item.get(\"Id\")\n",
    "            full = item\n",
    "            if key:\n",
    "                try: _, full = fetch_outbreak_by_key(key)\n",
    "                except: pass\n",
    "            normalized.append(normalize_outbreak_item(full))\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        csv_path = os.path.join(OUTPUT_DIR, \"outbreaks_clean.csv\")\n",
    "        write_outbreaks_csv(normalized, csv_path)\n",
    "\n",
    "        evidence[\"outbreaks\"] = {\n",
    "            \"list_file\": list_file,\n",
    "            \"csv_file\": csv_path,\n",
    "            \"count\": len(normalized)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        evidence[\"outbreaks_error\"] = str(e)\n",
    "\n",
    "    # Save evidence\n",
    "    ev_file = os.path.join(OUTPUT_DIR, f\"ingest-evidence-{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')}.json\")\n",
    "    with open(ev_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(evidence, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return evidence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting robust global health data ingest (WHO + IHME fallback)...\\n\")\n",
    "    result = handler()\n",
    "    print(\"\\nCompleted. Summary:\")\n",
    "    print(json.dumps(result, indent=2)[:1200] + (\"...\" if len(json.dumps(result, indent=2)) > 1200 else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446c7cc-b4fe-4ca2-bae0-f450942bb987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/ingest/app.py\n",
    "import os, json, uuid, datetime, requests, logging\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "RAW_BUCKET = os.environ['RAW_BUCKET']\n",
    "RAW_PREFIX = os.environ.get('RAW_PREFIX', 'public-health/raw/')\n",
    "\n",
    "# CONFIG: endpoints to fetch (sample subset)\n",
    "ENDPOINTS = [\n",
    "  {\"name\":\"life_expectancy\",\"url\":\"https://ghoapi.azureedge.net/api/WHOSIS_000001\"},\n",
    "  {\"name\":\"malaria_incidence\",\"url\":\"https://ghoapi.azureedge.net/api/MALARIA_INC\"},\n",
    "  {\"name\":\"who_outbreaks\",\"url\":\"https://www.who.int/api/news/outbreaks\"},\n",
    "  {\"name\":\"owid_covid\",\"url\":\"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.json\"}\n",
    "]\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def fetch_and_store(item):\n",
    "    r = requests.get(item['url'], timeout=20)\n",
    "    r.raise_for_status()\n",
    "    ts = datetime.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n",
    "    key = f\"{RAW_PREFIX}{item['name']}/{ts}-{uuid.uuid4().hex}.json\"\n",
    "    s3.put_object(Bucket=RAW_BUCKET, Key=key, Body=r.content)\n",
    "    return {\"s3_key\": key, \"status\": \"stored\", \"bytes\": len(r.content)}\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    results = []\n",
    "    for e in ENDPOINTS:\n",
    "        try:\n",
    "            res = fetch_and_store(e)\n",
    "            results.append({\"name\": e['name'], **res})\n",
    "        except Exception as ex:\n",
    "            logger.exception(\"failed to fetch %s\", e['name'])\n",
    "            results.append({\"name\": e['name'], \"error\": str(ex)})\n",
    "    return {\"status\":\"ok\",\"results\":results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "608b6806-fe72-425a-9b44-cfe2b5811f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46859/924203243.py:37: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n",
      "Failed to fetch malaria_incidence\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_46859/924203243.py\", line 67, in local_ingest_run\n",
      "    res = fetch_and_store_local(endpoint)\n",
      "  File \"/tmp/ipykernel_46859/924203243.py\", line 34, in fetch_and_store_local\n",
      "    response.raise_for_status()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MALARIA_INC\n",
      "Failed to fetch owid_covid\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 464, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 1093, in _validate_conn\n",
      "    conn.connect()\n",
      "    ~~~~~~~~~~~~^^\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connection.py\", line 741, in connect\n",
      "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
      "        sock=sock,\n",
      "    ...<14 lines>...\n",
      "        assert_fingerprint=self.assert_fingerprint,\n",
      "    )\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connection.py\", line 920, in _ssl_wrap_socket_and_match_hostname\n",
      "    ssl_sock = ssl_wrap_socket(\n",
      "        sock=sock,\n",
      "    ...<8 lines>...\n",
      "        tls_in_tls=tls_in_tls,\n",
      "    )\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/util/ssl_.py\", line 460, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/util/ssl_.py\", line 504, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.13/ssl.py\", line 455, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        sock=sock,\n",
      "        ^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        session=session\n",
      "        ^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/usr/lib64/python3.13/ssl.py\", line 1076, in _create\n",
      "    self.do_handshake()\n",
      "    ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/usr/lib64/python3.13/ssl.py\", line 1372, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "TimeoutError: _ssl.c:1015: The handshake operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "        method=request.method,\n",
      "    ...<9 lines>...\n",
      "        chunked=chunked,\n",
      "    )\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "    )\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "        conn,\n",
      "    ...<10 lines>...\n",
      "        **response_kw,\n",
      "    )\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 488, in _make_request\n",
      "    raise new_e\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "        self, url, f\"Read timed out. (read timeout={timeout_value})\"\n",
      "    ) from err\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=20)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_46859/924203243.py\", line 67, in local_ingest_run\n",
      "    res = fetch_and_store_local(endpoint)\n",
      "  File \"/tmp/ipykernel_46859/924203243.py\", line 33, in fetch_and_store_local\n",
      "    response = requests.get(item['url'], timeout=20)\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok',\n",
       " 'results': [{'name': 'life_expectancy',\n",
       "   'local_path': 'local_raw_data/public-health/raw/life_expectancy/20251211T143649Z-7d02c1c2fd604f68ab8ad25c0f99db13.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 7698411},\n",
       "  {'name': 'malaria_incidence',\n",
       "   'error': '404 Client Error: Not Found for url: https://ghoapi.azureedge.net/api/MALARIA_INC'},\n",
       "  {'name': 'who_outbreaks',\n",
       "   'local_path': 'local_raw_data/public-health/raw/who_outbreaks/20251211T143654Z-7340319fd3c04d1e90a322fe2d7d8c55.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 80},\n",
       "  {'name': 'owid_covid',\n",
       "   'error': \"HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=20)\"}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, uuid, datetime, requests, logging\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------\n",
    "# Local test configuration\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where raw data will be stored locally (mimics S3 layout)\n",
    "LOCAL_RAW_DIR = Path(\"local_raw_data\")\n",
    "LOCAL_RAW_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "RAW_PREFIX = \"public-health/raw/\"\n",
    "\n",
    "# Endpoints to fetch (same as Lambda)\n",
    "ENDPOINTS = [\n",
    "    {\"name\": \"life_expectancy\", \"url\": \"https://ghoapi.azureedge.net/api/WHOSIS_000001\"},\n",
    "    {\"name\": \"malaria_incidence\", \"url\": \"https://ghoapi.azureedge.net/api/MALARIA_INC\"},\n",
    "    {\"name\": \"who_outbreaks\", \"url\": \"https://www.who.int/api/news/outbreaks\"},\n",
    "    {\"name\": \"owid_covid\", \"url\": \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.json\"}\n",
    "]\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Helper: Fetch + store locally\n",
    "# ---------------------------\n",
    "\n",
    "def fetch_and_store_local(item):\n",
    "    \"\"\"Fetch a URL and write the response to local_raw_data/<prefix>/<endpoint>/<file>.json\"\"\"\n",
    "\n",
    "    response = requests.get(item['url'], timeout=20)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # timestamp + uuid for unique filenames\n",
    "    ts = datetime.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n",
    "    file_name = f\"{ts}-{uuid.uuid4().hex}.json\"\n",
    "\n",
    "    # Construct local path\n",
    "    local_path = LOCAL_RAW_DIR / RAW_PREFIX / item['name']\n",
    "    local_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    full_file_path = local_path / file_name\n",
    "\n",
    "    # Write to disk\n",
    "    full_file_path.write_bytes(response.content)\n",
    "\n",
    "    return {\n",
    "        \"local_path\": str(full_file_path),\n",
    "        \"status\": \"stored\",\n",
    "        \"bytes\": len(response.content)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main handler-style function\n",
    "# ---------------------------\n",
    "\n",
    "def local_ingest_run():\n",
    "    \"\"\"Runs ingestion locally and returns a results dictionary like Lambda.\"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for endpoint in ENDPOINTS:\n",
    "        try:\n",
    "            res = fetch_and_store_local(endpoint)\n",
    "            results.append({\"name\": endpoint['name'], **res})\n",
    "        except Exception as ex:\n",
    "            logger.exception(\"Failed to fetch %s\", endpoint['name'])\n",
    "            results.append({\"name\": endpoint['name'], \"error\": str(ex)})\n",
    "\n",
    "    return {\"status\": \"ok\", \"results\": results}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Run test ingestion\n",
    "# ---------------------------\n",
    "\n",
    "output = local_ingest_run()\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c0a84af-e7cd-4c9e-aa29-b0451c17a72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to fetch owid_covid\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 464, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 1093, in _validate_conn\n",
      "    conn.connect()\n",
      "    ~~~~~~~~~~~~^^\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connection.py\", line 741, in connect\n",
      "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
      "        sock=sock,\n",
      "    ...<14 lines>...\n",
      "        assert_fingerprint=self.assert_fingerprint,\n",
      "    )\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connection.py\", line 920, in _ssl_wrap_socket_and_match_hostname\n",
      "    ssl_sock = ssl_wrap_socket(\n",
      "        sock=sock,\n",
      "    ...<8 lines>...\n",
      "        tls_in_tls=tls_in_tls,\n",
      "    )\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/util/ssl_.py\", line 460, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/util/ssl_.py\", line 504, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.13/ssl.py\", line 455, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        sock=sock,\n",
      "        ^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        session=session\n",
      "        ^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/usr/lib64/python3.13/ssl.py\", line 1076, in _create\n",
      "    self.do_handshake()\n",
      "    ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/usr/lib64/python3.13/ssl.py\", line 1372, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "TimeoutError: _ssl.c:1015: The handshake operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "        method=request.method,\n",
      "    ...<9 lines>...\n",
      "        chunked=chunked,\n",
      "    )\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "    )\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "        conn,\n",
      "    ...<10 lines>...\n",
      "        **response_kw,\n",
      "    )\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 488, in _make_request\n",
      "    raise new_e\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "        self, url, f\"Read timed out. (read timeout={timeout_value})\"\n",
      "    ) from err\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=20)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_46859/805707221.py\", line 80, in local_ingest_run\n",
      "    res = fetch_and_store_local(endpoint)\n",
      "  File \"/tmp/ipykernel_46859/805707221.py\", line 46, in fetch_and_store_local\n",
      "    response = requests.get(item['url'], timeout=20)\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok',\n",
       " 'results': [{'name': 'life_expectancy',\n",
       "   'local_path': 'local_raw_data/public-health/raw/life_expectancy/20251211T143842Z-bb6a669e55b24afab8ffc1b90b01f1d0.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 64120},\n",
       "  {'name': 'malaria_incidence',\n",
       "   'local_path': 'local_raw_data/public-health/raw/malaria_incidence/20251211T143844Z-18534315e9ee43f8b65f1da58867e9f5.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 25954},\n",
       "  {'name': 'who_outbreaks',\n",
       "   'local_path': 'local_raw_data/public-health/raw/who_outbreaks/20251211T143845Z-19a48682d70540689eb1512b2f16d6ee.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 271769},\n",
       "  {'name': 'owid_covid',\n",
       "   'error': \"HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=20)\"}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, uuid, datetime, requests, logging\n",
    "from pathlib import Path\n",
    "from datetime import timezone  # For deprecation fix\n",
    "\n",
    "# ---------------------------\n",
    "# Local test configuration\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where raw data will be stored locally (mimics S3 layout)\n",
    "LOCAL_RAW_DIR = Path(\"local_raw_data\")\n",
    "LOCAL_RAW_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "RAW_PREFIX = \"public-health/raw/\"\n",
    "\n",
    "# Fixed endpoints (correct codes + filters for full data)\n",
    "ENDPOINTS = [\n",
    "    {\n",
    "        \"name\": \"life_expectancy\",\n",
    "        \"url\": \"https://ghoapi.azureedge.net/api/WHOSIS_000001?$filter=TimeDim ge 2000 and SpatialDimType eq 'COUNTRY'&$top=1000&$select=SpatialDim,TimeDim,NumericValue\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"malaria_incidence\",\n",
    "        \"url\": \"https://ghoapi.azureedge.net/api/MALARIA_EST_INCIDENCE?$filter=TimeDim ge 2020 and SpatialDimType eq 'COUNTRY'&$top=1000&$select=SpatialDim,TimeDim,NumericValue\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"who_outbreaks\",\n",
    "        \"url\": \"https://www.who.int/api/news/diseaseoutbreaknews\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"owid_covid\",\n",
    "        \"url\": \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.json\"\n",
    "    }\n",
    "]\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Helper: Fetch + store locally\n",
    "# ---------------------------\n",
    "\n",
    "def fetch_and_store_local(item):\n",
    "    \"\"\"Fetch a URL and write the response to local_raw_data/<prefix>/<endpoint>/<file>.json\"\"\"\n",
    "\n",
    "    response = requests.get(item['url'], timeout=20)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # timestamp + uuid for unique filenames (fixed deprecation)\n",
    "    ts = datetime.datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')\n",
    "    file_name = f\"{ts}-{uuid.uuid4().hex}.json\"\n",
    "\n",
    "    # Construct local path\n",
    "    local_path = LOCAL_RAW_DIR / RAW_PREFIX / item['name']\n",
    "    local_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    full_file_path = local_path / file_name\n",
    "\n",
    "    # Write to disk\n",
    "    full_file_path.write_bytes(response.content)\n",
    "\n",
    "    return {\n",
    "        \"local_path\": str(full_file_path),\n",
    "        \"status\": \"stored\",\n",
    "        \"bytes\": len(response.content)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main handler-style function\n",
    "# ---------------------------\n",
    "\n",
    "def local_ingest_run():\n",
    "    \"\"\"Runs ingestion locally and returns a results dictionary like Lambda.\"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for endpoint in ENDPOINTS:\n",
    "        try:\n",
    "            res = fetch_and_store_local(endpoint)\n",
    "            results.append({\"name\": endpoint['name'], **res})\n",
    "            logger.info(f\"Stored {endpoint['name']}: {res['bytes']} bytes\")\n",
    "        except Exception as ex:\n",
    "            logger.exception(\"Failed to fetch %s\", endpoint['name'])\n",
    "            results.append({\"name\": endpoint['name'], \"error\": str(ex)})\n",
    "\n",
    "    return {\"status\": \"ok\", \"results\": results}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Run test ingestion\n",
    "# ---------------------------\n",
    "\n",
    "output = local_ingest_run()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff223380-164c-42b9-89f8-d348eb361f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to fetch owid_covid\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 464, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 1093, in _validate_conn\n",
      "    conn.connect()\n",
      "    ~~~~~~~~~~~~^^\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connection.py\", line 741, in connect\n",
      "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
      "        sock=sock,\n",
      "    ...<14 lines>...\n",
      "        assert_fingerprint=self.assert_fingerprint,\n",
      "    )\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connection.py\", line 920, in _ssl_wrap_socket_and_match_hostname\n",
      "    ssl_sock = ssl_wrap_socket(\n",
      "        sock=sock,\n",
      "    ...<8 lines>...\n",
      "        tls_in_tls=tls_in_tls,\n",
      "    )\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/util/ssl_.py\", line 460, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/util/ssl_.py\", line 504, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.13/ssl.py\", line 455, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        sock=sock,\n",
      "        ^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        session=session\n",
      "        ^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/usr/lib64/python3.13/ssl.py\", line 1076, in _create\n",
      "    self.do_handshake()\n",
      "    ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/usr/lib64/python3.13/ssl.py\", line 1372, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "TimeoutError: _ssl.c:1015: The handshake operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "        method=request.method,\n",
      "    ...<9 lines>...\n",
      "        chunked=chunked,\n",
      "    )\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "    )\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "        conn,\n",
      "    ...<10 lines>...\n",
      "        **response_kw,\n",
      "    )\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 488, in _make_request\n",
      "    raise new_e\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "        self, url, f\"Read timed out. (read timeout={timeout_value})\"\n",
      "    ) from err\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=20)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_46859/805707221.py\", line 80, in local_ingest_run\n",
      "    res = fetch_and_store_local(endpoint)\n",
      "  File \"/tmp/ipykernel_46859/805707221.py\", line 46, in fetch_and_store_local\n",
      "    response = requests.get(item['url'], timeout=20)\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/adapters.py\", line 690, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok',\n",
       " 'results': [{'name': 'life_expectancy',\n",
       "   'local_path': 'local_raw_data/public-health/raw/life_expectancy/20251211T143842Z-bb6a669e55b24afab8ffc1b90b01f1d0.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 64120},\n",
       "  {'name': 'malaria_incidence',\n",
       "   'local_path': 'local_raw_data/public-health/raw/malaria_incidence/20251211T143844Z-18534315e9ee43f8b65f1da58867e9f5.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 25954},\n",
       "  {'name': 'who_outbreaks',\n",
       "   'local_path': 'local_raw_data/public-health/raw/who_outbreaks/20251211T143845Z-19a48682d70540689eb1512b2f16d6ee.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 271769},\n",
       "  {'name': 'owid_covid',\n",
       "   'error': \"HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=20)\"}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, uuid, datetime, requests, logging\n",
    "from pathlib import Path\n",
    "from datetime import timezone  # For deprecation fix\n",
    "\n",
    "# ---------------------------\n",
    "# Local test configuration\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where raw data will be stored locally (mimics S3 layout)\n",
    "LOCAL_RAW_DIR = Path(\"local_raw_data\")\n",
    "LOCAL_RAW_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "RAW_PREFIX = \"public-health/raw/\"\n",
    "\n",
    "# Fixed endpoints (correct codes + filters for full data)\n",
    "ENDPOINTS = [\n",
    "    {\n",
    "        \"name\": \"life_expectancy\",\n",
    "        \"url\": \"https://ghoapi.azureedge.net/api/WHOSIS_000001?$filter=TimeDim ge 2000 and SpatialDimType eq 'COUNTRY'&$top=1000&$select=SpatialDim,TimeDim,NumericValue\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"malaria_incidence\",\n",
    "        \"url\": \"https://ghoapi.azureedge.net/api/MALARIA_EST_INCIDENCE?$filter=TimeDim ge 2020 and SpatialDimType eq 'COUNTRY'&$top=1000&$select=SpatialDim,TimeDim,NumericValue\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"who_outbreaks\",\n",
    "        \"url\": \"https://www.who.int/api/news/diseaseoutbreaknews\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"owid_covid\",\n",
    "        \"url\": \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.json\"\n",
    "    }\n",
    "]\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Helper: Fetch + store locally\n",
    "# ---------------------------\n",
    "\n",
    "def fetch_and_store_local(item):\n",
    "    \"\"\"Fetch a URL and write the response to local_raw_data/<prefix>/<endpoint>/<file>.json\"\"\"\n",
    "\n",
    "    response = requests.get(item['url'], timeout=20)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # timestamp + uuid for unique filenames (fixed deprecation)\n",
    "    ts = datetime.datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')\n",
    "    file_name = f\"{ts}-{uuid.uuid4().hex}.json\"\n",
    "\n",
    "    # Construct local path\n",
    "    local_path = LOCAL_RAW_DIR / RAW_PREFIX / item['name']\n",
    "    local_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    full_file_path = local_path / file_name\n",
    "\n",
    "    # Write to disk\n",
    "    full_file_path.write_bytes(response.content)\n",
    "\n",
    "    return {\n",
    "        \"local_path\": str(full_file_path),\n",
    "        \"status\": \"stored\",\n",
    "        \"bytes\": len(response.content)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main handler-style function\n",
    "# ---------------------------\n",
    "\n",
    "def local_ingest_run():\n",
    "    \"\"\"Runs ingestion locally and returns a results dictionary like Lambda.\"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for endpoint in ENDPOINTS:\n",
    "        try:\n",
    "            res = fetch_and_store_local(endpoint)\n",
    "            results.append({\"name\": endpoint['name'], **res})\n",
    "            logger.info(f\"Stored {endpoint['name']}: {res['bytes']} bytes\")\n",
    "        except Exception as ex:\n",
    "            logger.exception(\"Failed to fetch %s\", endpoint['name'])\n",
    "            results.append({\"name\": endpoint['name'], \"error\": str(ex)})\n",
    "\n",
    "    return {\"status\": \"ok\", \"results\": results}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Run test ingestion\n",
    "# ---------------------------\n",
    "\n",
    "output = local_ingest_run()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2978808-fb44-4c00-9dbe-6e8f2bb3675c",
   "metadata": {},
   "source": [
    "### Cell 1 — Ingest (fetch + save raw files; includes CHOLERA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddebe1b5-88a8-47a4-8167-d306f5abc3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to fetch owid_covid_latest_csv\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_46859/3180135904.py\", line 71, in local_ingest_run\n",
      "    res = fetch_and_store_local(ep)\n",
      "  File \"/tmp/ipykernel_46859/3180135904.py\", line 43, in fetch_and_store_local\n",
      "    r.raise_for_status()\n",
      "    ~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 503 Server Error: Service Unavailable for url: https://github.com/owid/covid-19-data/raw/refs/heads/master/public/data/latest/owid-covid-latest.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok',\n",
       " 'results': [{'name': 'life_expectancy',\n",
       "   'local_path': 'local_raw_data/public-health/raw/life_expectancy/20251211T152027Z-ab387a6f17c348808c9377a1d6cf632d.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 96134},\n",
       "  {'name': 'malaria_incidence',\n",
       "   'local_path': 'local_raw_data/public-health/raw/malaria_incidence/20251211T152030Z-130f7545af434bbfa073b4dcb000b9f4.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 42168},\n",
       "  {'name': 'cholera',\n",
       "   'local_path': 'local_raw_data/public-health/raw/cholera/20251211T152035Z-cef9eea553df40aa9d1e8339ddd11be4.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 84135},\n",
       "  {'name': 'who_outbreaks',\n",
       "   'local_path': 'local_raw_data/public-health/raw/who_outbreaks/20251211T152036Z-c227a9c177ea45beb09910458de9cdbd.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 271769},\n",
       "  {'name': 'owid_covid_latest_csv',\n",
       "   'error': '503 Server Error: Service Unavailable for url: https://github.com/owid/covid-19-data/raw/refs/heads/master/public/data/latest/owid-covid-latest.csv'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: Ingest (fetch & store raw files). Includes WHO GHO cholera endpoint.\n",
    "import os, json, uuid, datetime, requests, logging\n",
    "from pathlib import Path\n",
    "from datetime import timezone\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Base directory where raw data will be stored locally (mimics S3 layout)\n",
    "LOCAL_RAW_DIR = Path(\"local_raw_data\")\n",
    "LOCAL_RAW_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "RAW_PREFIX = \"public-health/raw/\"\n",
    "\n",
    "# Fixed endpoints (correct syntax: parentheses in $filter, valid fields, $top<=1000)\n",
    "ENDPOINTS = [\n",
    "    {\n",
    "        \"name\": \"life_expectancy\",\n",
    "        \"url\": \"https://ghoapi.azureedge.net/api/WHOSIS_000001?$filter=(TimeDim ge 2000) and (SpatialDimType eq 'COUNTRY')&$top=1000&$select=SpatialDim,TimeDim,NumericValue,IndicatorCode\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"malaria_incidence\",\n",
    "        \"url\": \"https://ghoapi.azureedge.net/api/MALARIA_EST_INCIDENCE?$filter=(TimeDim ge 2020) and (SpatialDimType eq 'COUNTRY')&$top=1000&$select=SpatialDim,TimeDim,NumericValue,IndicatorCode\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"cholera\",\n",
    "        \"url\": \"https://ghoapi.azureedge.net/api/CHOLERA_0000000001?$filter=(TimeDim ge 2000) and (SpatialDimType eq 'COUNTRY')&$top=1000&$select=SpatialDim,TimeDim,NumericValue,IndicatorCode\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"who_outbreaks\",\n",
    "        \"url\": \"https://www.who.int/api/news/diseaseoutbreaknews\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"owid_covid_latest_csv\",\n",
    "        # We accept CSV (latest snapshot) and will also handle JSON if needed\n",
    "        \"url\": \"https://github.com/owid/covid-19-data/raw/refs/heads/master/public/data/latest/owid-covid-latest.csv\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def fetch_and_store_local(item):\n",
    "    \"\"\"Fetch a URL and write the response to local_raw_data/<prefix>/<endpoint>/<file>.(json|csv)\"\"\"\n",
    "    r = requests.get(item['url'], timeout=30)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    # timestamp + uuid for unique filenames (UTC)\n",
    "    ts = datetime.datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')\n",
    "    # choose extension from content-type or URL\n",
    "    ext = None\n",
    "    ct = r.headers.get('Content-Type','').lower()\n",
    "    if 'csv' in ct or item['url'].lower().endswith('.csv'):\n",
    "        ext = 'csv'\n",
    "    else:\n",
    "        ext = 'json'\n",
    "    file_name = f\"{ts}-{uuid.uuid4().hex}.{ext}\"\n",
    "\n",
    "    # Construct local path\n",
    "    local_path = LOCAL_RAW_DIR / RAW_PREFIX / item['name']\n",
    "    local_path.mkdir(parents=True, exist_ok=True)\n",
    "    full_file_path = local_path / file_name\n",
    "\n",
    "    # Write to disk\n",
    "    full_file_path.write_bytes(r.content)\n",
    "\n",
    "    logger.info(\"Fetched %s -> %s (%d bytes)\", item['name'], full_file_path, len(r.content))\n",
    "    return {\"local_path\": str(full_file_path), \"status\": \"stored\", \"bytes\": len(r.content)}\n",
    "\n",
    "def local_ingest_run():\n",
    "    results = []\n",
    "    for ep in ENDPOINTS:\n",
    "        try:\n",
    "            res = fetch_and_store_local(ep)\n",
    "            results.append({\"name\": ep['name'], **res})\n",
    "        except Exception as ex:\n",
    "            logger.exception(\"Failed to fetch %s\", ep['name'])\n",
    "            results.append({\"name\": ep['name'], \"error\": str(ex)})\n",
    "    return {\"status\":\"ok\", \"results\":results}\n",
    "\n",
    "# Run ingestion\n",
    "output = local_ingest_run()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ace71678-dc52-444f-8660-b53dbc001a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to fetch life_expectancy\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_46859/2001162242.py\", line 75, in local_ingest_run\n",
      "    res = fetch_and_store_local(ep)\n",
      "  File \"/tmp/ipykernel_46859/2001162242.py\", line 47, in fetch_and_store_local\n",
      "    r.raise_for_status()\n",
      "    ~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://ghoapi.azureedge.net/api/WHOSIS_000001?$filter=TimeDim%20ge%202000%20and%20SpatialDimType%20eq%20'COUNTRY'&$top=10000&$select=SpatialDim,SpatialDimName,TimeDim,NumericValue,IndicatorCode,Indicator\n",
      "Failed to fetch malaria_incidence\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_46859/2001162242.py\", line 75, in local_ingest_run\n",
      "    res = fetch_and_store_local(ep)\n",
      "  File \"/tmp/ipykernel_46859/2001162242.py\", line 47, in fetch_and_store_local\n",
      "    r.raise_for_status()\n",
      "    ~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://ghoapi.azureedge.net/api/MALARIA_EST_INCIDENCE?$filter=TimeDim%20ge%202020%20and%20SpatialDimType%20eq%20'COUNTRY'&$top=10000&$select=SpatialDim,SpatialDimName,TimeDim,NumericValue,IndicatorCode,Indicator\n",
      "Failed to fetch cholera\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_46859/2001162242.py\", line 75, in local_ingest_run\n",
      "    res = fetch_and_store_local(ep)\n",
      "  File \"/tmp/ipykernel_46859/2001162242.py\", line 47, in fetch_and_store_local\n",
      "    r.raise_for_status()\n",
      "    ~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/cmogbo/.local/lib/python3.13/site-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://ghoapi.azureedge.net/api/CHOLERA_0000000001?$filter=TimeDim%20ge%202000%20and%20SpatialDimType%20eq%20'COUNTRY'&$top=10000&$select=SpatialDim,SpatialDimName,TimeDim,NumericValue,IndicatorCode,Indicator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok',\n",
       " 'results': [{'name': 'life_expectancy',\n",
       "   'error': \"400 Client Error: Bad Request for url: https://ghoapi.azureedge.net/api/WHOSIS_000001?$filter=TimeDim%20ge%202000%20and%20SpatialDimType%20eq%20'COUNTRY'&$top=10000&$select=SpatialDim,SpatialDimName,TimeDim,NumericValue,IndicatorCode,Indicator\"},\n",
       "  {'name': 'malaria_incidence',\n",
       "   'error': \"400 Client Error: Bad Request for url: https://ghoapi.azureedge.net/api/MALARIA_EST_INCIDENCE?$filter=TimeDim%20ge%202020%20and%20SpatialDimType%20eq%20'COUNTRY'&$top=10000&$select=SpatialDim,SpatialDimName,TimeDim,NumericValue,IndicatorCode,Indicator\"},\n",
       "  {'name': 'cholera',\n",
       "   'error': \"400 Client Error: Bad Request for url: https://ghoapi.azureedge.net/api/CHOLERA_0000000001?$filter=TimeDim%20ge%202000%20and%20SpatialDimType%20eq%20'COUNTRY'&$top=10000&$select=SpatialDim,SpatialDimName,TimeDim,NumericValue,IndicatorCode,Indicator\"},\n",
       "  {'name': 'who_outbreaks',\n",
       "   'local_path': 'local_raw_data/public-health/raw/who_outbreaks/20251211T153755Z-1abcb779a76e4294a46e49d7b3df686c.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 271769},\n",
       "  {'name': 'wb_hospital_beds_per_1000',\n",
       "   'local_path': 'local_raw_data/public-health/raw/wb_hospital_beds_per_1000/20251211T153758Z-5f2dc14f9d174e5ea6ea833071ba4f94.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 1441870},\n",
       "  {'name': 'wb_physicians_per_1000',\n",
       "   'local_path': 'local_raw_data/public-health/raw/wb_physicians_per_1000/20251211T153759Z-6f6f725399754ed1adf77d625310082a.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 1417866}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: Ingest (fetch & store raw files). OWID removed; World Bank WDI indicators added.\n",
    "import os, json, uuid, datetime, requests, logging\n",
    "from pathlib import Path\n",
    "from datetime import timezone\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Base directory where raw data will be stored locally (mimics S3 layout)\n",
    "LOCAL_RAW_DIR = Path(\"local_raw_data\")\n",
    "LOCAL_RAW_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "RAW_PREFIX = \"public-health/raw/\"\n",
    "\n",
    "# Fixed endpoints: WHO GHO indicators + WHO outbreaks + World Bank indicators (WDI)\n",
    "ENDPOINTS = [\n",
    "    {\n",
    "        \"name\": \"life_expectancy\",\n",
    "        \"url\": \"https://ghoapi.azureedge.net/api/WHOSIS_000001?$filter=TimeDim ge 2000 and SpatialDimType eq 'COUNTRY'&$top=10000&$select=SpatialDim,SpatialDimName,TimeDim,NumericValue,IndicatorCode,Indicator\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"malaria_incidence\",\n",
    "        \"url\": \"https://ghoapi.azureedge.net/api/MALARIA_EST_INCIDENCE?$filter=TimeDim ge 2020 and SpatialDimType eq 'COUNTRY'&$top=10000&$select=SpatialDim,SpatialDimName,TimeDim,NumericValue,IndicatorCode,Indicator\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"cholera\",\n",
    "        \"url\": \"https://ghoapi.azureedge.net/api/CHOLERA_0000000001?$filter=TimeDim ge 2000 and SpatialDimType eq 'COUNTRY'&$top=10000&$select=SpatialDim,SpatialDimName,TimeDim,NumericValue,IndicatorCode,Indicator\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"who_outbreaks\",\n",
    "        \"url\": \"https://www.who.int/api/news/diseaseoutbreaknews\"\n",
    "    },\n",
    "    # World Bank WDI: Hospital beds per 1,000 (SH.MED.BEDS.ZS) and Physicians per 1,000 (SH.MED.PHYS.ZS)\n",
    "    {\n",
    "        \"name\": \"wb_hospital_beds_per_1000\",\n",
    "        \"url\": \"https://api.worldbank.org/v2/country/all/indicator/SH.MED.BEDS.ZS?format=json&date=2000:2024&per_page=20000\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"wb_physicians_per_1000\",\n",
    "        \"url\": \"https://api.worldbank.org/v2/country/all/indicator/SH.MED.PHYS.ZS?format=json&date=2000:2024&per_page=20000\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def fetch_and_store_local(item):\n",
    "    \"\"\"Fetch a URL and write the response to local_raw_data/<prefix>/<endpoint>/<file>.(json|csv)\"\"\"\n",
    "    r = requests.get(item['url'], timeout=60)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    # timestamp + uuid for unique filenames (UTC)\n",
    "    ts = datetime.datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')\n",
    "    # choose extension from content-type or URL\n",
    "    ext = None\n",
    "    ct = r.headers.get('Content-Type','').lower()\n",
    "    if 'csv' in ct or item['url'].lower().endswith('.csv'):\n",
    "        ext = 'csv'\n",
    "    else:\n",
    "        ext = 'json'\n",
    "    file_name = f\"{ts}-{uuid.uuid4().hex}.{ext}\"\n",
    "\n",
    "    # Construct local path\n",
    "    local_path = LOCAL_RAW_DIR / RAW_PREFIX / item['name']\n",
    "    local_path.mkdir(parents=True, exist_ok=True)\n",
    "    full_file_path = local_path / file_name\n",
    "\n",
    "    # Write to disk\n",
    "    full_file_path.write_bytes(r.content)\n",
    "\n",
    "    logger.info(\"Fetched %s -> %s (%d bytes)\", item['name'], full_file_path, len(r.content))\n",
    "    return {\"local_path\": str(full_file_path), \"status\": \"stored\", \"bytes\": len(r.content)}\n",
    "\n",
    "def local_ingest_run():\n",
    "    results = []\n",
    "    for ep in ENDPOINTS:\n",
    "        try:\n",
    "            res = fetch_and_store_local(ep)\n",
    "            results.append({\"name\": ep['name'], **res})\n",
    "        except Exception as ex:\n",
    "            logger.exception(\"Failed to fetch %s\", ep['name'])\n",
    "            results.append({\"name\": ep['name'], \"error\": str(ex)})\n",
    "    return {\"status\":\"ok\", \"results\":results}\n",
    "\n",
    "# Run ingestion\n",
    "output = local_ingest_run()\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc45de54-312a-46ec-b912-8206892e0077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok',\n",
       " 'results': [{'name': 'life_expectancy',\n",
       "   'local_path': 'local_raw_data/public-health/raw/life_expectancy/20251212T093422Z-25addc47a02f44b688f6c873d453e85e.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 96134},\n",
       "  {'name': 'malaria_incidence',\n",
       "   'local_path': 'local_raw_data/public-health/raw/malaria_incidence/20251212T093424Z-b7183a13ce1a45ce819e7a4b1d858f47.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 42168},\n",
       "  {'name': 'cholera',\n",
       "   'local_path': 'local_raw_data/public-health/raw/cholera/20251212T093427Z-d6e7d4833bad47c1b12947e74641b5ff.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 84135},\n",
       "  {'name': 'who_outbreaks',\n",
       "   'local_path': 'local_raw_data/public-health/raw/who_outbreaks/20251212T093433Z-0a4884e9d650416dad1fd032853575c0.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 271769},\n",
       "  {'name': 'wb_hospital_beds_per_1000',\n",
       "   'local_path': 'local_raw_data/public-health/raw/wb_hospital_beds_per_1000/20251212T093442Z-89f86b6358344a36b40f9d01d4ebdbbd.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 1441870},\n",
       "  {'name': 'wb_physicians_per_1000',\n",
       "   'local_path': 'local_raw_data/public-health/raw/wb_physicians_per_1000/20251212T093447Z-6f2603a9d53e4d06b0cfa5504106005b.json',\n",
       "   'status': 'stored',\n",
       "   'bytes': 1417866}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: Ingest (fetch & store raw files). OWID removed; World Bank WDI indicators added.\n",
    "import os, json, uuid, datetime, requests, logging\n",
    "from pathlib import Path\n",
    "from datetime import timezone\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Base directory where raw data will be stored locally (mimics S3 layout)\n",
    "LOCAL_RAW_DIR = Path(\"local_raw_data\")\n",
    "LOCAL_RAW_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "RAW_PREFIX = \"public-health/raw/\"\n",
    "\n",
    "# Fixed endpoints: WHO GHO indicators + WHO outbreaks + World Bank indicators (WDI)\n",
    "ENDPOINTS = [\n",
    "    {\n",
    "        \"name\": \"life_expectancy\",\n",
    "        \"url\": \"https://ghoapi.azureedge.net/api/WHOSIS_000001?$filter=(TimeDim ge 2000) and (SpatialDimType eq 'COUNTRY')&$top=1000&$select=SpatialDim,TimeDim,NumericValue,IndicatorCode\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"malaria_incidence\",\n",
    "        \"url\": \"https://ghoapi.azureedge.net/api/MALARIA_EST_INCIDENCE?$filter=(TimeDim ge 2020) and (SpatialDimType eq 'COUNTRY')&$top=1000&$select=SpatialDim,TimeDim,NumericValue,IndicatorCode\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"cholera\",\n",
    "        \"url\": \"https://ghoapi.azureedge.net/api/CHOLERA_0000000001?$filter=(TimeDim ge 2000) and (SpatialDimType eq 'COUNTRY')&$top=1000&$select=SpatialDim,TimeDim,NumericValue,IndicatorCode\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"who_outbreaks\",\n",
    "        \"url\": \"https://www.who.int/api/news/diseaseoutbreaknews\"\n",
    "    },\n",
    "    # World Bank WDI: Hospital beds per 1,000 (SH.MED.BEDS.ZS) and Physicians per 1,000 (SH.MED.PHYS.ZS)\n",
    "    {\n",
    "        \"name\": \"wb_hospital_beds_per_1000\",\n",
    "        \"url\": \"https://api.worldbank.org/v2/country/all/indicator/SH.MED.BEDS.ZS?format=json&date=2000:2024&per_page=20000\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"wb_physicians_per_1000\",\n",
    "        \"url\": \"https://api.worldbank.org/v2/country/all/indicator/SH.MED.PHYS.ZS?format=json&date=2000:2024&per_page=20000\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def fetch_and_store_local(item):\n",
    "    \"\"\"Fetch a URL and write the response to local_raw_data/<prefix>/<endpoint>/<file>.(json|csv)\"\"\"\n",
    "    r = requests.get(item['url'], timeout=60)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    # timestamp + uuid for unique filenames (UTC)\n",
    "    ts = datetime.datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')\n",
    "    # choose extension from content-type or URL\n",
    "    ext = None\n",
    "    ct = r.headers.get('Content-Type','').lower()\n",
    "    if 'csv' in ct or item['url'].lower().endswith('.csv'):\n",
    "        ext = 'csv'\n",
    "    else:\n",
    "        ext = 'json'\n",
    "    file_name = f\"{ts}-{uuid.uuid4().hex}.{ext}\"\n",
    "\n",
    "    # Construct local path\n",
    "    local_path = LOCAL_RAW_DIR / RAW_PREFIX / item['name']\n",
    "    local_path.mkdir(parents=True, exist_ok=True)\n",
    "    full_file_path = local_path / file_name\n",
    "\n",
    "    # Write to disk\n",
    "    full_file_path.write_bytes(r.content)\n",
    "\n",
    "    logger.info(\"Fetched %s -> %s (%d bytes)\", item['name'], full_file_path, len(r.content))\n",
    "    return {\"local_path\": str(full_file_path), \"status\": \"stored\", \"bytes\": len(r.content)}\n",
    "\n",
    "def local_ingest_run():\n",
    "    results = []\n",
    "    for ep in ENDPOINTS:\n",
    "        try:\n",
    "            res = fetch_and_store_local(ep)\n",
    "            results.append({\"name\": ep['name'], **res})\n",
    "        except Exception as ex:\n",
    "            logger.exception(\"Failed to fetch %s\", ep['name'])\n",
    "            results.append({\"name\": ep['name'], \"error\": str(ex)})\n",
    "    return {\"status\":\"ok\", \"results\":results}\n",
    "\n",
    "# Run ingestion\n",
    "output = local_ingest_run()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9708da23-a3c5-42bc-96bc-891472fe2cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL helpers ready! Run the processor calls below when you want to convert raw → clean:\n",
      "\n",
      "life_expectancy: 2,000 total records\n",
      "  → Wrote 92 rows → part-00000.parquet (year=2000)\n",
      "  → Wrote 84 rows → part-00000.parquet (year=2001)\n",
      "  → Wrote 78 rows → part-00000.parquet (year=2002)\n",
      "  → Wrote 80 rows → part-00000.parquet (year=2003)\n",
      "  → Wrote 106 rows → part-00000.parquet (year=2004)\n",
      "  → Wrote 122 rows → part-00000.parquet (year=2005)\n",
      "  → Wrote 94 rows → part-00000.parquet (year=2006)\n",
      "  → Wrote 74 rows → part-00000.parquet (year=2007)\n",
      "  → Wrote 84 rows → part-00000.parquet (year=2008)\n",
      "  → Wrote 102 rows → part-00000.parquet (year=2009)\n",
      "  → Wrote 90 rows → part-00000.parquet (year=2010)\n",
      "  → Wrote 84 rows → part-00000.parquet (year=2011)\n",
      "  → Wrote 82 rows → part-00000.parquet (year=2012)\n",
      "  → Wrote 100 rows → part-00000.parquet (year=2013)\n",
      "  → Wrote 94 rows → part-00000.parquet (year=2014)\n",
      "  → Wrote 94 rows → part-00000.parquet (year=2015)\n",
      "  → Wrote 80 rows → part-00000.parquet (year=2016)\n",
      "  → Wrote 68 rows → part-00000.parquet (year=2017)\n",
      "  → Wrote 110 rows → part-00000.parquet (year=2018)\n",
      "  → Wrote 100 rows → part-00000.parquet (year=2019)\n",
      "  → Wrote 86 rows → part-00000.parquet (year=2020)\n",
      "  → Wrote 96 rows → part-00000.parquet (year=2021)\n",
      "malaria_incidence: 810 total records\n",
      "  → Wrote 210 rows → part-00000.parquet (year=2020)\n",
      "  → Wrote 200 rows → part-00000.parquet (year=2021)\n",
      "  → Wrote 204 rows → part-00000.parquet (year=2022)\n",
      "  → Wrote 196 rows → part-00000.parquet (year=2023)\n",
      "cholera: 883 total records\n",
      "  → Wrote 56 rows → part-00000.parquet (year=2000)\n",
      "  → Wrote 59 rows → part-00000.parquet (year=2001)\n",
      "  → Wrote 50 rows → part-00000.parquet (year=2002)\n",
      "  → Wrote 44 rows → part-00000.parquet (year=2003)\n",
      "  → Wrote 55 rows → part-00000.parquet (year=2004)\n",
      "  → Wrote 70 rows → part-00000.parquet (year=2005)\n",
      "  → Wrote 66 rows → part-00000.parquet (year=2006)\n",
      "  → Wrote 59 rows → part-00000.parquet (year=2007)\n",
      "  → Wrote 59 rows → part-00000.parquet (year=2008)\n",
      "  → Wrote 45 rows → part-00000.parquet (year=2009)\n",
      "  → Wrote 47 rows → part-00000.parquet (year=2010)\n",
      "  → Wrote 58 rows → part-00000.parquet (year=2011)\n",
      "  → Wrote 48 rows → part-00000.parquet (year=2012)\n",
      "  → Wrote 47 rows → part-00000.parquet (year=2013)\n",
      "  → Wrote 42 rows → part-00000.parquet (year=2014)\n",
      "  → Wrote 42 rows → part-00000.parquet (year=2015)\n",
      "  → Wrote 36 rows → part-00000.parquet (year=2016)\n",
      "wb_hospital_beds_per_1000: 13,050 total records\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2000)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2001)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2002)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2003)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2004)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2005)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2006)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2007)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2008)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2009)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2010)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2011)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2012)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2013)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2014)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2015)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2016)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2017)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2018)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2019)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2020)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2021)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2022)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2023)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2024)\n",
      "wb_physicians_per_1000: 13,050 total records\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2000)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2001)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2002)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2003)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2004)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2005)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2006)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2007)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2008)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2009)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2010)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2011)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2012)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2013)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2014)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2015)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2016)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2017)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2018)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2019)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2020)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2021)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2022)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2023)\n",
      "  → Wrote 522 rows → part-00000.parquet (year=2024)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: ETL helpers — parse WHO GHO + World Bank JSON → clean DataFrames + manifest\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow  # ensures Parquet support\n",
    "from pathlib import Path\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Paths\n",
    "# ------------------------------------------------------------------\n",
    "RAW_BASE = Path(\"local_raw_data/public-health/raw\")\n",
    "CLEAN_BASE = Path(\"local_clean_data\")\n",
    "CLEAN_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MANIFEST_PATH = Path(\"ingest_manifest.json\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Manifest: incremental processing\n",
    "# ------------------------------------------------------------------\n",
    "def load_manifest():\n",
    "    if MANIFEST_PATH.exists():\n",
    "        return json.loads(MANIFEST_PATH.read_text())\n",
    "    return {\"processed\": {}}\n",
    "\n",
    "def save_manifest():\n",
    "    MANIFEST_PATH.write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "manifest = load_manifest()\n",
    "\n",
    "def file_hash(path: Path) -> str:\n",
    "    return hashlib.sha256(path.read_bytes()).hexdigest()\n",
    "\n",
    "def list_new_raw_files_for_endpoint(endpoint_name: str):\n",
    "    folder = RAW_BASE / endpoint_name\n",
    "    if not folder.exists():\n",
    "        return []\n",
    "    files = sorted(folder.glob(\"*.json\"))\n",
    "    new_files = []\n",
    "    for f in files:\n",
    "        key = str(f)\n",
    "        seen = manifest[\"processed\"].get(key, {})\n",
    "        if not seen or seen.get(\"hash\") != file_hash(f):\n",
    "            new_files.append(f)\n",
    "    return new_files\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. WHO GHO OData Parser (tested on your real files)\n",
    "# ------------------------------------------------------------------\n",
    "def parse_gho_json_to_df(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Parse WHO GHO JSON (has @odata.context and 'value' list)\"\"\"\n",
    "    data = json.loads(path.read_text())\n",
    "    \n",
    "    # Extract the actual records\n",
    "    rows = data.get(\"value\", [])\n",
    "    if not rows and isinstance(data, list):\n",
    "        rows = data  # fallback\n",
    "    \n",
    "    records = []\n",
    "    for rec in rows:\n",
    "        records.append({\n",
    "            \"country_code\": rec.get(\"SpatialDim\"),\n",
    "            \"year\": int(rec[\"TimeDim\"]) if rec.get(\"TimeDim\") else None,\n",
    "            \"value\": pd.to_numeric(rec.get(\"NumericValue\"), errors=\"coerce\"),\n",
    "            \"indicator_code\": rec.get(\"IndicatorCode\")\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    # Clean: keep only real countries (3-letter ISO) and valid years\n",
    "    df = df[df[\"country_code\"].str.len() == 3]\n",
    "    df = df.dropna(subset=[\"country_code\", \"year\"])\n",
    "    df[\"year\"] = df[\"year\"].astype(int)\n",
    "    \n",
    "    return df[[\"country_code\", \"year\", \"value\", \"indicator_code\"]]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. World Bank WDI Parser (tested on your SH.MED.*.json files)\n",
    "# ------------------------------------------------------------------\n",
    "def parse_worldbank_json_to_df(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Parse World Bank API response: [meta, [records]]\"\"\"\n",
    "    raw = json.loads(path.read_text())\n",
    "    \n",
    "    # Data is always in index 1\n",
    "    if isinstance(raw, list) and len(raw) >= 2:\n",
    "        data = raw[1]\n",
    "    else:\n",
    "        data = raw  # fallback\n",
    "    \n",
    "    records = []\n",
    "    for item in data:\n",
    "        country = item.get(\"country\", {})\n",
    "        indicator = item.get(\"indicator\", {})\n",
    "        \n",
    "        records.append({\n",
    "            \"country_code\": item.get(\"countryiso3code\") or country.get(\"id\"),\n",
    "            \"country_name\": country.get(\"value\"),\n",
    "            \"year\": int(item[\"date\"]) if item.get(\"date\") else None,\n",
    "            \"value\": pd.to_numeric(item.get(\"value\"), errors=\"coerce\"),\n",
    "            \"indicator_id\": indicator.get(\"id\"),\n",
    "            \"indicator_label\": indicator.get(\"value\")\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    # Filter: real countries only + valid data\n",
    "    df = df[df[\"country_code\"].str.len() == 3]\n",
    "    df = df.dropna(subset=[\"country_code\", \"year\"])\n",
    "    df[\"year\"] = df[\"year\"].astype(int)\n",
    "    \n",
    "    return df[[\"country_code\", \"year\", \"value\", \"indicator_id\", \"indicator_label\"]]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Generic ETL Processor (writes partitioned Parquet)\n",
    "# ------------------------------------------------------------------\n",
    "def process_endpoint(endpoint_name: str, parser_func, output_subfolder: str):\n",
    "    files = list_new_raw_files_for_endpoint(endpoint_name)\n",
    "    if not files:\n",
    "        print(f\"No new files for {endpoint_name}\")\n",
    "        return\n",
    "\n",
    "    out_dir = CLEAN_BASE / output_subfolder / endpoint_name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = parser_func(f)\n",
    "            if df.empty:\n",
    "                continue\n",
    "            df[\"source_file\"] = f.name\n",
    "            df[\"ingested_at\"] = datetime.now(timezone.utc).isoformat(timespec='seconds') + \"Z\"\n",
    "            all_dfs.append(df)\n",
    "\n",
    "            # Update manifest\n",
    "            manifest[\"processed\"][str(f)] = {\n",
    "                \"hash\": file_hash(f),\n",
    "                \"rows\": len(df),\n",
    "                \"processed_at\": datetime.now(timezone.utc).isoformat(timespec='seconds') + \"Z\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {f.name}: {e}\")\n",
    "\n",
    "    if not all_dfs:\n",
    "        print(f\"No valid data in {endpoint_name}\")\n",
    "        return\n",
    "\n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"{endpoint_name}: {len(combined):,} total records\")\n",
    "\n",
    "    # Partition by year\n",
    "    for year, group in combined.groupby(\"year\"):\n",
    "        year_dir = out_dir / f\"year={year}\"\n",
    "        year_dir.mkdir(parents=True, exist_ok=True)\n",
    "        out_file = year_dir / \"part-00000.parquet\"\n",
    "        group.to_parquet(out_file, index=False)\n",
    "        print(f\"  → Wrote {len(group)} rows → {out_file.name} (year={year})\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# RUN IT (uncomment when ready)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"ETL helpers ready! Run the processor calls below when you want to convert raw → clean:\\n\")\n",
    "\n",
    "# Example usage (uncomment one by one):\n",
    "process_endpoint(\"life_expectancy\", parse_gho_json_to_df, \"who\")\n",
    "process_endpoint(\"malaria_incidence\", parse_gho_json_to_df, \"who\")\n",
    "process_endpoint(\"cholera\", parse_gho_json_to_df, \"who\")\n",
    "process_endpoint(\"wb_hospital_beds_per_1000\", parse_worldbank_json_to_df, \"worldbank\")\n",
    "process_endpoint(\"wb_physicians_per_1000\", parse_worldbank_json_to_df, \"worldbank\")\n",
    "\n",
    "save_manifest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "947192f1-5510-4e2b-bd93-6471045f26fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ETL — creating Parquet files NOW!\n",
      "\n",
      "Written 984 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2000/data.parquet\n",
      "Written 948 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2001/data.parquet\n",
      "Written 921 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2002/data.parquet\n",
      "Written 930 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2003/data.parquet\n",
      "Written 1047 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2004/data.parquet\n",
      "Written 1119 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2005/data.parquet\n",
      "Written 993 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2006/data.parquet\n",
      "Written 903 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2007/data.parquet\n",
      "Written 948 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2008/data.parquet\n",
      "Written 1029 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2009/data.parquet\n",
      "Written 975 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2010/data.parquet\n",
      "Written 948 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2011/data.parquet\n",
      "Written 939 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2012/data.parquet\n",
      "Written 1020 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2013/data.parquet\n",
      "Written 993 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2014/data.parquet\n",
      "Written 993 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2015/data.parquet\n",
      "Written 930 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2016/data.parquet\n",
      "Written 876 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2017/data.parquet\n",
      "Written 1065 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2018/data.parquet\n",
      "Written 1020 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2019/data.parquet\n",
      "Written 957 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2020/data.parquet\n",
      "Written 1002 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/life_expectancy/year=2021/data.parquet\n",
      "Written 945 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/malaria_incidence/year=2020/data.parquet\n",
      "Written 900 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/malaria_incidence/year=2021/data.parquet\n",
      "Written 918 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/malaria_incidence/year=2022/data.parquet\n",
      "Written 882 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/malaria_incidence/year=2023/data.parquet\n",
      "Written 392 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2000/data.parquet\n",
      "Written 413 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2001/data.parquet\n",
      "Written 350 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2002/data.parquet\n",
      "Written 308 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2003/data.parquet\n",
      "Written 385 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2004/data.parquet\n",
      "Written 490 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2005/data.parquet\n",
      "Written 462 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2006/data.parquet\n",
      "Written 413 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2007/data.parquet\n",
      "Written 413 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2008/data.parquet\n",
      "Written 315 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2009/data.parquet\n",
      "Written 329 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2010/data.parquet\n",
      "Written 406 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2011/data.parquet\n",
      "Written 336 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2012/data.parquet\n",
      "Written 329 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2013/data.parquet\n",
      "Written 294 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2014/data.parquet\n",
      "Written 294 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2015/data.parquet\n",
      "Written 252 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/who/cholera/year=2016/data.parquet\n",
      "Written 588 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2000/data.parquet\n",
      "Written 592 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2001/data.parquet\n",
      "Written 604 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2002/data.parquet\n",
      "Written 648 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2003/data.parquet\n",
      "Written 608 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2004/data.parquet\n",
      "Written 744 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2005/data.parquet\n",
      "Written 744 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2006/data.parquet\n",
      "Written 688 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2007/data.parquet\n",
      "Written 672 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2008/data.parquet\n",
      "Written 732 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2009/data.parquet\n",
      "Written 780 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2010/data.parquet\n",
      "Written 744 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2011/data.parquet\n",
      "Written 692 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2012/data.parquet\n",
      "Written 716 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2013/data.parquet\n",
      "Written 716 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2014/data.parquet\n",
      "Written 724 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2015/data.parquet\n",
      "Written 720 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2016/data.parquet\n",
      "Written 688 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2017/data.parquet\n",
      "Written 668 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2018/data.parquet\n",
      "Written 668 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2019/data.parquet\n",
      "Written 596 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2020/data.parquet\n",
      "Written 280 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_hospital_beds_per_1000/year=2021/data.parquet\n",
      "Written 584 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2000/data.parquet\n",
      "Written 396 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2001/data.parquet\n",
      "Written 376 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2002/data.parquet\n",
      "Written 380 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2003/data.parquet\n",
      "Written 556 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2004/data.parquet\n",
      "Written 428 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2005/data.parquet\n",
      "Written 408 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2006/data.parquet\n",
      "Written 444 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2007/data.parquet\n",
      "Written 532 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2008/data.parquet\n",
      "Written 520 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2009/data.parquet\n",
      "Written 668 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2010/data.parquet\n",
      "Written 456 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2011/data.parquet\n",
      "Written 464 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2012/data.parquet\n",
      "Written 456 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2013/data.parquet\n",
      "Written 472 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2014/data.parquet\n",
      "Written 472 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2015/data.parquet\n",
      "Written 500 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2016/data.parquet\n",
      "Written 520 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2017/data.parquet\n",
      "Written 592 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2018/data.parquet\n",
      "Written 488 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2019/data.parquet\n",
      "Written 680 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2020/data.parquet\n",
      "Written 412 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2021/data.parquet\n",
      "Written 244 rows → /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data/worldbank/wb_physicians_per_1000/year=2022/data.parquet\n",
      "\n",
      "ETL COMPLETE! Files are in: /home/cmogbo/Documents/jupyter_test/RESEARCH_DATA_ANALYST_PORTOFOLIO/cloud-data-portfolio/project-03-public-health-datalake/clean_data\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — FINAL VERSION THAT WORKS NO MATTER WHERE YOU RUN IT FROM\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# These paths are relative to where the notebook is saved\n",
    "NOTEBOOK_DIR = Path.cwd()  # This is where you are right now\n",
    "LOCAL_RAW_DIR = NOTEBOOK_DIR / \"local_raw_data\"\n",
    "CLEAN_BASE = NOTEBOOK_DIR / \"clean_data\"\n",
    "CLEAN_BASE.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "MANIFEST_PATH = NOTEBOOK_DIR / \"ingest_manifest.json\"\n",
    "\n",
    "def load_manifest():\n",
    "    if MANIFEST_PATH.exists():\n",
    "        return json.loads(MANIFEST_PATH.read_text())\n",
    "    return {\"processed\": {}, \"last_run\": None}\n",
    "\n",
    "def save_manifest():\n",
    "    MANIFEST_PATH.write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "manifest = load_manifest()\n",
    "\n",
    "def file_hash(path: Path):\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def list_new_raw_files(endpoint_name: str):\n",
    "    folder = LOCAL_RAW_DIR / \"public-health/raw\" / endpoint_name\n",
    "    if not folder.exists():\n",
    "        return []\n",
    "    return list(folder.glob(\"*.json\"))\n",
    "\n",
    "# ----------------------- GHO Parser -----------------------\n",
    "def parse_gho_json_to_df(file_path: Path) -> pd.DataFrame:\n",
    "    data = json.loads(file_path.read_text())\n",
    "    rows = data.get(\"value\") if isinstance(data, dict) and \"value\" in data else data\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.rename(columns={\"SpatialDim\": \"country_code\", \"TimeDim\": \"year\", \"NumericValue\": \"value\"})\n",
    "    if \"year\" in df.columns:\n",
    "        df[\"year\"] = pd.to_numeric(df[\"year\"], errors='coerce')\n",
    "        df = df.dropna(subset=[\"year\"])\n",
    "        df[\"year\"] = df[\"year\"].astype(int)\n",
    "    if \"country_code\" in df.columns:\n",
    "        df = df[df[\"country_code\"].str.len() == 3]\n",
    "    return df[[\"country_code\", \"year\", \"value\"]]\n",
    "\n",
    "# FIXED World Bank parser — now extracts the real number correctly\n",
    "def parse_worldbank_json_to_df(file_path: Path) -> pd.DataFrame:\n",
    "    raw = json.loads(file_path.read_text())\n",
    "    if len(raw) != 2 or not raw[1]:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    rows = raw[1]\n",
    "    records = []\n",
    "    for item in rows:\n",
    "        # The actual numeric value is directly in \"value\" — but only if not null\n",
    "        val = item.get(\"value\")\n",
    "        if val is None:\n",
    "            continue  # Skip missing data\n",
    "        records.append({\n",
    "            \"country_code\": item[\"countryiso3code\"],\n",
    "            \"year\": int(item[\"date\"]),\n",
    "            \"value\": float(val)\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# ----------------------- Generic Processor -----------------------\n",
    "def process_endpoint(endpoint_name: str, parser_func, base_folder: str):\n",
    "    files = list_new_raw_files(endpoint_name)\n",
    "    if not files:\n",
    "        logger.info(\"No files for %s\", endpoint_name)\n",
    "        return\n",
    "\n",
    "    out_dir = CLEAN_BASE / base_folder / endpoint_name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_dfs = []\n",
    "    for f in files:\n",
    "        df = parser_func(f)\n",
    "        if not df.empty:\n",
    "            df[\"source_file\"] = f.name\n",
    "            all_dfs.append(df)\n",
    "        manifest[\"processed\"][str(f)] = {\n",
    "            \"hash\": file_hash(f),\n",
    "            \"rows\": len(df),\n",
    "            \"processed_at\": datetime.now(timezone.utc).isoformat(timespec='seconds') + \"Z\"\n",
    "        }\n",
    "\n",
    "    if not all_dfs:\n",
    "        logger.info(\"No data after parsing for %s\", endpoint_name)\n",
    "        return\n",
    "\n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    logger.info(\"Total records for %s: %d\", endpoint_name, len(combined))\n",
    "\n",
    "    for year, group in combined.groupby(\"year\"):\n",
    "        year_dir = out_dir / f\"year={year}\"\n",
    "        year_dir.mkdir(parents=True, exist_ok=True)\n",
    "        out_file = year_dir / \"data.parquet\"\n",
    "        group.to_parquet(out_file, index=False)\n",
    "        print(f\"Written {len(group)} rows → {out_file}\")  # No .relative_to() → no crash!\n",
    "\n",
    "# ----------------------- RUN -----------------------\n",
    "print(\"Starting ETL — creating Parquet files NOW!\\n\")\n",
    "\n",
    "process_endpoint(\"life_expectancy\", parse_gho_json_to_df, \"who\")\n",
    "process_endpoint(\"malaria_incidence\", parse_gho_json_to_df, \"who\")\n",
    "process_endpoint(\"cholera\", parse_gho_json_to_df, \"who\")\n",
    "process_endpoint(\"wb_hospital_beds_per_1000\", parse_worldbank_json_to_df, \"worldbank\")\n",
    "process_endpoint(\"wb_physicians_per_1000\", parse_worldbank_json_to_df, \"worldbank\")\n",
    "\n",
    "save_manifest()\n",
    "print(\"\\nETL COMPLETE! Files are in:\", CLEAN_BASE.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "415d2084-9926-4850-aad0-c29c380264d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== QUICK DATA EXPLORATION ===\n",
      "\n",
      "Cholera data loaded: 6,181 total records\n",
      "\n",
      "  country_code  year  value                                        source_file\n",
      "0          SOM  2000    NaN  20251211T150305Z-270fcfda84ed407ab006186c9c3fe...\n",
      "1          COM  2000    NaN  20251211T150305Z-270fcfda84ed407ab006186c9c3fe...\n",
      "2          GBR  2000    NaN  20251211T150305Z-270fcfda84ed407ab006186c9c3fe...\n",
      "3          ECU  2000    NaN  20251211T150305Z-270fcfda84ed407ab006186c9c3fe...\n",
      "4          SLV  2000    NaN  20251211T150305Z-270fcfda84ed407ab006186c9c3fe... \n",
      "\n",
      "Top 10 countries by most recent cholera cases:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>value</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>HTI</td>\n",
       "      <td>2016</td>\n",
       "      <td>41421.0</td>\n",
       "      <td>20251211T150608Z-583e7f3cce2f4ebba88bc53228ba7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>COD</td>\n",
       "      <td>2016</td>\n",
       "      <td>28093.0</td>\n",
       "      <td>20251211T150608Z-583e7f3cce2f4ebba88bc53228ba7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>YEM</td>\n",
       "      <td>2016</td>\n",
       "      <td>15751.0</td>\n",
       "      <td>20251211T150502Z-5f3e7626c9454f0fa888198cf8060...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>SOM</td>\n",
       "      <td>2016</td>\n",
       "      <td>15619.0</td>\n",
       "      <td>20251211T150608Z-583e7f3cce2f4ebba88bc53228ba7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>TZA</td>\n",
       "      <td>2016</td>\n",
       "      <td>11360.0</td>\n",
       "      <td>20251211T150502Z-5f3e7626c9454f0fa888198cf8060...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>KEN</td>\n",
       "      <td>2016</td>\n",
       "      <td>5866.0</td>\n",
       "      <td>20251211T150608Z-583e7f3cce2f4ebba88bc53228ba7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>IRQ</td>\n",
       "      <td>2015</td>\n",
       "      <td>4965.0</td>\n",
       "      <td>20251211T150608Z-583e7f3cce2f4ebba88bc53228ba7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>SSD</td>\n",
       "      <td>2016</td>\n",
       "      <td>4295.0</td>\n",
       "      <td>20251211T150608Z-583e7f3cce2f4ebba88bc53228ba7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>MWI</td>\n",
       "      <td>2016</td>\n",
       "      <td>1792.0</td>\n",
       "      <td>20251211T150608Z-583e7f3cce2f4ebba88bc53228ba7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>PAK</td>\n",
       "      <td>2014</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>20251211T150608Z-583e7f3cce2f4ebba88bc53228ba7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country_code  year    value  \\\n",
       "50           HTI  2016  41421.0   \n",
       "22           COD  2016  28093.0   \n",
       "122          YEM  2016  15751.0   \n",
       "104          SOM  2016  15619.0   \n",
       "116          TZA  2016  11360.0   \n",
       "60           KEN  2016   5866.0   \n",
       "55           IRQ  2015   4965.0   \n",
       "105          SSD  2016   4295.0   \n",
       "78           MWI  2016   1792.0   \n",
       "89           PAK  2014   1218.0   \n",
       "\n",
       "                                           source_file  \n",
       "50   20251211T150608Z-583e7f3cce2f4ebba88bc53228ba7...  \n",
       "22   20251211T150608Z-583e7f3cce2f4ebba88bc53228ba7...  \n",
       "122  20251211T150502Z-5f3e7626c9454f0fa888198cf8060...  \n",
       "104  20251211T150608Z-583e7f3cce2f4ebba88bc53228ba7...  \n",
       "116  20251211T150502Z-5f3e7626c9454f0fa888198cf8060...  \n",
       "60   20251211T150608Z-583e7f3cce2f4ebba88bc53228ba7...  \n",
       "55   20251211T150608Z-583e7f3cce2f4ebba88bc53228ba7...  \n",
       "105  20251211T150608Z-583e7f3cce2f4ebba88bc53228ba7...  \n",
       "78   20251211T150608Z-583e7f3cce2f4ebba88bc53228ba7...  \n",
       "89   20251211T150608Z-583e7f3cce2f4ebba88bc53228ba7...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hospital beds per 1,000 — 232 countries\n",
      "Top 15 countries (latest data, non-NaN value found):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9671</th>\n",
       "      <td>MCO</td>\n",
       "      <td>2014</td>\n",
       "      <td>22.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8243</th>\n",
       "      <td>PRK</td>\n",
       "      <td>2012</td>\n",
       "      <td>13.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14366</th>\n",
       "      <td>KOR</td>\n",
       "      <td>2021</td>\n",
       "      <td>12.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13815</th>\n",
       "      <td>JPN</td>\n",
       "      <td>2020</td>\n",
       "      <td>12.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14377</th>\n",
       "      <td>MNG</td>\n",
       "      <td>2021</td>\n",
       "      <td>10.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13111</th>\n",
       "      <td>BLR</td>\n",
       "      <td>2019</td>\n",
       "      <td>9.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13801</th>\n",
       "      <td>DEU</td>\n",
       "      <td>2020</td>\n",
       "      <td>7.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13779</th>\n",
       "      <td>BGR</td>\n",
       "      <td>2020</td>\n",
       "      <td>7.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13769</th>\n",
       "      <td>AUT</td>\n",
       "      <td>2020</td>\n",
       "      <td>7.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13852</th>\n",
       "      <td>ROU</td>\n",
       "      <td>2020</td>\n",
       "      <td>7.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14384</th>\n",
       "      <td>RUS</td>\n",
       "      <td>2021</td>\n",
       "      <td>7.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14359</th>\n",
       "      <td>HUN</td>\n",
       "      <td>2021</td>\n",
       "      <td>6.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13817</th>\n",
       "      <td>KAZ</td>\n",
       "      <td>2020</td>\n",
       "      <td>6.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13788</th>\n",
       "      <td>CZE</td>\n",
       "      <td>2020</td>\n",
       "      <td>6.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13738</th>\n",
       "      <td>CEB</td>\n",
       "      <td>2020</td>\n",
       "      <td>6.389553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      country_code  year      value\n",
       "9671           MCO  2014  22.020000\n",
       "8243           PRK  2012  13.200000\n",
       "14366          KOR  2021  12.750000\n",
       "13815          JPN  2020  12.720000\n",
       "14377          MNG  2021  10.550000\n",
       "13111          BLR  2019   9.690000\n",
       "13801          DEU  2020   7.800000\n",
       "13779          BGR  2020   7.770000\n",
       "13769          AUT  2020   7.060000\n",
       "13852          ROU  2020   7.060000\n",
       "14384          RUS  2021   7.030000\n",
       "14359          HUN  2021   6.840000\n",
       "13817          KAZ  2020   6.720000\n",
       "13788          CZE  2020   6.600000\n",
       "13738          CEB  2020   6.389553"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Physicians per 1,000 — 240 countries\n",
      "Top 15 countries (latest data, non-NaN value found):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10417</th>\n",
       "      <td>CUB</td>\n",
       "      <td>2021</td>\n",
       "      <td>9.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9832</th>\n",
       "      <td>MCO</td>\n",
       "      <td>2020</td>\n",
       "      <td>8.889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10431</th>\n",
       "      <td>GRC</td>\n",
       "      <td>2021</td>\n",
       "      <td>6.367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6759</th>\n",
       "      <td>SMR</td>\n",
       "      <td>2014</td>\n",
       "      <td>6.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10468</th>\n",
       "      <td>PRT</td>\n",
       "      <td>2021</td>\n",
       "      <td>5.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10828</th>\n",
       "      <td>GEO</td>\n",
       "      <td>2022</td>\n",
       "      <td>5.613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10806</th>\n",
       "      <td>AUT</td>\n",
       "      <td>2022</td>\n",
       "      <td>5.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10460</th>\n",
       "      <td>NOR</td>\n",
       "      <td>2021</td>\n",
       "      <td>5.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10840</th>\n",
       "      <td>LTU</td>\n",
       "      <td>2022</td>\n",
       "      <td>5.127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10811</th>\n",
       "      <td>BGR</td>\n",
       "      <td>2022</td>\n",
       "      <td>4.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10862</th>\n",
       "      <td>URY</td>\n",
       "      <td>2022</td>\n",
       "      <td>4.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10832</th>\n",
       "      <td>ISL</td>\n",
       "      <td>2022</td>\n",
       "      <td>4.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10429</th>\n",
       "      <td>DEU</td>\n",
       "      <td>2021</td>\n",
       "      <td>4.518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10478</th>\n",
       "      <td>ESP</td>\n",
       "      <td>2021</td>\n",
       "      <td>4.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9767</th>\n",
       "      <td>BLR</td>\n",
       "      <td>2020</td>\n",
       "      <td>4.470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      country_code  year  value\n",
       "10417          CUB  2021  9.429\n",
       "9832           MCO  2020  8.889\n",
       "10431          GRC  2021  6.367\n",
       "6759           SMR  2014  6.018\n",
       "10468          PRT  2021  5.767\n",
       "10828          GEO  2022  5.613\n",
       "10806          AUT  2022  5.508\n",
       "10460          NOR  2021  5.168\n",
       "10840          LTU  2022  5.127\n",
       "10811          BGR  2022  4.896\n",
       "10862          URY  2022  4.630\n",
       "10832          ISL  2022  4.524\n",
       "10429          DEU  2021  4.518\n",
       "10478          ESP  2021  4.480\n",
       "9767           BLR  2020  4.470"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "17 countries with >500 cholera cases AND fewest hospital beds (values are now correct):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>cholera_cases</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AFG</td>\n",
       "      <td>677.0</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BEN</td>\n",
       "      <td>761.0</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>UGA</td>\n",
       "      <td>516.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NGA</td>\n",
       "      <td>768.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PAK</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TZA</td>\n",
       "      <td>11360.0</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YEM</td>\n",
       "      <td>15751.0</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MOZ</td>\n",
       "      <td>883.0</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COD</td>\n",
       "      <td>28093.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SOM</td>\n",
       "      <td>15619.0</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country_code  cholera_cases  value\n",
       "15          AFG          677.0   0.36\n",
       "14          BEN          761.0   0.45\n",
       "16          UGA          516.0   0.50\n",
       "13          NGA          768.0   0.50\n",
       "8           PAK         1218.0   0.51\n",
       "4           TZA        11360.0   0.63\n",
       "2           YEM        15751.0   0.71\n",
       "10          MOZ          883.0   0.73\n",
       "1           COD        28093.0   0.80\n",
       "3           SOM        15619.0   0.87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 4 — FINAL WORKING VERSION (with NaN Fix)\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CLEAN_BASE = Path(\"clean_data\")\n",
    "\n",
    "print(\"=== QUICK DATA EXPLORATION ===\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Cholera – latest reported cases per country\n",
    "# ------------------------------------------------------------------\n",
    "cholera_path = CLEAN_BASE / \"who\" / \"cholera\"\n",
    "cholera_parquets = list(cholera_path.rglob(\"*.parquet\"))\n",
    "\n",
    "if cholera_parquets:\n",
    "    chol_df = pd.concat([pd.read_parquet(p) for p in cholera_parquets], ignore_index=True)\n",
    "    print(f\"Cholera data loaded: {len(chol_df):,} total records\\n\")\n",
    "    print(chol_df.head(), \"\\n\")\n",
    "\n",
    "    latest_cholera = (\n",
    "        chol_df.sort_values(\"year\")\n",
    "        .groupby(\"country_code\", as_index=False)\n",
    "        .last()\n",
    "        .sort_values(\"value\", ascending=False)\n",
    "    )\n",
    "    print(\"Top 10 countries by most recent cholera cases:\")\n",
    "    display(latest_cholera.head(10))\n",
    "else:\n",
    "    print(\"No cholera files — something went wrong\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Hospital beds per 1,000 (FIXED)\n",
    "# ------------------------------------------------------------------\n",
    "beds_path = CLEAN_BASE / \"worldbank\" / \"wb_hospital_beds_per_1000\"\n",
    "beds_parquets = list(beds_path.rglob(\"*.parquet\"))\n",
    "\n",
    "if beds_parquets:\n",
    "    beds_df = pd.concat([pd.read_parquet(p) for p in beds_parquets], ignore_index=True)\n",
    "    \n",
    "    # Drop rows where 'value' is NaN, ensuring we only consider valid data points.\n",
    "    beds_df_clean = beds_df.dropna(subset=['value'])\n",
    "    \n",
    "    # Now, find the latest year (idxmax) from the CLEANED data\n",
    "    latest_beds = beds_df_clean.loc[beds_df_clean.groupby('country_code')['year'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nHospital beds per 1,000 — {len(latest_beds)} countries\")\n",
    "    print(\"Top 15 countries (latest data, non-NaN value found):\")\n",
    "    display(latest_beds.nlargest(15, 'value')[['country_code', 'year', 'value']])\n",
    "else:\n",
    "    print(\"No beds files found — check folder name\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Physicians per 1,000 (FIXED)\n",
    "# ------------------------------------------------------------------\n",
    "phys_path = CLEAN_BASE / \"worldbank\" / \"wb_physicians_per_1000\"\n",
    "phys_parquets = list(phys_path.rglob(\"*.parquet\"))\n",
    "\n",
    "if phys_parquets:\n",
    "    phys_df = pd.concat([pd.read_parquet(p) for p in phys_parquets], ignore_index=True)\n",
    "    \n",
    "    # Drop rows where 'value' is NaN, ensuring we only consider valid data points.\n",
    "    phys_df_clean = phys_df.dropna(subset=['value'])\n",
    "    \n",
    "    # Now, find the latest year (idxmax) from the CLEANED data\n",
    "    latest_phys = phys_df_clean.loc[phys_df_clean.groupby('country_code')['year'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nPhysicians per 1,000 — {len(latest_phys)} countries\")\n",
    "    print(\"Top 15 countries (latest data, non-NaN value found):\")\n",
    "    display(latest_phys.nlargest(15, 'value')[['country_code', 'year', 'value']])\n",
    "else:\n",
    "    print(\"No physicians files found\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. High cholera + low hospital beds (RESULT WILL NOW BE ACCURATE)\n",
    "# ------------------------------------------------------------------\n",
    "if cholera_parquets and beds_parquets:\n",
    "    merged = latest_cholera[['country_code', 'value']].rename(columns={'value': 'cholera_cases'})\n",
    "    # The 'value' column in latest_beds will now contain actual numbers!\n",
    "    merged = merged.merge(latest_beds[['country_code', 'value']], on='country_code', suffixes=('', '_beds'))\n",
    "    high_risk = merged[merged['cholera_cases'] > 500]\n",
    "    if len(high_risk) > 0:\n",
    "        print(f\"\\n{len(high_risk)} countries with >500 cholera cases AND fewest hospital beds (values are now correct):\")\n",
    "        display(high_risk.sort_values('value').head(10))\n",
    "    else:\n",
    "        print(\"\\nNo high-risk countries found.\")\n",
    "else:\n",
    "    print(\"Missing data for high-risk analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a7e8318-e158-4de4-859c-19ed64f471ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking clean_data folder structure...\n",
      "\n",
      "DIR:  who\n",
      "DIR:  worldbank\n",
      "DIR:  who/life_expectancy\n",
      "DIR:  who/malaria_incidence\n",
      "DIR:  who/cholera\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000\n",
      "DIR:  worldbank/wb_physicians_per_1000\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2000\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2001\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2002\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2003\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2004\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2005\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2006\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2007\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2008\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2009\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2010\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2011\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2012\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2013\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2014\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2015\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2016\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2017\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2018\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2019\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2020\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2021\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2022\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2023\n",
      "DIR:  worldbank/wb_hospital_beds_per_1000/year=2024\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2000\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2001\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2002\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2003\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2004\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2005\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2006\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2007\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2008\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2009\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2010\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2011\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2012\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2013\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2014\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2015\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2016\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2017\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2018\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2019\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2020\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2021\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2022\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2023\n",
      "DIR:  worldbank/wb_physicians_per_1000/year=2024\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2000/data.parquet  (5.3 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2001/data.parquet  (4.9 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2002/data.parquet  (4.8 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2003/data.parquet  (4.8 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2004/data.parquet  (5.0 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2005/data.parquet  (5.0 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2006/data.parquet  (4.9 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2007/data.parquet  (5.1 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2008/data.parquet  (5.1 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2009/data.parquet  (5.1 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2010/data.parquet  (5.4 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2011/data.parquet  (5.2 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2012/data.parquet  (4.8 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2013/data.parquet  (5.1 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2014/data.parquet  (5.2 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2015/data.parquet  (5.2 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2016/data.parquet  (5.4 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2017/data.parquet  (5.2 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2018/data.parquet  (5.0 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2019/data.parquet  (5.3 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2020/data.parquet  (5.5 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2021/data.parquet  (4.9 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2022/data.parquet  (4.2 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2023/data.parquet  (4.7 KB)\n",
      "FILE: worldbank/wb_physicians_per_1000/year=2024/data.parquet  (4.7 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2000/data.parquet  (5.1 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2001/data.parquet  (5.0 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2002/data.parquet  (5.0 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2003/data.parquet  (5.3 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2004/data.parquet  (5.0 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2005/data.parquet  (5.5 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2006/data.parquet  (5.5 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2007/data.parquet  (5.2 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2008/data.parquet  (5.1 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2009/data.parquet  (5.3 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2010/data.parquet  (5.4 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2011/data.parquet  (5.4 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2012/data.parquet  (5.4 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2013/data.parquet  (5.3 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2014/data.parquet  (5.3 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2015/data.parquet  (5.5 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2016/data.parquet  (5.4 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2017/data.parquet  (5.2 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2018/data.parquet  (5.2 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2019/data.parquet  (5.2 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2020/data.parquet  (5.1 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2021/data.parquet  (4.3 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2022/data.parquet  (4.7 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2023/data.parquet  (4.7 KB)\n",
      "FILE: worldbank/wb_hospital_beds_per_1000/year=2024/data.parquet  (4.7 KB)\n",
      "DIR:  who/life_expectancy/year=2000\n",
      "DIR:  who/life_expectancy/year=2001\n",
      "DIR:  who/life_expectancy/year=2002\n",
      "DIR:  who/life_expectancy/year=2003\n",
      "DIR:  who/life_expectancy/year=2004\n",
      "DIR:  who/life_expectancy/year=2005\n",
      "DIR:  who/life_expectancy/year=2006\n",
      "DIR:  who/life_expectancy/year=2007\n",
      "DIR:  who/life_expectancy/year=2008\n",
      "DIR:  who/life_expectancy/year=2009\n",
      "DIR:  who/life_expectancy/year=2010\n",
      "DIR:  who/life_expectancy/year=2011\n",
      "DIR:  who/life_expectancy/year=2012\n",
      "DIR:  who/life_expectancy/year=2013\n",
      "DIR:  who/life_expectancy/year=2014\n",
      "DIR:  who/life_expectancy/year=2015\n",
      "DIR:  who/life_expectancy/year=2016\n",
      "DIR:  who/life_expectancy/year=2017\n",
      "DIR:  who/life_expectancy/year=2018\n",
      "DIR:  who/life_expectancy/year=2019\n",
      "DIR:  who/life_expectancy/year=2020\n",
      "DIR:  who/life_expectancy/year=2021\n",
      "DIR:  who/malaria_incidence/year=2020\n",
      "DIR:  who/malaria_incidence/year=2021\n",
      "DIR:  who/malaria_incidence/year=2022\n",
      "DIR:  who/malaria_incidence/year=2023\n",
      "DIR:  who/cholera/year=2000\n",
      "DIR:  who/cholera/year=2001\n",
      "DIR:  who/cholera/year=2002\n",
      "DIR:  who/cholera/year=2003\n",
      "DIR:  who/cholera/year=2004\n",
      "DIR:  who/cholera/year=2005\n",
      "DIR:  who/cholera/year=2006\n",
      "DIR:  who/cholera/year=2007\n",
      "DIR:  who/cholera/year=2008\n",
      "DIR:  who/cholera/year=2009\n",
      "DIR:  who/cholera/year=2010\n",
      "DIR:  who/cholera/year=2011\n",
      "DIR:  who/cholera/year=2012\n",
      "DIR:  who/cholera/year=2013\n",
      "DIR:  who/cholera/year=2014\n",
      "DIR:  who/cholera/year=2015\n",
      "DIR:  who/cholera/year=2016\n",
      "FILE: who/cholera/year=2000/data.parquet  (3.5 KB)\n",
      "FILE: who/cholera/year=2001/data.parquet  (3.6 KB)\n",
      "FILE: who/cholera/year=2002/data.parquet  (3.5 KB)\n",
      "FILE: who/cholera/year=2003/data.parquet  (3.4 KB)\n",
      "FILE: who/cholera/year=2004/data.parquet  (3.6 KB)\n",
      "FILE: who/cholera/year=2005/data.parquet  (3.8 KB)\n",
      "FILE: who/cholera/year=2006/data.parquet  (3.7 KB)\n",
      "FILE: who/cholera/year=2007/data.parquet  (3.6 KB)\n",
      "FILE: who/cholera/year=2008/data.parquet  (3.6 KB)\n",
      "FILE: who/cholera/year=2009/data.parquet  (3.5 KB)\n",
      "FILE: who/cholera/year=2010/data.parquet  (3.5 KB)\n",
      "FILE: who/cholera/year=2011/data.parquet  (3.6 KB)\n",
      "FILE: who/cholera/year=2012/data.parquet  (3.8 KB)\n",
      "FILE: who/cholera/year=2013/data.parquet  (4.0 KB)\n",
      "FILE: who/cholera/year=2014/data.parquet  (3.8 KB)\n",
      "FILE: who/cholera/year=2015/data.parquet  (3.8 KB)\n",
      "FILE: who/cholera/year=2016/data.parquet  (3.7 KB)\n",
      "FILE: who/malaria_incidence/year=2020/data.parquet  (6.0 KB)\n",
      "FILE: who/malaria_incidence/year=2021/data.parquet  (4.9 KB)\n",
      "FILE: who/malaria_incidence/year=2022/data.parquet  (5.3 KB)\n",
      "FILE: who/malaria_incidence/year=2023/data.parquet  (5.2 KB)\n",
      "FILE: who/life_expectancy/year=2000/data.parquet  (10.1 KB)\n",
      "FILE: who/life_expectancy/year=2001/data.parquet  (10.2 KB)\n",
      "FILE: who/life_expectancy/year=2002/data.parquet  (10.2 KB)\n",
      "FILE: who/life_expectancy/year=2003/data.parquet  (10.3 KB)\n",
      "FILE: who/life_expectancy/year=2004/data.parquet  (10.3 KB)\n",
      "FILE: who/life_expectancy/year=2005/data.parquet  (10.4 KB)\n",
      "FILE: who/life_expectancy/year=2006/data.parquet  (10.3 KB)\n",
      "FILE: who/life_expectancy/year=2007/data.parquet  (10.2 KB)\n",
      "FILE: who/life_expectancy/year=2008/data.parquet  (10.1 KB)\n",
      "FILE: who/life_expectancy/year=2009/data.parquet  (10.3 KB)\n",
      "FILE: who/life_expectancy/year=2010/data.parquet  (10.3 KB)\n",
      "FILE: who/life_expectancy/year=2011/data.parquet  (10.1 KB)\n",
      "FILE: who/life_expectancy/year=2012/data.parquet  (10.2 KB)\n",
      "FILE: who/life_expectancy/year=2013/data.parquet  (10.3 KB)\n",
      "FILE: who/life_expectancy/year=2014/data.parquet  (10.3 KB)\n",
      "FILE: who/life_expectancy/year=2015/data.parquet  (10.3 KB)\n",
      "FILE: who/life_expectancy/year=2016/data.parquet  (10.3 KB)\n",
      "FILE: who/life_expectancy/year=2017/data.parquet  (10.2 KB)\n",
      "FILE: who/life_expectancy/year=2018/data.parquet  (10.3 KB)\n",
      "FILE: who/life_expectancy/year=2019/data.parquet  (10.3 KB)\n",
      "FILE: who/life_expectancy/year=2020/data.parquet  (10.3 KB)\n",
      "FILE: who/life_expectancy/year=2021/data.parquet  (10.5 KB)\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTIC: Show exactly what exists on disk right now\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Checking clean_data folder structure...\\n\")\n",
    "\n",
    "CLEAN_BASE = Path(\"clean_data\")\n",
    "\n",
    "for p in CLEAN_BASE.rglob(\"*\"):\n",
    "    if p.is_dir():\n",
    "        print(\"DIR: \", p.relative_to(CLEAN_BASE))\n",
    "    elif p.suffix == \".parquet\":\n",
    "        size_kb = p.stat().st_size / 1024\n",
    "        print(f\"FILE: {p.relative_to(CLEAN_BASE)}  ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ceeafca8-9f12-49ff-85f2-64e91ba5b709",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowTypeError",
     "evalue": "Unable to merge: Field year has incompatible types: int64 vs dictionary<values=int32, indices=int32, ordered=0>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowTypeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Load from Athena or local Parquet\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m who = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclean_data/who/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m wb = pd.read_parquet(\u001b[33m\"\u001b[39m\u001b[33mclean_data/worldbank/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Latest life expectancy\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/parquet.py:265\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    259\u001b[39m     path,\n\u001b[32m    260\u001b[39m     filesystem,\n\u001b[32m    261\u001b[39m     storage_options=storage_options,\n\u001b[32m    262\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    263\u001b[39m )\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     pa_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n\u001b[32m    274\u001b[39m         filterwarnings(\n\u001b[32m    275\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmake_block is deprecated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    277\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    278\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pyarrow/parquet/core.py:1843\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, filesystem, filters, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001b[39m\n\u001b[32m   1831\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mread_table\u001b[39m(source, *, columns=\u001b[38;5;28;01mNone\u001b[39;00m, use_threads=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1832\u001b[39m                schema=\u001b[38;5;28;01mNone\u001b[39;00m, use_pandas_metadata=\u001b[38;5;28;01mFalse\u001b[39;00m, read_dictionary=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1833\u001b[39m                binary_type=\u001b[38;5;28;01mNone\u001b[39;00m, list_type=\u001b[38;5;28;01mNone\u001b[39;00m, memory_map=\u001b[38;5;28;01mFalse\u001b[39;00m, buffer_size=\u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1839\u001b[39m                page_checksum_verification=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1840\u001b[39m                arrow_extensions_enabled=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1842\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1843\u001b[39m         dataset = \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1844\u001b[39m \u001b[43m            \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1845\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1846\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1847\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1849\u001b[39m \u001b[43m            \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1850\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbinary_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1851\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlist_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlist_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1852\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1854\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1855\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1856\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1859\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1860\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1861\u001b[39m \u001b[43m            \u001b[49m\u001b[43marrow_extensions_enabled\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrow_extensions_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1863\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1864\u001b[39m         \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[32m   1865\u001b[39m         \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[32m   1866\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pyarrow/parquet/core.py:1424\u001b[39m, in \u001b[36mParquetDataset.__init__\u001b[39m\u001b[34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001b[39m\n\u001b[32m   1420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m partitioning == \u001b[33m\"\u001b[39m\u001b[33mhive\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1421\u001b[39m     partitioning = ds.HivePartitioning.discover(\n\u001b[32m   1422\u001b[39m         infer_dictionary=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1424\u001b[39m \u001b[38;5;28mself\u001b[39m._dataset = \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1425\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1426\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1427\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pyarrow/dataset.py:790\u001b[39m, in \u001b[36mdataset\u001b[39m\u001b[34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[39m\n\u001b[32m    779\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    780\u001b[39m     schema=schema,\n\u001b[32m    781\u001b[39m     filesystem=filesystem,\n\u001b[32m   (...)\u001b[39m\u001b[32m    786\u001b[39m     selector_ignore_prefixes=ignore_prefixes\n\u001b[32m    787\u001b[39m )\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, FileInfo) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pyarrow/dataset.py:482\u001b[39m, in \u001b[36m_filesystem_dataset\u001b[39m\u001b[34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[39m\n\u001b[32m    474\u001b[39m options = FileSystemFactoryOptions(\n\u001b[32m    475\u001b[39m     partitioning=partitioning,\n\u001b[32m    476\u001b[39m     partition_base_dir=partition_base_dir,\n\u001b[32m    477\u001b[39m     exclude_invalid_files=exclude_invalid_files,\n\u001b[32m    478\u001b[39m     selector_ignore_prefixes=selector_ignore_prefixes\n\u001b[32m    479\u001b[39m )\n\u001b[32m    480\u001b[39m factory = FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfactory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pyarrow/_dataset.pyx:3196\u001b[39m, in \u001b[36mpyarrow._dataset.DatasetFactory.finish\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowTypeError\u001b[39m: Unable to merge: Field year has incompatible types: int64 vs dictionary<values=int32, indices=int32, ordered=0>"
     ]
    }
   ],
   "source": [
    "# scripts/generate-dashboard.py\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "\n",
    "# Load from Athena or local Parquet\n",
    "who = pd.read_parquet(\"clean_data/who/\")\n",
    "wb = pd.read_parquet(\"clean_data/worldbank/\")\n",
    "\n",
    "# Latest life expectancy\n",
    "le = who[who.indicator_code == \"WHOSIS_000001\"].copy()\n",
    "latest_le = le.loc[le.groupby(\"country_code\")[\"year\"].idxmax()]\n",
    "\n",
    "fig1 = px.choropleth(\n",
    "    latest_le,\n",
    "    locations=\"country_code\",\n",
    "    color=\"value\",\n",
    "    hover_name=\"country_code\",\n",
    "    color_continuous_scale=\"RdYlGn\",\n",
    "    title=\"Life Expectancy by Country (Latest Year)\"\n",
    ")\n",
    "fig1.write_html(\"dashboard/life-expectancy.html\")\n",
    "\n",
    "# Top 10 cholera\n",
    "cholera = who[who.indicator_code.str.contains(\"CHOLERA\", na=False)]\n",
    "latest_cholera = cholera.loc[cholera.groupby(\"country_code\")[\"year\"].idxmax()]\n",
    "top10 = latest_cholera.nlargest(10, \"value\")\n",
    "\n",
    "fig2 = px.bar(top10, x=\"country_code\", y=\"value\", title=\"Top 10 Countries — Cholera Cases (Latest)\")\n",
    "fig2.write_html(\"dashboard/top-cholera.html\")\n",
    "\n",
    "print(\"Dashboard generated → dashboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454d735e-cc7d-4a59-8728-2581e8639d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
